%\documentclass{ncc}
%\documentclass{article}
\documentclass{scrartcl}

\usepackage{amsmath,amssymb,amsfonts} % Typical maths resource packages
\usepackage{graphics}                 % Packages to allow inclusion of graphics
\usepackage{color}                    % For creating coloured text and background
\usepackage{hyperref}                 % For creating hyperlinks in
                                % cross references 

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}
\usepackage{listings}
\lstloadlanguages {[LaTeX]TeX, Octave}
\lstset {language=[LaTeX]TeX,
  extendedchars=true ,escapechar=|}


\begin{document}
\label{Chapter 1}
\section {Вводная}
\subsection* {Supervised Learning} - where the ``right (correct)
answers'' for
each example of data is given. \\
Usually we have one of two:\\
{\bf Regression:} predict continuous valued output \\
{\bf Classification:} assign discrete valued output (0, 1 etc) \\

\subsection* {Unsupervised Learning} - finding structure in ``blind''
set of data. \\
{\bf Clustering} algorithms  \\
Example: {\bf Cocktail Party Problem} (разделение записей на двух
микрофонах от двух источниках и восстановление сигналов от каждого
источника индивидуально). \\

\label{Chapter 2}
\section {Linear Regression with One Variable}

\subsection {Model Representation}
Notation: \\
{\bf m} =  Number of training examples \\
{\bf x}'s = ``input'' values / features \\
{\bf y}'s = ``output''variable / ``target'' variable \\
{\bf (x, y)} = one single training example \\
{\bf $(x^{(i)}, y^{(i)})$} = $i^{th}$ training example \\

Обычный алгоритм: берем Training Set, скармливаем его Learning
Algorithm. Оный строит ф-ю h (иначе hypothesis), которая по входному
значению $x$ строит Estimation. Иначе говоря, \\
h maps from x's to y's: \\
$h_\theta(x)=\theta_0 + \theta_1x$; shorthand: h(x) \\
- т.е. полагаем что ф-я линейная \\
Другие названия:
\begin{itemize}
\item Linear regression with one variable (x)
\item Univariate(single variable) linear regression
\end{itemize}
\subsection {Cost Function}
\label{2-2}
Идея - минимизировать \[ J(\theta_0, \theta_1) = \frac{1}{2m}
\sum_{i=1}^m\left( h_\theta (x^{(i)}) - y^{(i)} \right)^2
\]
Оная $J(\theta_0, \theta_1)$ и называется Cost Function, она же
Squared Error Function

\subsection {Gradient Descent}
\label {2-5}
Задача: имея функцию $J(\theta_0, \theta_1)$, заполучить ее
${\min \atop \theta_0, \theta_1} J(\theta_0, \theta_1)$ \\
Outline:
\begin {itemize}
\item Start with some $\theta_0, \theta_1$
\item Keep changing $\theta_0, \theta_1$ to reduce J($\theta_0,
  \theta_1$) until we hopefully end up at a minimum
\end{itemize}
Работает также для бОльшего количества параметров ( $\theta_0,
\theta_1,..., \theta_n$)
\subsubsection {Gradient descent algorithm}
repeat until convergence \{ \\
$\theta_j := \theta_j - {\alpha} \underbrace{ \frac {\delta}{\delta
    \theta_j} J(\theta_0,
  \theta_1)}_{derivative}$ (for j=0 and j=1) \\
\}, where: \\
$\alpha$ - learning rate \\
$\frac {\delta}{\delta \theta_j} J(\theta_0, \theta_1)$ - derivative
(производная, точнее, частная производная)

\subparagraph {Correct: Simultaneous upgrade}

temp0 := $\theta_0 - \alpha \frac {\delta}{\delta \theta_0}
J(\theta_0,
\theta_1)$ \\
temp1 := $\theta_1 - \alpha \frac {\delta}{\delta \theta_1}
J(\theta_0,
\theta_1)$ \\
$\theta_0$ := temp0 \\
$\theta_1$ := temp1 \\

\subsection {Gradient Descent Algorithm For Linear Regression }
\label {2-7}
После подстановок - получаем частные производные:
\[ \frac {\delta}{\delta \theta_j} J(\theta_0, \theta_1) = \frac
{\delta}{\delta \theta_j} \frac{1}{2m} \sum\limits_{i=1}^m\left(
  h_\theta (x^{(i)}) - y^{(i)} \right)^2 = \frac {\delta}{\delta
  \theta_j} \frac{1}{2m} \sum\limits_{i=1}^m\left(\theta_0 + \theta_1
  x^{(i)} - y^{(i)} \right)^2
\] \\
Тогда:
\[
\theta_0 : \frac {\delta}{\delta \theta_0} J(\theta_0, \theta_1) =
\frac{1}{m} \sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)}
\right) \]
\[
\theta_1 : \frac {\delta}{\delta \theta_1} J(\theta_0, \theta_1) =
\frac{1}{m} \sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)}
\right) * x^{(i)}
\]
Тогда алгоритм будет: \\
repeat until convergence \{ \\
\
\[\theta_0 := \theta_0 - {\alpha} \frac{1}{m}
\sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)} \right) \]
\[\theta_1 := \theta_1 - {\alpha} \frac{1}{m}
\sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)} \right) *
x^{(i)} \] \} Утверждает, что Cost Function для Linear Regression
всегда является ``bow shaped'', или ``Convex function'' - т.е. не
имеет локальных минимумов; единственный минимум - глобальный.

Дальше, он называет сей процесс ``Batch'' Gradient Descent - т.к. Each
step of gradient descent uses all the training examples.

\subsection {Extensions}
\label {2-8}
1. Существует прямой (не - итерационный) метод обсчета $\theta_0,
\theta_1$ \\
2. БОльшее количество параметров (features): $y = F(x_1, x_2, ...,
x_n)$

\label{Chapter 3}
\section {Linear Algebra Review}
Ничего интересного, обозначения:
\[
\left( \begin{array}{cc}
    1402 & 191 \\
    1371 & 821 \\ 949 & 1437 \\ 147 & 1448 \\
  \end{array} \right) = \mathbb{R}^{4\times2}
\]
Identity Matrix - единичная матрица вроде $\left( I
  = \begin{array}{cc}
    1 & 0 \\ 0 & 1 \\
  \end{array}
\right)$

Свойства: $A I = I A = A$ Обычно умножение матриц некомутативно, но
ассоциативно: $A B \neq B A$, но $(A B) C = A (B C)$

Inverse matrix - обратная матрица Transpose matrix - транспонированная
матрица

\label{Chapter 4}
\section {Linear Regression with Multiple Variables}
Notation: \\
\begin{itemize}
\item {m} = number of examples (size of training set)
\item {n} = number of features
\item{$x^{(i)}$} = input (features) of $i^{th}$ training example
  (vector of size n)
\item {$x_j^{(i)}$} = value of feature j in $i^{th}$ training example
\end{itemize}
Hypothesis: \\
\[
h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + ...
\theta_nx_n
\]
For convenience in notation , define $x_0=1$. $(x_0^{(i)}=1)$: Feature
vector (values for a single feature) is of dimention n+1:
\[
x = \left[ \begin{array}{c} x_0 \\ x_1 \\ x_2 \\ \vdots \\ x_n
  \end{array} \right] \in \mathbf{R^{n+1}}
\]
The same story (n+1 dimentional vector) - for parameters:
\[
\theta = \left[ \begin{array}{c} \theta_0 \\ \theta_1 \\ \theta_2 \\
    \vdots \\ \theta_n
  \end{array} \right] \in \mathbf{R^{n+1}}
\]
(another n+1 dimentional vector) Then hypothesis can be written as:
\[
h_\theta(x) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 +
... \theta_nx_n = \\ \theta^TX
\]
where: $\theta^T = [\theta_0 \theta_1 ... \theta_n]$ - so-called {\bf
  raw vector}. \\

This form is also called {\bf Multivariate linear regression }.

\subsection {Gradient Descent for Multiple Variables}
Cost function:
\[
J(\theta_0, \theta_1, ..., \theta_n) = \frac{1}{2m}
\sum\limits_{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)} \right)^2
\]
Gradient descent: \\
Repeat \{
\[ \theta_j := \theta_j - \alpha \frac
{\delta}{\delta\theta_j}J(\theta) \\
= \theta_j - \alpha \frac{1}{m}
\sum\limits_{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)}
\right)x_j^{(i)} \]
\} - simultaneously update for every j = 0,1,...n \\
for j = 0, the formula is the same as of Single-Variable Descent (as
$x_0 = 1$)
\label {4-3}
\subsection {Gradient Descent in practice}
Предлагается масштабировать features так, чтобы избегать ``вытянутых''
контуров на $\theta_1(\theta_2)$ диаграммах, т.к. на оных вытянутых
диаграммах процесс сходится плохо. Сие называется

\subsubsection{ Feature Scaling} 
- get every feature into approximately a $-1 \leq x_i \leq 1$ range.
\subsubsection {Mean normalisation}
- replace $x_i$ with $\frac{x_i - \mu_i}{S_i}$ to make features have
approximately zero mean (do not apply to $x_0 = 1$):
\begin{itemize}
\item{$\mu_i$} - mean value
\item{$S_i$} - the range (max - min) or standard deviation (would be
  fine too)
\end{itemize}
Example:
\begin{equation*}
  \begin{split}
    x_1 = \frac{size-1000}{2000} \\
    x_2 = \frac{\#bedrooms-2}{5} \\
    -0.5 \leq x_1 \leq 0.5, -0.5 \leq x_2 \leq 0.5
  \end{split}
\end{equation*}

\subsection {Gradient Descent in Practice - Learning Rate}
\label {4-4}
Gradient descent works correctly if $J(\theta)$ decrease after every
iteration. Usually convergence is declared if $J(\theta)$ decreases by
less then some constant (i.e. $10^-{3}$) in one iteration.

If $J(\theta)$ increases (oscillates up and down) - this usually means
that learning rate $\alpha$ is too big (but convergence becomes
slower).

So it is recommended to try \[ ..., 0.001,0.003, 0.01, 0.03, 0.1, 0.3,
1, ...
\]
 
\subsection{Features and Polynomial Regression}
\label{4-5}
\subsubsection{Polynomial Regression}
In polynomial regression formulas like $h_\theta(x)=\theta_0 +
\theta_qx + \theta_2x^2 + \theta^3x_3$ we can re-write the expression
as $h_\theta(x) = \theta_0 + \theta_qx_1 + \theta_2x_2 + \theta_3x_3$
where \begin{itemize}
\item{$x_1=x$}
\item{$x_2=x^2$}
\item{$x_1=x^3$}
\end{itemize}
and use the mechanics of the ``normal''linear regression. In addition,
it is usually a better idea to use some sort of ``inside'' information
- in order to choose the ``right'' set of feature. Such selection
process can also be automated.

\subsection {Normal Equation}
\label{4-6}
To solve the $\theta$ analytically. For 1-D features we can use
derivatives: $\frac{d}{d\theta}J(\theta) = 0$, solve it for $\theta$.
But if $\theta \in \mathbb{R}^{n+1}$, we need to solve \[
\frac{\delta}{\delta \theta_j}J(\theta) = \frac{\delta}{\delta
  \theta_j} \left( \frac{1}{2m}
  \sum\limits_{i=1}^m\big(h_\theta(x^{(i)}) - y^{(i)}\big)^2\right) =
0
\] for every j

The idea is: pack the training set into matrix $X^{m \times (n + 1)}$
(so-called design matrix) for features (n features + ones for $x_0$)
and m - dimentional vector y for results. Then:
\[ \theta = \mathbf{\left( X^TX \right)^{-1}X^T}y \] values in
$\theta$ will minimise the cost function. Feature scaling in this case
is NOT actual.

Octave: \emph{ pinv(X'*X)*X'*y }

\paragraph {Normal Equation Gradient Descent)} No need to choose
$\alpha$, don't need to iterate.

\paragraph {Gradient Descent vs Normal Equation} Gradient Descent
works well with large $n$, while calculation $n \times n$ matrix for
large n could be painful.

\subsection {Normal Equation Non-invertibility}
Idea: what if $\mathbf{X^TX}$ is non- invertible (singular)?
Causes:\begin{itemize}
\item Redundant features (linearly dependent). \\
  E.g. $x_1$ = size in $feet^2$, $x_2$ = size in $m^2$
\item Too many features ($e.g. m \leq n$). \\
  Delete some features or use regularisation.

\end{itemize}

In Octave, \emph{pinv(X'*X)*X'*y} will end with some meaningful result
- this is different from ``just'' \emph{inv} function (see Octave doc
for difference between the ``normal'' inversion inv and ``pseudo'' -
inversion pinv functions).

\section {Octave Tutorial}
\label {Chapter 5}
Add ``;'' to make a command (not output it immediately)\\
Commands can be comma - chained Commands / operators:\begin{itemize}
\item {PS1('>> ');} set up the command prompt
\item {help command} prints help on command
\item {\verb!~=!} works as ``not equal'' logical operator
\item {constants}: pi,
\item {output} \verb!disp(sprintf('2 decimals: %0.2f', a))!
\item {switch default output format}: format long / format short
\end{itemize}
\subsubsection {matrix}
\begin{verbatim}
A = [1 2; 3 4;
> 5 6]

>> V = [1 2 3]
\end{verbatim}
Automated init:
\begin{verbatim}
>>v = 1:0.1:2 % creates vector with 11 values: from 1.0 step 0.1 to 2.0
>>v = 1:6 % creates vector [1..6]
>> ones(2, 3) % generates 2 * 3 matrix filled with 1
>> w = zeros(1,3) % generates vector of "o"
>> rand(3,3) % generates matrix, inits with random (normal
distribution values in 0..1 range)
>> rand(1, 3) % generates matrix, inits with random (Gaussian
distribution with mean = 0 and sigma = 1)
>> eye(6) % identity matrix 6 * 6
\end{verbatim}
\verb!hist(w)! builds a histogram for given vector w

\subsection {Moving data around}
\label{5-2}
\begin{verbatim}
>> size (A) % returns dimensions of matrix A
>> size (A, 1) % returns the value for first dimension of A (rows)
>> size (A, 2) % returns the value for second dimension of A (columns)
>> length(v) % returns length of vector. For matrix - returns the
% longer dimension of matrix
>> pwd, cd, ls,   % CO
>> load filename.ext / load('filename.ext') % loads text files with
% data into relevant variable
>> who % lists variables in memory
>> whos % more detailed 'who'
>> clear <variable name> % deletes variable from memory. If called
% without parameter - clears all variables
>> v = V(1:10) % get the first 10 elements of V
>> save hello.mat v; % saves vector v to file hello.mat (binary
% format) to save in human - readable format, use -ascii option

>> A(3, 2) % returns element of matrix A
>> A(2, :) % fetch everything in the second row. Works for columns as
% well; can be used for assigning
>> A([1 3], :) % get everything from rows 1 and 3
>> A = [A , [100; 101; 102]]; % appends the column vector to the
% matrix (notice ';' as delimiter for elements in column)
>> A(:) % put all elements of A into a single vector
>> C = [A B] % concatenate matrices into a new one (horizontally).
% Same is [A, B] (comma instead of space)
>> C = [A;B] % concatenate matrices vertically 
\end{verbatim}

\subsection{Computing on Data}
\label{5-3}
\begin{verbatim}
>> A*C % matrix multiplication
>> A .* B % element-wise operations: each element of A is being
% multiplied by corresponding element of B
>> A .^ 2 % element-wise power
>> 1 ./ v % element-wise reciprocal of v (every element of a new
% vector is 1 / corresponding element of v)
>> log(v), exp(v), abs(a), -v % element-wise log, exp, abs, negative
>> v + ones(length(v), 1) % adds 1 to every element of v (one can do
% it simpler v + 1)
>> A' % A transposed
>> max(v) % maximum element of v. For matrix - give "per-row" maximum
% by default. Trick to calculate the maximum element of matrix:
% max(A(:))
>> [val, ind] = max(v) % assigns val to max value, ind, to the index
% of max element from v
>> v < 3 % element-wise comparing (builds matrix with 0/1 per element)
>> find (v < 3) % returns vector with elements satisfying the
% condition
>> magic(3) % returns "magic square"
>> sum(A), prod(A) % sum and product of all elements of matrix. By
% default - "per-column" (1st dimention - just like sum(A,1). Per-row
% will be calculated by sum(A(:))
>>sum(sum(A.*eye(9))) % trick to calculate the sum of all elements on
% the matrix diagonal. For other diagonal: sum(sum(A.*flipud(eye(9))))
>> floor(A), ceil(A) % rounding all elements
>> rand(3) % create random matrix 3x3
>> max(a, b) % generate matrix with values = element-wise max of both.
% Also exist "per-row" and "per-column" form
>> pinv(A) %"pseudo-inverse matrix 
\end{verbatim}

\subsection{Plotting Data}
\label{5-4}
\begin{itemize}
\item{plot(t, y)} - plot graph with points $t_i, y_i$ from vectors
\item{hold on} - plot 1st graph, ``hold on'', plot another one
\item{xlabel('time'), ylabel('value')} - set labelx for x and y
\item{legend('sin', 'cos'), title('my plot')} - legend and title
\item{print -dpng 'myPlot.png'} - save to file
\item{close} - remove plot
\item{figure(1);plot(t,y1);figure(2);plot(t,y2);} - builds two plots
  on different widows
\item{subplot(1, 2, 1)} - divide plot a 1x2 grid, access first element
\item{axis([0.5 1 -1 1])} - set x- and y- ranges
\item{clf;} - clear
\item{imagesc(A)} - draw image based on goven matrix A
\item{imagesc(A), colorbar, colormap grey} - as before, add colorbar
  legend, make legend grey-scale
\end{itemize}

\subsection{Control Statement}
\label{5-5}
\subsubsection{Control Statements}
\begin{verbatim}
for i=1:10
 v(i) = 2^i;
end;
% or, having a vector like indices=1:10; we can do similar:
for i=indices,
 disp(i);
end;
% _break_ and _continue_ apply
================
i = 1;
while i <= 5,
 v(i) = 100;
 i = i + 1;
end;
========
if i == 6, break;
=======
if i == 6, disp('oops');
elseif v(i) == 2, disp('aah');
else disp('grgrg');
end;
\end{verbatim}
\subsubsection{functions}
Create file named <function-name>.m (text). Example:
\begin{verbatim}
function y = squareThisNumber(x)

y = x^2;
\end{verbatim}
Then use it in Octave:
\begin{verbatim}
>> cd <folder with file>
>> squareThisNumber(5)
===== or use addpath ('some\path\to\function') - see search path idea
\end{verbatim}
Multiple-variable returning functions
\begin{verbatim}
function [y1, y2] = squareAndCubeThisNumber(x)

y1 = x^2;
y2 = x^3;
\end{verbatim}
Variant: $J(\theta)$
\begin{verbatim}
function j = costFunctionJ(X, y, theta)
% X - "design matrix" with training examples y - class labesls

m = size(X, 1)  % number of training examples
predictions = X*theta % predictionsof hypothesis on all m examples
sqrError = (predicitons-y).^2; %squared error (element-wise)

J = 1/(2*m) * sum(sqeErrors);
\end{verbatim}
\subsection{Vectorization}
\label{5-6}
Idea: instead of using cycles (loops) over data matrices or vectors,
use matrix(vector) multiplication operations.

\label {Chapter 6}
\section {Logistic Regression}
\subsection{Classification}
Idea is to separate the input into cathegories like Spam/Ham,
Malignant / Benign tumor etc. Separation: \begin{itemize}
\item {Two-class} (``binary class'') classification $y \in \{0, 1\}$
\item {Multi-class} classification $y \in \{0, 1, ... n\}$
\end{itemize}
Idea: to use Linear Regression with $h_\theta(x) = \theta^T x$ and
threshold which checks: \begin{itemize}
\item{if $h_\theta(x) \geq 0.5$} predict 1
\item{if $h_\theta(x) < 0.5$} predict 0
\end{itemize}
Problem emerges when the training set is not lineary separated, so
``if paramater greater ... then ...'' does not work. In addition, h
can be > 1 or < 0. As such , we need something like \[ Logistic
Regression: 0 \leq h_\theta(x) \leq 1
\]
It is named ``regression'' for historical reason, but this is actually
a classification algorithm.

\subsection{Hypothesis Representation}
We use Linear Regression Model ($h_\theta(x) = \theta^Tx$ ) with
so-called {\bf Sigmoid function} or {\bf Logistic function} : \[
h_\theta(x) = g(\theta^Tx), where \]
\[g(z) = \frac{1}{1 + e^{-z}}\] so
\begin{equation*}
  h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}
\end{equation*}
(see function graph for this)
\subsubsection{Interpretaion}
$h_\theta(x) = $ estimated probability that y=1 on input x
\begin{equation*}
  \textrm{Example: if }  x = \left[ \begin{array}{c} x_0 \\
      x_1  \end{array} \right] = \left[ \begin{array}{c} 1 \\
      tumorSize  \end{array} \right]
\end{equation*}
\begin{equation*} h_\theta(x) = 0.7 \end{equation*} - then patient has
70\% chance of tumor being malignant. Or: $ h_\theta(x) = \frac{1}{1 +
  e^{-\theta^Tx}}$ - ``probability that y = 1, given
x, parametrized by $\theta$'' \\
As y = 0 or 1, then:
\[P( y = 0 | x;\theta) + P(y=1|x;\theta) = 1\]
\[P( y = 0 | x;\theta) = 1 - P(y=1|x;\theta)\]

\subsection{Decision Boundary}
Suppose predict ``y = 1'' if $h_\theta(x) \geq 0.5$ \\
and ``y = 0'' if $h_\theta(x) < 0.5$ \\

as $g(z) \geq 0.5 \ when \ Z \geq 0 \Rightarrow h_\theta(x) =
g(\theta^Tx) \geq 0.5 \ whenever \ \theta^Tx \geq 0$
\label {page 10 from pdf}

Decision boundary - a line that separates regions with different
predictions. It is a property of a hypothesis (not training set!).

\subsection{Cost Function}
\label{sec:6-4}
Training set: $\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \dots ,
(x^{(m)},y^{(m)})\}$ of m examples; x is a vector of
$\mathbf{R^{n+1}}$, where $x_0 = 1, y \in \{0, 1\}$. We're choosing
the parameters $\theta$ to minimise
\[ \textrm{hypothesis:} h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}} \]
\[ \textrm{Modify Linear regression:} J(\theta) = \frac{1}{m}
\sum\limits_{i=1}^m Cost (h_\theta(x^{(i)}, y^{(i)}) \]
\[ \textrm{where } Cost (h_\theta(x^{(i)}, y^{(i)}) =
\frac{1}{2}(h_\theta(x^{(i)}) - y^{(i)})^2 \]

Logistic cost function is ``non-convex'', which means it can have a
number of local minimums, as a result we can not work with derivative.
Instead, we use iteration method based on \[ \textrm{hypothesis:}
Cost(h_\theta(x), y) = \left\{ \begin{array}
    {rr} -log(h_\theta(x)) & \textrm{if y = 1} \\
    -log(1- h_\theta(x)) & \textrm{if y = 0}
  \end{array} \right. \]

\paragraph{for y=1:}
Cost = 0 if y = 1, $h_\theta(x) = 1$
but as $h_\theta(x) \to 0 \Rightarrow \textrm{Cost } \to \infty $ \\
That captures intuition that if $h_\theta(x) = 0$, predict
$P(y=1|x;\theta) = 0$, but for y=1 we'll penaltize algorithm by a very
large cost.

\paragraph{for y=0:}
Cost = 0 for $h_\theta(x) = 0$, penalty for $h_\theta(x) = 1$

Together it gives convex function suitable for iterative processing.

\subsection{Gradient Descent}
\label{sec:6-5}
\[\textrm{minimising } J(\theta) = \frac{1}{m} \sum\limits_{i=1}^m
Cost (h_\theta(x^{(i)}, y^{(i)}) \] Initial Cost function is being
re-written as \[ Cost(h_\theta(x), y) = -ylog(h_\theta(x)) -
(1-y)log(1-h_\theta(x))
\]

Then J can be expressed as \[ J(\theta) = -\frac{1}{m} \left[
  \sum\limits_{i=1}^m y^{(i)} \log (h_\theta(x^{(i)})) + (1-y^{(i)})
  \log (1 - h_\theta(x^{(i)})) \right] \]

To make a prediction given new x: \[ \textrm{Output } h_\theta(x) =
\frac{1}{1 + e^{-\theta^Tx}} \quad \textrm{which means probability :}
p(y=1|x; \theta)
\]

\paragraph{Gradient Descent}
Calculating $\min_\theta J (\theta)$: \\
\[ \begin{array}{l}
  \textrm{Repeat }  \{ \\
  \theta_j := \theta_j - \alpha \frac {\delta}{\delta \theta} J
  (\theta) \\
  \textrm{where } \frac {\delta}{\delta \theta} J(\theta) =
  \frac{1}{m} \sum\limits_{i=1}^m (h_\theta(x^{(i)}) -
  y^{(i)})x_j^{(i)} \textrm{ -
    partial derivative} \\
  \textrm{so } \theta_j := \theta_j - \alpha \sum\limits_{i=1}^m
  (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\
  \}
\end{array} \]
{\bf simultaneously update all $\theta_j$} - looks exactly like the Linear
Regression algorithm (just hypothesis $h_\theta(x)$ is different), so
implementation can use both loop from 0 to m or vector form as well as
feature scaling idea.

\subsection{Optimisation}
\label{sec:6-6}
The idea is to play with different control algorithms (Gradient
Descent, Conjugate Descent, BFGS, L-BFGS) supplying them ways to
compute $J(\theta)$ and $\frac{\delta}{\delta \theta_j}J(\theta)$
(through some sort of plug-ins - ?).

The last three algorithms are more complex then the ``normal''
Gradient descent but:
\begin{itemize}
\item No need to manually pick $\alpha$ (learning rate)
\item Often faster then ``normal'' gradient descent
\end{itemize}

In Octave, this is implemented through generalised function and
function reference (delegate ?):

Example: \[\theta = \left[ \begin{array}{c} \theta_1 \\ \theta_2
  \end{array} \right]; J(\theta) = (\theta_1 - 5)^2 + (\theta_2- 5)^2 \]
\[\textrm{then } \frac{\delta}{\delta \theta_1}J(\theta) = 2(\theta_1
-5) ;\frac{\delta}{\delta \theta_2}J(\theta) = 2(\theta_2 - 5)
\]
in Octave:
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  function [jVal, gradient] = costFunction(theta) jVal = (theta(1) -
  5)^2 + (theta(2)-5)^2; gradient = zeros(2,1); gradient(1) = 2 *
  (theta(1) - 5); gradient(2) = 2 * (theta(2) - 5);
\end{lstlisting}
then
\begin{lstlisting} [language=Octave, caption=={Algorithm Call }]
  % set up process options
  options = optimset('GradObj', 'on', 'MaxIter', '100'); initialTheta
  = zeros(2, 1); [optTheta, functionVal, exitFlag] =
  fminunc(@costFunction, initialTheta, options);
\end{lstlisting}
This returns optTheta (value of $\theta$ for optimal point),
fuctionVal - cost value in optimal point, and exitFlag (1 for normally
converged operation).

Works for $\theta \in \mathbf{R^d}; d \geq 2$

\subsection{Mutliclass Classification}
\label{sec:6-7}
When you need to classify by more then 2 categories (as in Binary
Classification).

Idea is called ``One-vs-All'' or ``One-vs-Rest''. Build classifiers
[$h^{(1)}(x), h^{(2)}(x), h^{(3)}(x)... $] for each class: \[
h^{(i)}(x) = P(y = i|x;\theta) \ (i = 1, 2, 3)
\]
On a new input x, to make a prediction, pick the class i that
maximises $\max_i h^{(i)}(x)$

\section{Regularisation}
\label{sec:7}

\subsection{The Problem of Overfitting}
\label{sec:7-1}
The ``underfitting' (or ``high bias'') problem takes place when we try
to fit, say, linear model to the task which is rather quadratic - so
we have too strong pre-conception to the idea of the linear model.

Over-fitting (``high variance'') is a vise-verse state: for instance,
we apply the model of power 4 to the problem which would be fine
described by quadratic model. The graph fits to the training data
really well, but the curve itself becomes too ``windy''.

So, if we have too many features, the learned hypothesis may fit the
training set very well ($J(\theta) = \frac{1}{2m} \sum
\limits{i=1}{m}(h_\theta(c^{(i)}) - y^{(i)})^2 \approx 0$) but fail to
generalise to new examples (predict output on new examples). Similar
story - with Logistic Regression.

Addressing over-fitting:
\begin{enumerate}
\item Reduce number of features
  \begin{itemize}
  \item Manually select which features to keep
  \item Model selection algorithm
  \end{itemize}
\item Regularisation
  \begin{itemize}
  \item Keep all the features, but reduce magnitude/values of
    parameters $\theta_j$
  \item Works well when we have a lot of features, each of which
    contributes a bit to predicting y.
  \end{itemize}
\end{enumerate}

\subsection{Cost Function}
\label{sec:7-2}
The idea is to have small values high-order parameters. This will make
our hypothesis simpler and less prone to over-fitting. This is done
through ``penalising'' values in our cost function (keep in mind that
for our case $x^n$ is equivalent to $\theta_n$ so we can use the same
idea for both multi-feature training set and for high-order polynom:

\[J(\theta) = \frac{1}{2m} \left[ \sum \limits{i=1}{m}
  (h_\theta(x^{(i)}) - y^{(i)})^2 + \underbrace{ \lambda \sum
    \limits_{i=1}^{n}\theta_j^2}_{penalty} \right] \] Notice the
regularisation term (a ``penalty'') does not include $\theta_0$ - this
is a sort of convention. $\lambda$ is called ``regularisation
parameter''; it provides balance between two tasks:
\begin{itemize}
\item fitting the training set well
\item to keep the parameter values small
\end{itemize}

If $\lambda$ is too high, all $\theta_i$ end up near 0, so the
hypothesis becomes too flat - i.e. ``under-fit'': $h_\theta(x) =
\theta_0$.

\subsection{Regularised Linear Regression}
\label{sec:7-3}
For ``updated'' Linear Regression (with regularisation parameter), the
Gradient Descent looks a bit different:
\[ \begin{array}{l}
  \textrm{Repeat }  \{ \\
  \theta_0 := \theta_0 - \alpha \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_0^{(i)} \\
  \theta_j := \theta_j - \alpha \left[ \frac {1}{m} \sum
    \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
    \underbrace{+
      \frac{\lambda}{m}\theta_j} _{penalty} \right] \\
  (j= 1, 2, 3, \dots , n) \textrm{ - no zero here!} \\
  \}
\end{array}
\]

- actually, the good old partial derivative on regularised hypothesis.
After some transformations: \[ \theta_j := \theta_j(1 -
\alpha\frac{\lambda}{m}) - \alpha \frac{1}{m} \sum
\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
\]
and $1 - \alpha \frac{\lambda}{m} < 1$, usually about 0.99 (roughly).
Then first part looks like $\theta_j * 0.99$. While second part is a
``normal'' regression update, each step of the algorithm reduces
$\theta_j$.

\subsubsection{Normal Equation}
\label{sec:7-2-2}
\begin{equation*}
  \begin{array}{ll}
    \mathbf{X} = \left[ \begin{array}{c} (x^{(1)})^T \\ (x^{(2)})^T \\
        \vdots \\ (x^{(m)})^T \end{array} \right] & y =
    \left[ \begin{array}{c} y^{(1)}  \\ y^{(2)}  \\  \vdots \\ y^{(m)} \end{array} \right]
  \end{array}
\end{equation*}
where X is $M \times (n+1)$ matrix, y is vector of length m. To
minimise cost function $J(\theta)$: \[ \frac{\delta}{\delta
  \theta_j}J(\theta) \Rightarrow 0 \]
\[
\theta = \Big(X^T X + \lambda \left[ \begin{array}{ccccc} 0 & 0 & 0 &
    \dots & 0 \\ 0 & 1 & 0 & \dots & 0 \\ 0 & 0 & 1 & \dots & 0 \\
    \vdots & \vdots & \vdots & \ddots & 0 \\ 0 & 0 & 0 & \dots &
    1 \end{array} \right] \Big)^{-1} X^Ty
\]
This terrible matrix is of $(n+1) \times (n+1)$ size.

\paragraph{Non-invertibility}
Suppose $m \leq n$, for our $\theta = (X^T X)^{-1}X^Ty$, if $\lambda >
0$:

Then $(X^T X)$ is non-invertible (singular). ``pinv'' function could
still give some sane result, but, if working with different language
(or using regular inverse - inv),this would not work.

It is possible to prove that sum of $(X^T X)$ and this ugly matrix is
invertible (non-singular) for $\lambda > 0$

E.g. Regularisation helps in this pathetic case.

\subsection{Regularised Logistic Regression}
\label{sec:7-4}
Regularised cost function; \[ J(\theta) = -\left[ \frac{1}{m} \sum
  \limits_{i=1}^{m} y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)})
  \log (1 - h_\theta(x^{(i)})) \right] \underbrace{+
  \frac{\lambda}{2m} \sum \limits_{j=1}^{n} \theta_j^2} _{penalty}
\]

Cosmetically, the algorithm looks the same:
\[ \begin{array}{l}
  \textrm{Repeat }  \{ \\
  \theta_0 := \theta_0 - \alpha \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_0^{(i)} \\
  \theta_j := \theta_j - \alpha \underbrace{ \left[ \frac {1}{m} \sum
      \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
      \underbrace{+ \frac{\lambda}{m}\theta_j } _{penalty}
    \right]}_{\frac{\delta}{\delta \theta_j}J(\theta)} \\
  (j= 1, 2, 3, \dots , n) \textrm{ - no zero here!} \\
  \}
\end{array}
\]
where hypothesis is a bit different: $h_\theta(x) = \frac{1}{1 +
  e^{-\theta^Tx}}$

\subsubsection{Advanced Optimisation}
\label{sec:7-4-2}
In Octave:
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  function [jVal, gradient] = costFunction(theta) jVal = [code to
  compute |$J(\theta)$|]; |\[\textrm{becomes } J(\theta) = \left[
    -\frac{1}{m} \sum \limits_{i=1}^{m} y^{(i)} \log h_\theta(x^{(i)})
    + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)})) \right] +
  \frac{\lambda}{2m} \sum \limits_{j=1}^{n} \theta_j^2 \]| gradient(1)
  = [code to compute |$\frac{\delta}{\delta \theta_0} J(\theta)$|];
  |\[\frac {1}{m} \sum \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})
  x_0^{(i)} \]| gradient(2) = [code to compute |$\frac{\delta}{\delta
    \theta_1} J(\theta)$|]; |\[ \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_1^{(i)} +
  \frac{\lambda}{m}\theta_1 \]| gradient(3) = [code to compute
  |$\frac{\delta}{\delta \theta_2} J(\theta)$|]; |\[ \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_2^{(i)} +
  \frac{\lambda}{m}\theta_2 \]| ..... gradient(n+1) = [code to compute
  |$\frac{\delta}{\delta \theta_n} J(\theta)$|]; |\[ \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_n^{(i)} +
  \frac{\lambda}{m}\theta_n \]|
\end{lstlisting}

\section{Neural Networks: Representation}
\label{sec:8}
- to cope with the problems of too large feature sets. - to give ``one
more'' approach to the learning problem

\subsection{Neurons and Brain}
\label{sec:8-2}
The ``one learning algorithm'' hypothesis was quite popular 80-90s.
The idea was to re-use the ``almost universal'' algorithms to ``almost
any'' sensors.

\subsection{Model Representation}
\label{sec:8-3}
Simulating neurons in the brain:
\begin{itemize}
\item ``Input'' wires - dendrites
\item ``Output'' wire - axon
\item dendrite on one neuron is connected to the axon on another. The
  neuron itself makes some ``computations'' when passing signal.
\end{itemize}

Neuron model represents a neuron as a logistic unit with a number of
inputs and single output function (hypothesis) like $ h_\theta(x) $
like: \[ x = \left[ \begin{array}{c} x_0 \\ x_1 \\ x_2 \\
    x_3 \end{array} \right];
\theta = \left[ \begin{array}{c}  \theta_0 \\ \theta_1 \\ \theta_2 \\
    \theta_3 \end{array} \right] \textrm{(sometimaes called
  "weights")} \] and $h_\theta(x)=\frac{1}{1 + e^{-\theta^Tx}}$ is
called ``Sigmoid (logistic) activation function''. Sometimes $x_0$ is
referred as ``bias unit'' (``bias neuron'' on the diagram). It is
always equal to 1.

\subsubsection{Neural Network}
The Neural Network is a group of neurons like the one on a picture.

Layer 1 neurons are ``input neurons''; intermediate (Layer 2) is also
called ``the hidden layer'' (contains $a_0$ - the ``bias unit'' ; the
final one (Layer 3) - ``output layer''. Notations:
\begin{itemize}
\item {$a_j^{(j)}$} = ``activation'' of unit i in layer j
\item {$\theta^{(j)}$} = matrix of weights controlling function
  mapping from layer j to layer j + 1.
\end{itemize}
Example: for network with three input units and three hidden ones
(assuming g(z) is the sigmoid function):
\begin{equation}
  \label{eq:8-4}
  \begin{array}[c]{c}
    a_1^{(2)} = g(\theta_{10}^{(1)}x_0 + \theta_{11}^{(1)}x_1 +
    \theta_{12}^{(1)}x_2 + \theta_{13}^{(1)}x_3)  \\
    a_2^{(2)} = g(\theta_{20}^{(1)}x_0 + \theta_{21}^{(1)}x_1 +
    \theta_{22}^{(1)}x_2 + \theta_{23}^{(1)}x_3)  \\
    a_3^{(2)} = g(\underbrace{\theta_{30}^{(1)}x_0 + \theta_{31}^{(1)}x_1 +
      \theta_{32}^{(1)}x_2 + \theta_{33}^{(1)}x_3}_{z_3^{(2)}})
  \end{array}
\end{equation}
Finally,
\[ h_\theta(x) = a_1^{(3)} = g(\theta_{10}^{(2)}a_0^{(2)} +
\theta_{11}^{(2)}a_1^{(2)} + \theta_{12}^{(2)}a_2^{(2)} +
\theta_{13}^{(2)}a_3^{(2)} )\]
Dimensions: \\
If network has $s_j$ units in layer j, $s_{j+1}$ units in layer j+1,
then $\theta^{(j)}$ will be of dimension $s_{j+1} \times (s_j + 1)$.
\\
The whole hypothesis looks like $h_\theta (X)$ (in vectors!) -
``parametrized'' by $\theta$
\label{8-4}
 
\subsubsection{Calcualtion and Optimisation}
Terminology: The $\theta_{10}^{(1)}x_0 + \theta_{11}^{(1)}x_1 +
\theta_{12}^{(1)}x_2 + \theta_{13}^{(1)}x_3$ part is represented as
$z_1^{(2)}$ so that $a_1^{(2)} = g(z_1^{(2)})$.
% \label {8-4: 02:01}
The whole set of $z_1^{(i)}$ from \ref{eq:8-4} can be written as
$\theta^{(1)} \times x$

Further vectorization (with $z_0^{(2)} = 1$ always):
\begin{equation*}
  \label{eq:8-4:02:25}
  \begin{array}[c]{l}
    x = \left[  \begin{array}[c]{c}   x_0 \\ x_1 \\ x_2 \\ x_3 \end{array} \right]
    z^{(2)} = \left[\begin{array}[c]{c} z_1^{(2)} \\ z_2^{(2)}
        \\ z_3^{(2)} \end{array} \right] 
    \\ \\    z^{(2)} = \theta^{(1)}x 
    \\    a^{(2)} = g(z^{(2)})
  \end{array}
\end{equation*}
- both $a^{(2)}$ and $z^{(2)}$ are $\mathbf{R}^3$

Then , as $x = a^{(1)}$ (activation for the first layer), we can
generalise: \[ z^{(2)} = \theta^{(1)} a^{(1)}\]

With bias unit: $a_0^{(2)} = 1 \Rightarrow a^{(2)} \in
\mathbf{R}^4$: \[ z^{(3)} = \theta^{(2)}a^{(2)} \]
\[ h_\theta(x) = a^{(3)} = g(z^{(3)})\]

The whole story is called {\bf Forward Propagation}. At each level,
the process is our good old Logistic Regression, ``learning its own
features'', ergo, we can play with different architectures instead of
playing with high - order features as in Logistic Regression.

\subsection{Examples and Intuition}
\label{sec:8-5}
Exotic example 1: hypothesis like $y = x_1 \textrm{ XOR } x_2$ or $y =
x_1 \textrm{ NXOR } x_2$

Simple example: AND:
\[ x_1, x_2 \in \{0, 1\}\] \[ y = x_1 \textrm{ AND } x_2 \] The whole
network consists of 3 input parameters ($x_1, x_2$ and bias unit
``+1'') and single hidden unit (single-neuron network). We could solve
the task by assigning: $ \theta = [-30, +20, +20]$ - that is,
$h_\theta(x0) = g(-30 + 20 x_1 + 20 x_2)$

OR function is built similarly: $ \theta = [-10, +20, +20]$

Negation (NOT): $ \theta = [+10, -20]$ (2 input parameters in total)

Returning to $x_1 \textrm{ XNOR } x_2 = (x_1 \textrm{ AND } x_2)
\textrm{ OR } ((\textrm{NOT } x_1) \textrm{ AND } (\textrm{NOT }
x_2))$:
\[
\begin{array}{l}
  a_1^{(2)} = (-30 + 20 x_1 + 20 x_2) \\  
  a_2^{(2)} = (+10 - 20 x_1 - 20 x_2)  \\
  \textrm{ level 3: } a_1^{(3)} = (-10 + 20 a_1^{(2)} + 20 a_2^{(2)})
  \Rightarrow h_\theta(x)
\end{array}
\]
\label{8-6}

\subsection{Multiclass Classification}
\label{sec:8-7}
Output layer consists of more then one unit: n units each representing
probability of relevant output. As such, training examples will
be \[(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), (x^{(3)}, y^{(3)}), \dots
(x^{(m)}, y^{(m)}) \], where $x^{(i)}$ are images, and each $y^{(i)}$
is one of \[ \begin{array}{cccc} \left[ \begin{array}{c} 1 \\ 0 \\ 0
      \\ 0 \end{array} \right], & \left[ \begin{array}{c} 0 \\ 1 \\ 0
      \\ 0 \end{array} \right], & \left[ \begin{array}{c} 0 \\ 0 \\ 1
      \\ 0 \end{array} \right], & \left[ \begin{array}{c} 0 \\ 0 \\ 0
      \\ 1 \end{array} \right]
\end{array}\]
and use ``one-vs-all'' approach to training


\section{Neural Networks: Learning}
\label {sec:9}

\subsection{Cost Function}
\label{sec:9-1}
Notation:
\[ \{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}) \dots (x^{(m)}, y^{(m)})
\} \textrm{ - samples} \]
\[ \mathbf{L} \textrm{ = total no of layers in network} \]
\[ s_l \textrm{ = no of units (not counting bias unit in layer l}\]

We have 2 variants of the classification problem:

\subsubsection{Binary Classification}
y = 0 or 1; 1 output unit; heuristic will be: the single real number`;
$h_\theta(x) \in \mathbf{R}$, number of output units $S_L = 1$

\subsubsection{Multi-class Classification (K classification)}
\label{sec:9-1}
K output units
\[y \in \mathbf{R^K} \textrm{ e.g.} \begin{array}{cccc}
  \left[ \begin{array}{c} 1 \\ 0 \\ 0 \\ 0 \end{array} \right], &
  \left[ \begin{array}{c} 0 \\ 1 \\ 0 \\ 0 \end{array} \right], &
  \left[ \begin{array}{c} 0 \\ 0 \\ 1 \\ 0 \end{array} \right], &
  \left[ \begin{array}{c} 0 \\ 0 \\ 0
      \\ 1 \end{array} \right] \\
  pedesterian & car & motorcycle & truck
\end{array} \]
$h_\theta(x) \in \mathbf{R^K}$ and $S_L = K, K \geq 3$.  

The concrete cost function is generalisation of logistic regression:

\[ J(\theta) = -\frac{1}{m} \left[\sum \limits_{i=1}^{m} y^{(i)} \log
  h_\theta(x^{(i)}) + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)}))
\right] + \frac{\lambda}{2m} \sum \limits_{j=1}^{n} \theta_j^2 \]

instead of single $y^{(i)}$ we now have a vector of K such elements,
so, for $h_\theta(x) \in \mathbf{R^K} \textrm{ and output }
(h_\theta(x))_i = i^{th}$ :

\[ J(\theta) = - \frac{1}{m} \left[ \sum \limits_{i=1}^{m} \sum
  \limits_{k=1}^{K} y_k^{(i)} \log (h_\theta(x^{(i)})_k) + (1 -
  y_k^{(i)}) \log (1 - (h_\theta(x^{(i)}))_k) \right] +
\frac{\lambda}{2m} \sum \limits_{l=1}^{L-1} \sum \limits_{i=1}^{s_l}
\sum \limits_{j=1}^{s_{l+1}} (\theta_{ji}^{(l)})^2 \]
 
For binary classification (binary output) K = 1. Regularisation term
does NOT include the bias terms $\theta_{i0}x_0$

\subsection{Backpropagation Algorithm}
\label{sec:9-2}
To minimise Cost function we need a code to compute $J(\theta)$ and
$-\frac{\delta}{\delta \theta^{(l)}_{ij}} J(\theta)$

Intuition: $\delta_j^{(l)}$ = ``error'' of node $j$ in layer $l$.

For each output layer (layer L = 4) $ \delta_j^{(4)} =
\underbrace{a_j^{(4)}}_{(h_\theta(x))_j} - y_j$, in vector form: $\vec
\delta^{(4)} = \vec a^{(4)} - \vec y$

Further:
\[\delta^{(3)} = (\theta^{(3)})^T \delta^{(4)} .*
\underbrace{g'(Z^{(3)})}_{a^{(3)} .* (1-a^{(3)})} \] where .* -
element-wise multiplication; $a^{(3)}$ is activation vector at level
3.

Similarly:
\[\delta^{(2)} = (\theta^{(2)})^T \delta^{(3)} .* g'(Z^{(2)}) \]
no $\delta^{(1)}$ term, because the first layer corresponds to
features observed in training set.

The term {\bf back Propagation} comes from the fact that initially we
calculate $\delta^{(4)}$, then use it for calculating $\delta^{(3)}$ -
so we ``back-propagating'' the error from high level to level 1.

After quite complex derivation: \[\frac{\delta}{\delta
  \theta_{ij}^{(l)}} J(\theta) = a_j^{(l)}\delta_i^{(l+1)} \]
(ignoring $\lambda$ - regularisation!)

So, the algorithm itself:

Get training set $\{(x^{(1)}, y^{(1)}), \dots, (x^{(m)}, y^{(m)})\}$

Set $\Delta_{ij}^{(l)} = 0$ (for all $l, i, j$). This will be used to
compute the partial derivatives.
\begin{tabbing}
  For\quad \=$i$ = 1 to $m$  \\
  \>Set $a^{(1)} = x{(i)}$ \\
  \>Perform forward propagation to compute $a^{(l)}$ for $l = 2, 3,
  \dots, L $ \\
  \>Using $y^{(i)}$, compute $\delta^{(L)} = a^{(L)} - y^{(i)}$ \\
  \>Compute $\delta^{(L-1)}, \delta^{(L-2)}, \dots, \delta^{(2)}$ \\
  \>$\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + a_j^{(l)}
  \delta_i^{(l+1)}$ - possible to vectorise as $\Delta^{(l)} :=
  \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$ \\
\end{tabbing}   
After the loop:  \\
$[\mathbf{D}_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)} + \lambda
\theta_{ij}^{(l)} \textrm{ if }j \ne 0$ \\
$\mathbf{D}_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)} \textrm{ if }j
= 0$ \\

finally: $\frac{\delta}{\delta \theta_{ij}^{(l)}} J(\theta) =
\mathbf{D}_{ij}^{(l)}$

\subsection{Backpropagation Intuition}
\label{sec:9-3}

The idea is that $\delta_j^{(l)}$ is an ``error'' of cost for
$a_j^{(l)}$ (unit $j$ in layer $l$). Formally, $\delta_j^{(l)} =
\frac{\delta}{\delta z_j^{(l)}} \, cost(i)$ where $cost(i) = y^{(i)}
\log h_\theta(x^{(i)}) + (1 - y^{(i)}) \log h_\theta(x^{(i)})$

\subsection{Implementation: Unrolling Parameters}
\label{sec:9-4}
Traditional ``Advanced optimisation'' way s built using vectors for
weight values and gradients. In the neural networks we use{\bf
  matrices} for $\theta^{(l)}$ and $D^{(l)}$ - so need to ``unroll''
these into vectors.

Example: 10 parameters, 1 hidden layer (10 units), 1 output unit:
\[ s_1 = 10, s_2 = 10, s_3 = 1\]
\[ \Theta^{(1)} \in \mathbf{R}^{10\times11}, \Theta^{(2)} \in
\mathbf{R}^{10\times11}, \Theta^{(2)} \in \mathbf{R}^{1\times 11} \]
\[ D^{(1)} \in \mathbf{R}^{10\times11}, D^{(2)} \in
\mathbf{R}^{10\times11}, D^{(2)} \in \mathbf{R}^{1\times 11} \] In
Octave:
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  thetaVec = [ Theta1(:); Theta2 (:); Theta3(:)]; DVec = [D1(:);
  D2(:); D3(:) ];

  Theta1 = reshape(thetaVec(1:110), 10, 11); Theta2 =
  reshape(thetaVec(111:220), 10, 11); Theta3 =
  reshape(thetaVec(221:231), 1, 11);
\end{lstlisting}

\subsubsection{Learning Algorithm}
Have initial parameters $\Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}$. \\
Unroll to get {\it initialTheta} to pass to
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  fminunc(@costFunction,initialTheta,options)
\end{lstlisting}

Now for {\it costFunction}:
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  function [jval, gradientVec] = costFunction(thetaVec)
\end{lstlisting}
\begin{tabbing}
  \quad \quad \= From {\it thetaVec}, get $\Theta^{(1)}, \Theta^{(2)},
  \Theta^{(3)}$ (use {\it reshape}). \\
  \>Use forward prop/back prop to compute $D^{(1)}, D^{(2)}, D^{(3)}$
  and $J(\Theta)$. \\
  \>Unroll  $D^{(1)}, D^{(2)}, D^{(3)}$ to get {\it gradientVec}\\
\end{tabbing}

\subsection{Gradient Checking}
\label{sec:9-5}
This is an idea to eliminate possible bugs in back prop
implementation. We calculate some sort of ``derivative approximation''
in $\Theta$ using small value $\epsilon$ to calculate approximation:
\[ \frac{\delta}{\delta \Theta} J(\Theta) \approx \frac{J(\Theta +
  \epsilon) - J(\Theta - \epsilon)} { 2 \epsilon} \]

Usually value for $\epsilon$ is quite small: $\epsilon \approx
10^{-4}$ (but beware of too small value - this can case the numerical
problem).

Implementation:
\begin{lstlisting}[language=Octave]
  gradApprox = (J(theta + EPSILON) - J(theta - EPSILON)) / (2*EPSILON)
\end{lstlisting}
% 9-5 05:00
For general case (parameter vector $\theta$ can even be unrolled
version of $\Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}$, so that $\Theta
\in \mathbf{R}^n$ and $\theta = \left[\theta_1, \theta_2, \theta_3,
  \dots, \theta_n \right]$. Then:
\[ \frac{\delta}{\delta \theta_1} J(\theta) \approx \frac{J(\theta_1 +
  \epsilon, \theta_2, \theta_3, \dots, \theta_n) - J(\theta_1 -
  \epsilon, \theta_2, \theta_3, \dots, \theta_n)}{2 \epsilon} \]
\[ \frac{\delta}{\delta \theta_1} J(\theta) \approx \frac{J(\theta_1,
  \theta_2 + \epsilon, \theta_3, \dots, \theta_n) - J(\theta_1,
  \theta_2 - \epsilon, \theta_3, \dots, \theta_n)}{2 \epsilon} \]
\[ \frac{\delta}{\delta \theta_1} J(\theta) \approx \frac{J(\theta_1,
  \theta_2, \theta_3 + \epsilon, \dots, \theta_n) - J(\theta_1,
  \theta_2, \theta_3 - \epsilon, \dots, \theta_n)}{2 \epsilon} \]
\[ \vdots \]
\[ \frac{\delta}{\delta \theta_n} J(\theta) \approx \frac{J(\theta_1,
  \theta_2, \theta_3, \dots, \theta_n + \epsilon) - J(\theta_1,
  \theta_2, \theta_3, \dots, \theta_n - \epsilon)}{2 \epsilon} \] in
Octave:
\begin{lstlisting}[language=Octave]
  for i = 1 : n, thetaPlus = theta; thetaPlus(i) = thetaPlus(i) +
  EPSILON; thetaMinus = theta; thetaMinus(i) = thetaMinus(i) -
  EPSILON; gradApprox(i) = (J(thetaPlus) - J(thetaMinus)) /
  (2*EPSILON); end;
\end{lstlisting}

The criteria: check that $gradApprox \approx \underbrace{
  DVec}_{\textrm{derivatives from back prop}}$. The idea is to compare
the numerically obtained value for derivatives with formulated ones.

\subsubsection{Implementation Node}
\begin{itemize}
\item Implement backprop to compute {\it DVec} (unrolled $D^{(1)},
  D^{(2)}, D^{(3)}$).
\item Implement numerical gradient check to compute {\it gradApprox}.
\item Make sure they give similar values.
\item Turn off gradient checking. Use backprop code for learning.
\end{itemize}

\paragraph{Important.}
Be sure to disable the gradient checking code before training
classifier!

\subsection{Random Initialisation}
\label{sec:9-6}
For gradient descent and advanced optimisation method, need initial
value for $\Theta$:
\begin{lstlisting}[language=Octave]
  otpTheta = fminunc(@costFunciton, initialTheta, options)
\end{lstlisting}
Consider gradient descent: Set {\it initialTheta = zeros(n, 1)}? This
does not work for neural network as $\Theta_{ij}^{(l)} = 0$ for all
$i, j, l$ and $a_1^{(2)} = a_2^{(2)}$, also $\delta_1^{(2)} =
\delta_2^{(2)}$ and partial derivatives are equal to each other. After
each update, parameters corresponding to inputs going into each of two
hidden units are identical. As a result, all hidden units are
computing the same feature (the same function from the input).

\subsubsection{Random Initialisation: Symmetry breaking}
\label{sec:9-6-1}
Initialise each $\Theta_{ij}^{(l)}$ to a random value in $[ -\epsilon,
\epsilon ]$ so that $-\epsilon \leq \Theta_{ij}^{(l)} \leq \epsilon$.
\\
E.g.
\begin{lstlisting}[language=Octave]
  Theta1 = rand(10, 11) * (2*INIT_EPSILON) - INIT_EPSILON; Theta2 =
  rand(1, 11) * (2*INIT_EPSILON) - INIT_EPSILON;
\end{lstlisting}

\subsection{Putting it Together}
\label{sec:9-7}
\begin{itemize}
\item Pick a network architecture:\\
  No of input units: Dimension of features $x^{(i)}$ \\
  No of output units: Number of classes \\
  Reasonable default: 1 hidden layer, or if >1 hidden layer, have same
  no. of hidden units in every layer (usually the more the better, but
  this will be computation expensive).
\item Train a neural network
  \begin{enumerate}
  \item Randomly initialise weights
  \item Implement forward propagation to get $h_\Theta(x^{(i)})$ for
    any $x^{(i)}$
  \item Implement code to compute cost function $J(\Theta)$
  \item Implement back prop to compute partial derivatives
    $\frac{\delta}{\delta\Theta_{jk}^{(l)}} J(\Theta)$
  \end{enumerate}
  \begin{lstlisting}[language=Octave]
    for i = 1:m
  \end{lstlisting}
  \begin{tabbing}
    \quad \quad \quad \quad \= Perform forward propagation and back propagation using
    example ($x^{(i)}, y^{(i)}$)  \\
    \>Get activations $a^{(l)}$ and delta terms $\delta^{(l)}$ for $l=2,
    \dots, L$)\\
    \>Compute accumulation terms $\Delta^{(l)} := \Delta^{(l)} +
    \delta^{(li)}(a^{(l)})^T$
  \end{tabbing} 
  Compute $\frac{\delta}{\delta \Theta_{jk}^{(l)}} J(\Theta)$ (outside
  loop on 1:m)
\item Training a neural network
  \begin{enumerate}
  \item Use gradient checking to compare $\frac{\delta}{\delta
      \Theta_{jk}^{(l)}} J(\Theta)$ computed using back propagation
    vs. using numerical estimate of gradient of $J(\Theta)$. \\
    Then disable gradient checking code.
  \item Use gradient descent or advanced optimisation method with back
    propagation to try to minimise $J(\Theta)$ as a function of
    parameters $\Theta$
  \end{enumerate}
\end{itemize}

BTW, for neural networks, the function $J(\Theta)$ is non-convex so it
is possible it will have the local minimum (at least in theory). In
practice, it usually diverges quite OK.

\subsection{Automonous Driving}
\label{sec:9-8}
Баловство одно. But the idea of using more then one training set and
selecting the most ``confident'' output worth noting.


\section{Advice for Applying ML}
\label{sec:10}
Imagine we're debugging a regularised linear regression which predicts
the house prices:
\[J(\Theta) = \frac{1}{2m} \left[ \sum\limits_{i=1}^{m}
  (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum
  \limits_{i=1}^{n}\theta_j^2 \right] \] But, on new set of houses, we
get unacceptable large errors in predictions. Oops. Possible variants;
\begin{itemize}
\item get more training examples - works sometimes (we'll see when)
\item try smaller set of features (to avoid over- fitting)
\item try getting additional features
\item adding polynomial features
\item try decreasing / increasing $\lambda$;
\end{itemize}
The idea is to filter out the approaches which are not going to work:

\subsubsection{ML diagnostic}
Diagnostic: A test that you can run to gain insight what is/isn't
working with a learning algorithm, and gain guidance as to how best to
improve its performance.

Sometimes takes time to implement, but usually it is a good spent
time.

\subsection{Evaluating a Hypothesis}
\label{sec:10-2}
Idea is to check how hypothesis adapts the new data. We split the data
into two portions; first (near 70\%) used as the training set; the
second is the test set ($m_{test}$).

It is better to select testing set randomly (to avoid any ordering
data effects).

\paragraph{Training / test procedures for linear regression}
\label{sec:10-2-1}
\begin{itemize}
\item Learn parameter $\theta$ from training data (minimising training
  error $J(\theta)$).
\item Compute test set error:
  \[J_{test}(\theta) = \frac{1}{2m_{test}}\sum
  \limits_{i=1}^{m_{test}} \left(h_\theta(x_{test}^{(i)}) -
    y_{test}^{(i)} \right)^2 \]
\end{itemize}

\paragraph{Training / test procedures for logistic regression}
\label{sec:10-2-2}
\begin{itemize}
\item Learn parameter $\theta$ from training data.
\item Compute test set error:
  \[J_{test}(\theta) = \frac{1}{2m_{test}}\sum
  \limits_{i=1}^{m_{test}} y_{test}^{(i)} \log
  h_\theta(x_{test}^{(i)}) + (1 - y_{test}^{(i)}) \log (1 - h_\theta
  (x_{test}^{(i)})) \]
\item Miss- classification error (0/1 misclassification error): often
  used fr evaluation: \[ err(h_\theta^{(c)}, y) =
  \left\{ \begin{array}
      {lll} 1 & \textrm{ if } h_\theta(x) \leq 0.5, & y = 0  \\
      & \textrm{ or } h_\theta(x) < 0.5, & y = 1  \\
      0 & \textrm{ otherwise } &
    \end{array} \right.\]
  Test error = $\frac{1}{m_{test}} \sum \limits_{i=1}^{m_{test}}
  err(h_\theta(x_{test}^{(i)}), y_{test}^{(i)})$.
\end{itemize}

\subsection{Model Selection and Train Validation Test Sets}
\label{sec:10-3}

\subsubsection{Model Selection}
Introduce a parameter d (a ``degree of polynomial'') which is, well,
the max power of x in hypothesis. We can compute the relevant
$\Theta^{(i)}$ and corresponding test errors:
\[ d = 1 \rightarrow h_\theta(x) = \theta_0 + \theta_1x \rightarrow
\Theta^{(1)} \rightarrow J_{test}(\Theta^{(1)}) \]
\[ d = 1 \rightarrow h_\theta(x) = \theta_0 + \theta_1x + \theta_2x^2
\rightarrow \Theta^{(2)} \rightarrow J_{test}(\Theta^{(2)}) \]
\[ \vdots\]

\[ d = 1 \rightarrow h_\theta(x) = \theta_0 + \theta_1x \rightarrow +
\theta_2x^2 + \dots + \theta_{10}x^{10} \rightarrow \Theta^{(10)}
\rightarrow J_{test}(\Theta^{(10)}) \]

Now we can take the bets performing (i.e. returning minimal
$J_{test}$). Assume for our example we have the 5th order polynomial.
We could report $J_{test}$ as estimated but this is likely to be an
optimistic estimate of generalization error, i.e. our extra parameter
(d = degree of polynomial) is fit to test set (in other words, we
optimised the d value as a model parameter on the test set).

So we divide the data on {\bf three} parts:
\begin{itemize}
\item training set (about 60\%)
\item cross validation set (about 20\%) values are
  $(x_{cv}^{(i)},y_{cv}^{(i)})$
\item test set (20\%)
\end{itemize}
The we calculate: Training error:
\[ J_{training}(\theta) = \frac{1}{2m} \sum
\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 \] Cross Validation
error:
\[ J_{cv}(\theta) = \frac{1}{2m_{cv}} \sum
\limits_{i=1}^{m_{cv}}(h_\theta(x_{cv}^{(i)}) - y_{cv}^{(i)})^2 \]
Test error:
\[ J_{test}(\theta) = \frac{1}{2m_{test}} \sum
\limits_{i=1}^{m_{test}}(h_\theta(x_{test}^{(i)}) -
y_{test}^{(i)})^2 \] Returning to Model Selection task, we:
\begin{itemize}
\item calculate $\Theta^{(d)}$ for each reasonable d using the
  training data set;
\item calculate $J_{cv}(\Theta^{(d)})$ for each $\Theta^{(d)}$; select
  d with minimal $J_{cv}$ - assume this is for d=4
\item estimate generalisation error for test set
  $J_{test}(\theta^{(4)})$.
\end{itemize}
Sometimes CV set us used to report generalisation error (i.e. CV set
is used as the test set too. It is only acceptable is the test set is
sufficiently large, but usually it is a bad practice.

\subsection{Diagnosing Bias vs Variance}
\label{sec:10-4}

Usual reason for poor performance is either Bias (underfitting) or
high variance (over-fitting). To check this, compare values of Cross
Validation vs Training errors.
% see 10-4, Diagnosing bias vs. variance diagram
High values for $J_{cv}$ correspond to either Bias or High Variance
situation. Plus:
\begin{itemize}
\item Bias (underfit):
  $J_{train}(\theta)$ will be high; \\
  $J_{cv}(\theta) \approx J_{train}(\theta) $ - also high
\item Variance (overfit)
  $J_{train}(\theta)$ will be low; \\
  $J_{cv}(\theta) >> J_{train}(\theta) $
\end{itemize}

\subsection{Regularisation and Bias / Variance}
\label{sec:10-5}

Large $\lambda$ values lead to High bias (underfit) training results;
small $\lambda$ - lead to high variance (overfit). The idea is to
select $\lambda$ automatically.

Despite the ``normal'' cost function $J(\theta)$ now contains
$\lambda$:
\[J(\theta) = \frac{1}{2m} \sum \limits_{i=1}^{m}(h_\theta(x^{(i)}) -
y^{(i)})^2 + \frac{\lambda}{2m} \sum \limits_{j=1}^{m} \theta_j^2 \]

our error values are calculated without one - ``as usual'' Training
error:
\[ J_{train}(\theta) = \frac{1}{2m} \sum
\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 \] Cross Validation
error:
\[ J_{cv}(\theta) = \frac{1}{2m_{cv}} \sum
\limits_{i=1}^{m_{cv}}(h_\theta(x_{cv}^{(i)}) - y_{cv}^{(i)})^2 \]
Test error:
\[ J_{test}(\theta) = \frac{1}{2m_{test}} \sum
\limits_{i=1}^{m_{test}}(h_\theta(x_{test}^{(i)}) -
y_{test}^{(i)})^2 \]

So, to chose the value for regularisation parameter $\lambda$, we step
with powers of 2:
\begin{enumerate}
\item 0.01
\item 0.02
\item 0.04
\item 0.08

  $\vdots$
\item[12] 10 (almost 10.24)
\end{enumerate}

On each step minimise $J(\Theta)$, getting $\Theta^{(i)}$. Determine
cross validation error $J_{cv}(\theta^{d})$. Then check the test
error.

For $\lambda$, the picture differs a bit from the Model Selection: we
have high variance (high $J_{cv}(\theta)$ / low $J_{test}(\theta)$)
for low value of $\lambda$ and high Bias (high values for both
$J_{cv}(\theta)$ and $J_{test}(\theta)$) for high values of $\lambda$.

\subsection{Learning Curves}
\label{sec:10-6}

Used for sanity check or performance proves. Usually the plots of
$J_{training}(\theta)$ and $J_{cv}(\theta)$ are plotted as the
functions of m (training set size).

With increasing m, the training error increases. The Cross Validation
(CV) error decreases with increase in training set size.

In High Bias case, the training error slightly increases but
stabilises at some plato; CV error decreases to the similar value. The
resulting error is quite high, and this {\bf error does not decreases
  with increasing training set size }.

In High Variance case, the training error increases slightly with
increasing m, while CV error decreases to quite high error value (well
above the training error). This gap (between training error and Cross
Validation error) works as indicative diagnostic for the case of High
Variance (too low $\lambda$). Adding more data is likely to help in
this case.
% see pictures from PDF!

\subsection{Deciding What to Do Next Revisited}
\label{sec:10-7}

So, if the ML algorithm does not provide sufficient performance, what
should you try next?
\begin{itemize}
\item get more training examples - to fix high variance
\item try smaller set of features - foxes high variance
\item try getting additional features - fixes high bias problem
\item adding polynomial features - for fixing the high bias problem
\item try decreasing - fixes high bias
\item try decreasing - fixes high variance
\end{itemize}

\subsubsection{Neural Networks and Overfitting}
\label{sec:10-7-1}
\begin{itemize}
\item ``Small'' neural networks (fewer parameters; more prone to
  under-fitting) are computationally cheaper
\item ``large'' network (more hidden unit, more hidden layers) - more
  prone to overfitting. Computationally more expensive; use $\lambda$
  to address over-fitting.
\end{itemize}
In any case, use $L_{cv}$ to find out the best network config (namely,
the amount of hidden layers and amount of hidden element).

\section{ML System Design}
\label{sec:11}

\subsubsection{Prioritising}
\label{sec:11-1}
Example: building a spam filter.

Naive approach: choose 100 words indicative of spam/not spam, like
deal, buy, discount, andrew (name of addressee), now...

then build a training set $\mathbf{x} \in \mathbf{R}^{100}$ such as \[
x_j = \left\{
  \begin{array}{ll}
    1 & \textrm{ if word j appears in email } \\
    0 & \textrm{ otherwise}
  \end{array} \right. \]

In practice, take most frequently occurring n words (10,000 to 50,000)
in training set, rather then manually pick 100 words.

Variants to spend time working here:
\begin{itemize}
\item Collect lots of data (like in ``honeypot'' project): not always
  work as we see previously;
\item Develop sophisticated features based on email routing
  information (from email header).
\item Develop sophisticated features for message body, e.g. should
  ``discount'' and ``discounts'' be treated as the same word? How
  about ``deal'' and ``Dealer''? Features about punctuation?
\item Develop algorithm to detect misspellings like m0rtgage,
  med1cine, w4tches
\end{itemize}
The selection is often made based on the ``gut feeling'' - not a best
way to make decisions.

\subsubsection{Error Analysis}
\label{sec:11-2}
Recommended approach:
\begin{itemize}
\item Start with a simple algorithm that you can implement quickly (in
  24 hours). Implement it and test on the cross - validation data.
\item Plot learning curves to decide if more data, more features, etc.
  are likely to help.
\item Error analysis: Manually examine the examples (in cross
  validation set) that your algorithm made errors on. See if you spot
  any systematic trend in what type of examples it is making errors
  on.
\end{itemize}

\paragraph{Example}
Let $m_{CV} = 500$ examples in cross validation set. \\
Algorithm miss-classified 100 emails. \\
Manually examine 100 errors and categorise them based on:
\begin{enumerate}
\item What type of email it is
\item What cues (features) you thing would have helped the algorithm
  classify them correctly
\end{enumerate}
For instance, imagine out of 100 emails, the ones which where
miss-classified as ``normal'' are:
\begin{itemize}
\item contains word ``Pharma'': 12
\item contains word ``Replica/fake'': 4
\item tries to steal password : 53
\item Other: 31
\end{itemize}
It become obvious that we need to look more carefully at password -
stealing emails and find out how we can add features like ``tries to
steal password''.

Other possible features:
\begin{itemize}
\item Deliberate misspelling (m0rgage, med1cine, etc.)
\item Unusual email routing
\item Unusual (spamming) punctuation
\end{itemize}

\paragraph{The importance of numerical evaluation}

E.g. should discount/discounts/discounted/disccounting be treated as
the same word?

For NLP projects it is often good idea to use ``stemming'' software -
like ``Porter stemming''. The problem is that it can hurt (like in
case universe/university). We need the numerical evaluation (e.g.
cross validation error) of algorithms performance with and without
stemming. Similar story - for distinguishing upper vs. lower case
(Mom/mom - should it be treated as the same or different words?)

\subsection{Error Metrics for Skewed Classes}
\label{sec:11-3}

\paragraph{Cancel classification example}
Train logistic regression model $h_\theta(x)$: y=1 if cancer, y = 0
otherwise.

Find that you got 1\% error on your test set (99\% correct diagnoses).
Looks good, but:

If only 0.50\% of patients actually do have cancer, our 1\% error does
not look that good: function which always return 0 will be even better
(it will fail in 0.5\% of cases - better then our 1\%).

Skewed classes - classification when the number of examples is too
close to one of extremes (so that returning constant will give
reasonably good result)..

In this case we need a different evaluation metric, like:

\subparagraph{Precision/Recall} if y = 1 in presence of rare class
that we want to detect (which is pure convention versus y = 0 for rare
case):

\begin{tabular}{cc}
  \quad & Actual class \\
  Predicted class &
  \begin{tabular}{c|c|c}
    \quad & 1 & 0 \\
    \hline
    1 & True positive & False positive \\
    0 & False negative & True negative 
  \end{tabular}
\end{tabular}

We calculate two parameters:
\begin{itemize}
\item Precision \\
  of all cases when we predicted y = 1, what fraction actually should
  be classified 1? $\frac{\textrm{True positives}}{\textrm{\#predicted
      pos}} = \frac{\textrm{True pos}}{\textrm{True pos + False pos}}$
  Shows how accurate our prediction is.
\item Recall \\
  Of all cases actually having 1, what fraction did we correctly
  detect as y = 1? $\frac{\textrm{True pos}}{\textrm{\#actual pos}} =
  \frac{\textrm{True pos}}{\textrm{True pos + False pos}}$ Gives a
  rough sense of how well is our classifier.
\end{itemize}
The idea is that good classified gives high both Precision and Recall,
while ``cheating'' one (e.g. the one which always returns 0) will have
one of these parameters quite low.

\subsection{Trading Off Precision and Recall}
\label{sec:11-4}

Let's say we work on logistic regression: $0 \leq h_\theta(x) \leq 1$ \\
Predict 1 if $h_\theta(x) \geq 0.5$ \\ Predict 0 if $h_\theta(x) <
0.5$ \\

Suppose we want to predict y = 1 (e.g. cancer) only when confident. In
this case we could set a threshold to 0.7. We'll end up with {\bf high
  precision } but {\bf lower recall}.

If we want to avoid missing too many cases of cancer (avoid false
negatives - conservative approach), we set threshold to 0.3. We'll get
a {\bf high recall} (correctly flagging a higher fraction of patients
actually having cancer) but {\bf lower precision} (higher fraction of
patients we've diagnosed with cancer actually do not have one).

More generally: see lecture at 06:50 for dependency between Precision
and Recall.

Next step - to find way to choose this threshold automatically -
particularly with different learning algorithms (or the same algorithm
with different parameters). We introduce some single - value metric
named:

\subsubsection{F Score ($F_1$ Score)}
\label{sec:11-4-1}
% 08:30
\begin{tabular}{c|cc|c|c}
  \quad & Precision (P) & Recall (R) & Average & $F_1$ Score\\
  \hline
  Algorithm 1 & 0.5 & 0.4 & 0.45 & 0.444 \\
  Algorithm 2 & 0.7 & 0.1 & 0.4 & 0.175\\
  Algorithm 3 & 0.02 & 1.0 & 0.51 & 0.0392
\end{tabular} \\

Variants:
\begin{itemize}
\item Average: $\frac{P + R}{2}$ - is not a good idea, because the
  variants with R = 1 (predict 1 all times) will get high level - like
  for Algorithm 3.
\item $F_1$ Score: $2\frac{PR}{P + R}$ - works much better. P = 0 or R
  = 0 $ \rightarrow F_1$ Score = 0; P = 1 and R = 1 $\rightarrow F_1$
  Score = 1;
\end{itemize}

\subsection{Data for Machine Learning}
\label{sec:11-5}

... It is not who has the best algorithm that wins, it's who has the
most data.

\subsubsection{Large data rationale}
\label{sec:11-5-1}

Assume feature $x \in \mathbf{R}^{n+1}$ has sufficient information to
predict y accurately

Example: For breakfast I ate (to?/too?/two?) eggs.

Counterexample: predict housing price from only size (feet) and no
other features.

Useful test: Given the input x, can a human expert confidently predict
y?

Better to use a learning algorithm with many parameters (e.g. logistic
regression/linear regression) neural network with many hidden units).
Use a very large training set (unlikely yo overfit):
$J_{train}(\Theta) \approx J_{test}(\Theta)$ and $J_{test}(\Theta)$
will be small.

Or: in good algorithm we do not want either high bias or high
variance. The bias problem is solved by selecting algorithm with many
parameters; the variance problem is solved by using the large training
set.


\section{Support Vector Machines (SVM)}
\label{sec:SVM}
The last supervised learning algorithms in this course.
\subsection{Optimisation Objective}
\label{sec:12-1}
Alternative view to logistic regression. We use $h_\theta = \frac{1}{1
  + e ^{-\theta^Tx}}$ to express: if $y=1$, we want $h_\theta(x)
\approx 1, \theta^{T} >> 0$ and $h_\theta(x) \approx 0, \theta^{T} <<
0$. Cost of example (for single example):
\[-(y \log h_\theta(x) + (1-y) \log(1-h_\theta(x))) = -y \log
\frac{1}{1 + e^{-\theta ^T x}} - (1-y) \log (1 - \frac{1}{1 +
  e^{-\theta^Tx}}) \]

This is, really, the generalisation of idea that when $y=1$ we use [y
* ]$-\log \frac{1}{1 + e^{-z}}, z = \theta^Tx; \theta^T >> 0$ part
while for $y=0$ we use (1-y) * $-\log (1 - \frac{1}{1 + e^{-z}}), z =
\theta^Tx; \theta^T << 0 $ part. For SVM we change the cost function:
\begin{itemize}
\item for $y = 1$: $cost_1(z)$ equals 0 for $z \geq 1$ and linearly
  decreases from ``somewhere'' to 0 for $z < 1$.
\item for $y = 0$: $cost_0(z)$ equals 0 for $z \leq 1$ and linearly
  increases to ``somewhere'' form 0 for $z > -1$.
\end{itemize}

Now re-write the cost function, replacing as shown in Logistic
regression:
\[\min \limits _\theta \frac{1}{m} \left[\sum \limits_{i=1}^{m}
  y^{(i)} \underbrace{\left(- \log h_\theta
      (x^{(i)})\right)}_{cost_1(\Theta^Tx^{(i)})} + (1-y^{(i)})
  \underbrace{\left(-\log(1 - h\theta(x^{(i)})
    \right)}_{cost_0(\Theta^Tx^{(i)})} \right] + \frac{\lambda}{2m}
\sum \limits_{j=1}^{n}\theta_j^2 \]

so in Support Vector Machine:
\[\min \limits _\theta \frac{1}{m} \left[\sum \limits_{i=1}^{m}
  y^{(i)}cost_1(\Theta^Tx^{(i)}) + (1-y^{(i)}) cost_0(\Theta^Tx^{(i)})
\right] + \frac{\lambda}{2m} \sum \limits_{j=1}^{n}\theta_j^2 \]

By convection, the regularisation parameter is written slightly
differently: we remove $\frac{1}{m}$ (as gradient descent will find
the minimum of cost function regardless of concrete values). Then,
instead of (roughly) using cost function as $A + \lambda B$ (where A
is the ``main'' part of formula, B - regularisation member; we play
with $\lambda$ to tune our process), the SVM cost is written as $C A +
B$, where we play with C to tune. (Set C to be large to simulate small
$\lambda$ , think of C as $C = \frac{1}{\lambda}$). Finally, SVM
process:
\[\min \limits _\theta C \sum \limits_{i=1}^{m} \left[
  y^{(i)}cost_1(\theta^Tx^{(i)}) + (1-y^{(i)}) cost_0(\theta^Tx^{(i)})
\right] + \frac{1}{2} \sum \limits_{j=1}^{n}\theta_j^2 \]
% 12-01; 13:45
Support Vector Machine (SVM) does not return the probability - it
outputs the value from hypothesis:
\[ h_\theta(x) = \left\{
  \begin{array}{cc}
    1 & \textrm{ if } \Theta^T \geq 0\\
    0 & \textrm{otherwise}
  \end{array}
\right. \]

\subsection{Large Margin Intuition}
\label{sec:12-2}

Sometimes SVM is called ``Large Margin Classifier'':
\begin{itemize}
\item for $y = 1$ we want $\theta^T x \geq 1$ (not just $\geq 0$)
\item for $y = 0$ we want $\theta^T x \leq -1$ (not just $< 0$)
\end{itemize}

Consider high value for C (like 100,000): in this case, the ``main''
part (the sum of $cost_1$ and $cost_0$) will be $\approx 0$, so the
cost function to minimise will be min $ C \times 0 + \frac{1}{2} \sum
\limits_{i=1}^{n} \Theta_j^2$. With condition that
\[ \begin{array}{cc}
  \Theta^T x^{(i)} \geq 1 & \textrm{ if } y^{(i)} = 1 \\
  \Theta^T x^{(i)} \leq -1 & \textrm{ if } y^{(i)} = 0
\end{array} \]
after optimisation, we'll get quite natural decision boundary - the
ones that have the biggest margin  (see slide 9).

\subsection{Mathematics Behind Large Margin Classification}
\label{sec:12-3}

\paragraph{Vector Operations}
Inner product of vectors u and v is $u^Tv$. If $u =
\left[ \begin{array}{c} u_1 \\ u_2 \end{array} \right] $, $v =
\left[ \begin{array}{c} v_1 \\ v_2 \end{array} \right] $. Length of u:
$||u|| = \sqrt{u_1^2 + u_2^2}; ||u|| \in \mathbf{R}$

p = length of projection of v onto u. And: $u^Tv = p \times ||u|| =
u_1 v_1 + u_2 v_2 = v^Tu$. Also $p \in \mathbf{R}$ - signed (can be
negative)!

\subsubsection{SVM Decision Boundary}
\label{sec:12-3-1}
Assume $\theta_0 = 0; n = 2$. Our cost function (to be minimised):
\[J(\theta) = \frac{1}{2} \sum \limits_{j=1}^{n}\theta_j^2 =
\frac{1}{2}(\theta_1^2 + \theta_2^2) = \frac{1}{2}(\sqrt{\theta_1^2 +
  \theta_2^2})^2 = \frac{1}{2}||\theta||^2\]
where \[ \begin{array}{ll}
  \theta^Tx^{(i)} \geq 1 & \textrm{ if } y^{(i)} = 1 \\
  \theta^Tx^{(i)} \leq -1 & \textrm{ if } y^{(i)} = 0
\end{array} \]

Consider $x^{(i)}$. Our product will be $\theta^Tx^{(i)} = p^{(i)}
||\theta|| = \theta_1 x_1^{(i)} + \theta_2 x_2^{(i)}$ - see page 13.

Re - writing optimisation objective: $ \min \limits_{\theta}
\frac{1}{2}\sum \limits_{j=1}^{n} \theta_j^2$,
where \[\begin{array}{ll}
  p^{(i)} * ||\theta|| \geq 1 & \textrm{ if } y^{(i)} = 1 \\
  p^{(i)} * ||\theta|| \leq -1 & \textrm{ if } y^{(i)} = 0
\end{array}\]
where $p^{(i)}$ is the projection of $x^{(i)}$ onto the vector
$\theta$. Simplification: $\theta_0 = 0$ (so the decision boundary
goes through the (0;0); very large C.

(see page 14 for graphical explanation). If $\theta$ vector (as a
normal to the boundary) will be sub-optimal (that is, not having
optimal angle), then:
\begin{itemize}
\item for y = 1: $p^{(i)} * ||\theta|| \geq 1$. The projection
  $p^{(i)}$ is relatively small, so $||\theta||$ should be large;
\item for y = 0: $p^{(i)} * ||\theta|| \leq -1$. The $p^{(i)}$ is
  negative, but relatively small in length, so $||\theta||$ should be
  large;
\end{itemize}
The problem is that this breaks our optimisation criteria (minimising
$||\theta||$). To hold $||\theta||$ small we need to turn the
$\theta$'s angle to optimal value. As such, the longer projections
are, the wider is the margin.

\subsection{Kernels}
\label{sec:12-4}
Kernels is a technique for developing non-linear classifier. Obvious
solution is:

Predict y = 1 if $\theta_0 + \theta_1x_1 + \theta_2x_2 +
\theta_3x_1x_2 + \theta_4x_1^2 + \theta_5x_2^2 + \dots \geq 0$, or
\[ h_\theta(x) = \left\{
  \begin{array}{ll}
    1 & \textrm{ if } \theta_0 + \theta_1x_1 + \dots \geq 0 \\
    0 & \textrm{ otherwise } 
  \end{array} \right. \]
In other format: $\theta_0 + \theta_1f_1 + \theta_2f_2 +
\theta_3f_3 + \theta_4f_4 + \theta_5f_5 + \dots$ where $f_1 = x_1, f_2
= x_2, f_3 = x_1x_2, f_4 = x_1^2, f_5 = x_2^2 \dots $ - need to find
the best choice for $f_i$

\paragraph{Idea}: Given x, compute new feature depending on proximity
to landmarks $l^{(1)}, l^{(2)}, l^{(3)}$ (see page 17): \[f_1 =
similarity(x, l^{(1)}) = \exp(- \frac{\overbrace{||x -
    l^{(1)}||^2}^{\textrm{length of } w = x - l^{(1)}}
}{2\sigma^2}) \]
\[f_2 = similarity(x, l^{(2)}) = \exp(- \frac{||x -
  l^{(2)}||^2}{2\sigma^2}) \]

This ``similarity'' function is called Kernel (or ``Gaussian Kernel'')
function, also written as $k(x, l^{(i)})$

\subsubsection{Kernels and Similarity}
\[f_1 = similarity(x, l^{(1)}) = \exp(- \frac{||x -
  l^{(1)}||^2}{2\sigma^2}) = \exp (- \frac{\sum \limits_{j=1}^{n} (x_j
  - l_j^{(1)})^2}{2\sigma^2}) \]

if $x\approx l^{(1)} : f_1 \approx \exp(-\frac{0^2}{2\sigma^2})
\approx 1 $;

if x is far from $l^{(1)}$ : $f_1 \approx \exp(-\frac{(\textrm{ large
    number })^2}{2\sigma^2}) \approx 0$

Each of these landmarks define a new feature (see page 19). The
$\sigma$ parameter defines how fast the feature value decreases with
distance.

Then, using idea to predict y = 1 if $\theta_0 + \theta_1x_1 +
\theta_2x_2 + \theta_3x_1x_2 + \theta_4x_1^2 + \theta_5x_2^2 + \dots
\geq 0$, we can write $\theta_0, \theta_1, \dots$ - see page 20 for
details.
\label{12-5}

In practice, the landmarks are put into the same location as the
points of the training set - so we measure how close the feature is to
the training set points. Then the feature vector is: \[ f = \left[
  \begin{array}{l} f_0 \\ f_1 \\ f_2 \\ \vdots \\ f_m \end{array}
\right] \], by convention $f_0 = 1$.

Then, for training example $(x^{(i)}, y^{(i)}$: $f_i^{(i)} =
sim(x^{(i)}, l^{(i)}) = 1$; $x^{(i)} \in \mathbf{R}^{n+1}$

\subsection{SVM with Kernels}
\label{sec:12-5-1}

Hypothesis: Given x, compute features $f \in \mathbf{R}^{m+1}$.

Predict ``y=1'' if $\theta^T f \geq 0$

Training:
\[ \min \limits_{\theta} C \sum \limits_{i=1}^{m} y^{(i)}
cost_1(\theta^T f^{(i)}) + (1 - y^{(i)})cost_0(\theta^T f^{(i)}) +
\frac{1}{2}\sum \limits_{j=1}^{n}\theta_j^2 \] but, instead of
$\theta^Tx^{(i)}$ we use $\theta^Tf^{(i)}$ (and regularisation
parameters sum is from 1 to m, not n) - and solve ``as normal''.

Another way to write the regularisation sum: $\sum \limits_{j}
\theta_j^2 = \theta^T \theta$ - ignoring $\theta_0$! So many SVM
implementations use some sort of simplification: instead of
calculating $||\theta||^2$ they try to minimise $\theta^T M \theta$ -
this allows to scale to much bigger training sets (up to m = 10,000).

Concrete implementation - ``use off-the-shelf'' soft 8-(.

\subsubsection{SVM Parameters}
\label{sec:12-5-2}

$C ( = \frac{1}{\lambda})$.
\begin{itemize}
\item Large C: Lower bias, high variance, over fitting (small
  $\lambda$).
\item Small C: Higher bias, low variance, under fitting (large
  $\lambda$).
\end{itemize}

$\sigma^2$ : recall $\exp (-\frac{||x-l^{(i)}||^2}{2\sigma^2})$
\begin{itemize}
\item Large $\sigma^2$: features $f_i$ vary more smoothly. Higher
  bias, lower variance.
\item Small $\sigma^2$: features $f_i$ vary less smoothly. Lower bias,
  higher variance.
\end{itemize}

\subsection{Using An SVM}
\label{sec:12-6}
Ideas:
\begin{itemize}
\item use SVM software packages (e.g. liblinear, libsvm) to solve for
  parameters $\theta$ There you need to specify:
  \begin{itemize}
  \item choice of parameter C.
  \item choice of kernel (similarity function), e.g.
    \begin{itemize}
    \item No kernel (``linear kernel'', predict y=1 if $\theta^T \geq
      0$ where $\theta$ is a ``standard'' linear classifier). Works
      fine when you have a large number of features (and relatively
      small no of train samples)
    \item Gaussian kernel: $f1 = \exp(-\frac{||x-l^{(i)}||^2}{2
        \sigma^2})$ where $l^{(i)} = x^{(i)}$. Need to choose
      $\sigma^2$. Works better with smaller n and relatively large
      training sets. Do perform feature scaling before using the
      Gaussian kernel!
    \item Other choices of kernel. Not all similarity functions make a
      vali kernels (need to satisfy technical condition called
      ``Mercer's Theorem'' to make sure SVM packages' optimisation
      runs correctly and do not diverge. These are: polynomial kernel
      ($k(x, l) = (x^T l)^2$, or $(x^T l)^3$, $(x^T l + 1)^3$ etc. -
      kind of $(x^T l + constant)^{degree}$. Usually diverges quite
      slowly, not used too often.
    \item more esoteric: String kernel, chi-square kernel, histogram
      intersection kernel, etc.
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsubsection{Multi-class classification}
\label{sec:12-6-1}
When several classes (K): $y \in \{1, 2, 3, \dots \mathbf{K} \}$. Many
SVM packages do already have the built-in multi-class classification
functionality. Otherwise, use one-vs-all method: train K SVMs, one to
distinguish y = i from the rest, for $i = 1, 2, 3, \dots \mathbf{K}$,
get $\theta^{(1)}, \theta^{(2)}, \dots , \theta^{(K)}$. For concrete
x, pick class i with the largest $(\theta^{(i)})^T x$

\subsubsection{Logistic Regression vs SVMs}
n = number of features ($x \in \mathbf{R}^{n+1}$), m = number of
training examples.
\begin{itemize}
\item If n is large (relative to m), like n = 10,000; m = 10 \ldots
  1000: use logistic regression , or SVM without a kernel (``linear
  kernel'').
\item n is small, m is intermediate (n = 1 \ldots 1000, m = 10 \ldots
  10,000). Use SVM with Gaussian kernel
\item if n is small, m is large (n = 1 \ldots 1000, m > 50,000) :
  create/add more features. then use logistic regression or SVM
  without a kernel (just like a case $n \geq m$ - in both cases we do
  similar things)
\end{itemize}

Neural networks likely to work well for most of these settings but may
be slower to train.

Usually good SVM implementation are quite fast, plus they care about
the local optima problems (convex problems) which cause headache for
Neural Networks.

\section{Clustering}
\label{sec:13}

Training set is given without labels: $\left\{ x^{(1)}, x^{(2)},
  x^{(3)}, \dots , x^{(m)} \right\}$

\subsection{K-Means Algorithm}
\label{sec:13-2}
Steps:
\begin{itemize}
\item put two cluster centroids (in random places)
\item assign each of the data points to the relevant cluster -
  depending on distance to centroid
\item move each centroid to the centre of relevant cluster (the mean
  value for coordinates)
\item perform the cluster (re)assignment, move centroid {\it quantum
    sates}
\end{itemize}

\paragraph{K-means algorithm}

\subparagraph{Input:}
\begin{itemize}
\item $\mathbf{ K }$ (number of clusters)
\item Training set $\left\{ x^{(1)}, x^{(2)}, x^{(3)}, \dots , x^{(m)}
  \right\}$
\end{itemize}
$x^{(i)} \in \mathbf{R}^n$ (drop $x_0 = 1$ convention)

\subparagraph{Algorithm:} Randomly initialise $\mathbf{K}$ cluster
centroids $\mu_1, \mu_2, \dots, \mu_K \in \mathbf{R}^n$

\begin{tabbing}
  Repeat \{    \\
  \quad \quad \= for i = 1 to m \\
  \quad \quad \> \quad \=  $c^{(i)}$ := index (from 1 to K) of cluster
  centroid \\
  \quad \quad \> \quad \> closest to $x^{(i)}$ (usually using
  $||x^{(i)} - \mu_k||^2$ for minimisation - the k will be the same) \\
  \quad \quad \> for k = 1 to $\mathbf{K}$ \\
  \quad \quad \> \quad \> $\mu_k$ := average (mean) of points assigned
  to cluster k \\
  \}
\end{tabbing}
Sometimes, some clusters end up with no points assigned -- in this
case it is better to remove excessive cluster (so to end up with K-1
cluster). If you really need K clusters - re-initiate the whole
process with K random centroids (but better go on with K-1).

\subsection{Optimisation Objective}
\label{sec:13-3}
terms:
\begin{itemize}
\item $c^{(i)}$ = index of cluster (1, 2, \dots, K) to which example
  $x^{(i)}$ is currently assigned
\item $\mu_k$ = cluster centroid k $(\mu_k \in \mathbf{R}^n)$
\item $\mu_c(i)$ = cluster centroid of cluster to which example
  $x^{(i)}$ has been assigned
\end{itemize}

Optimisation objective: \[ J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots,
\mu_K) = \frac{1}{m} \sum \limits_{i=1}^m || x^{(i)} - \mu_c(i) ||^2
\]
\[ \min \limits_{ \begin{array}{l}
    c^{(1)}, \dots, c^{(m)}, \\
    \mu_1, \dots, \mu_K
  \end{array}} J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots, \mu_K) \]

This function J is sometimes called ``Distortion''

As such, our algorithm becomes:
\begin{tabbing}
  Repeat \{    \\
  Cluster assignment step: minimise J(...) with $c^{(1)}, \dots,
  c^{(m)}$ holding $\mu_1, \dots, \mu_K$  \\

  Move centroid step: minimise J(...) with  $\mu_1, \dots, \mu_K$
  holding  $c^{(1)}, \dots, c^{(m)}$ \\
  \}
\end{tabbing}

\subsection{Random Initialisation}
\label{sec:13-4}
The items are:
\begin{itemize}
\item Should have $\mathbf{K} < m$
\item Randomly pick K training examples
\item Set $\mu_1, \dots, \mu_K$ equal to these K examples
\end{itemize}

As K-means result depends on starting positions (in particular, it can
end up with local optima - i.e. joining two clusters into one etc). To
avoid this, we should try a number of different initialisation
variants (lets say, 100):

\begin{tabbing}
  For i = 1 to 100 \{    \\
  \quad Randomly initialise K-means \\
  \quad Run K-means. Get $c^{(1)}, \dots,c^{(m)}, \mu_1, \dots, \mu_K$  \\
  \quad Compute cost function (distortion)  $J(\mu_1, \dots, \mu_K, c^{(1)}, \dots, c^{(m)})$ \\
  \}
\end{tabbing}
Pick clustering that gave lowest cost $J(\mu_1, \dots, \mu_K, c^{(1)},
\dots, c^{(m)})$

That works fine for low K (say, from 2 to 10).

\subsection{Choosing the Number of Clusters}
\label{sec:13-5}

The best way is manually (from visualisation or so). Sometimes
so-called ``elbow method'' is used: compute distortion for different
values of K and select the one which provides the ``elbow'' - a point
where distortion goes down much slower then before.

Another way - if we're using K-means to get clusters to use for some
later/downstream purpose. In this case - evaluate K-means based on a
metric for how well it performs for that later purpose.

\section{Dimentionality Reduction}
\label{sec:14}

\subsection{Motivation I - Data Compression}
\label{sec:14-1}
Example:
\begin{itemize}
\item Reduce from 1D to 2D (through introduction of new dimension
  instead of existing x, y:
  \[
  \begin{array}[l]{rl}
    x^{(1)} \in \mathbf{R}^2 & \rightarrow z^{(1)} \in
    \mathbf{R} \\
    x^{(2)} \in \mathbf{R}^2 & \rightarrow z^{(2)} \in
    \mathbf{R} \\
    \vdots \\
    x^{(m)} \in \mathbf{R}^2 & \rightarrow z^{(m)} \in
    \mathbf{R} \\
  \end{array}
  \]
\item Reduce from 3D to 2D ... 1000D to 100D

  Done through projecting this data (x, y, z) on the plane ($z_1,
  z_2$)
\end{itemize}

\subsection{Motivation II - Visualisation}
\label{sec:14-2}

The idea to project the data set of $\mathbf{R}^{50}$ to
$\mathbf{R}^{2}$ with subsequent drawing it on the plane

\subsection{Principal Component Analysis (PCA) - formulation}
\label{sec:14-3}
It is good idea to perform data normalisation first.

To reduce from 2-dimension to 1-dimension:find a direction (a vector
$u^{(1)} \in \mathbf{R}^2$) onto which to project the data so as to
minimise the projection error.

To reduce from n-dimension to k-dimension:find k vectors $u^{(1)},
u^{(2)}, \dots, u^{(k)} \in \mathbf{R}^n$ - so-called ``linear
subspace of dimension k'' onto which to project the data so as to
minimise the projection error.

PCA is completely separated from the ``linear regression'': here we
find the line minimising the projection error while in latter we try
to predict y-value based on x-value and providing a ``prediction
line'' based on ``pure'' difference in y values. (see pp. 12, 13). All
dimensions are ``equal'': we do not have a separate values we know and
other values we want to predict.

\subsection{Principal Component Analysis (PCA) - algorithm}
\label{sec:14-4}

\paragraph{Data Prepossessing}

Training set: $x^{(1)}, x^{(2)}, \dots, x^{(m)}$

Prepossessing (feature scaling / mean normalisation):
\[ \mu_j = \frac{1}{m} \sum \limits_{i=1}^{m} x_j^{(i)} \]

Replace each $x_j^{(i)}$ with $x_j - \mu_j$.

If different features on different scales (e.g, $x_1$ = size of house,
$x_2$ = number of bedrooms), scale features to have comparable range
of values: $ x_j^{(i)} \gets \frac{x_j - \mu_j}{S_j}$, where $S_j$ -
standard deviation for dimension j.

Full math proof is ``beyond the scope of the course''. Result is:

\begin{algorithm}
  \caption{Principal Component Analysis (PCA) Algorithm - Reduce data
    from n-dimensions to k-dimensions }
  \begin{algorithmic}
    \STATE Compute ``covariance matrix'': \[ \Sigma = \frac{1}{m} \sum
    \limits_{i=1}^{n} (x^{(i)})(x^{(i)})^T \]
    \STATE Compute ``eigenvectors'' of matrix $\Sigma$: \\
    $\left[U, S, V\right] = svd(Sigma);$; Sigma is $n \times n$ matrix
    Then we'll get the U matrix: \[U = \left[
      \begin{array}{ccccc}
        | & | & | & & | \\
        u^{(1)} & u^{(2)} & u^{(3)} & \vdots & u^{(n)} \\
        | & | & | & & | \\
      \end{array}
    \right], U \in \mathbf{R}^{n \times n}\] \STATE So we take first k
    columns (vectors): $u^{(1)}, \dots, u^{(k)} - n \times k$ matrix
    (named $U_{reduce}$). So operation $x \in \mathbf{R}^n \rightarrow
    z \in \mathbf{R}^k$ becomes:

    \[ z = U_{reduce} x = \left[
      \begin{array}{ccccc}
        | & | & | & & | \\
        u^{(1)} & u^{(2)} & u^{(3)} & \vdots & u^{(k)} \\
        | & | & | & & | 
      \end{array}
    \right]^T x = \left[
      \begin{array}{ccc}
        --- & (u^{(1)})^T & --- \\
        \   & \vdots & \   \\
        --- & (u^{(k)})^T & --- 
      \end{array}
    \right] x
    \]
    where $U_{reduce}$ is of dimension $n \times k$, x is of $n \times
    1$, so $(U_{reduce})^T \times x$ becomes $k \times 1$
    \STATE in Octave, it will be: \\
    \quad $\left[U, S, V\right] = svd(Sigma);$ \\
    \quad $U_{reduce} = U(:, 1: k);$ \\
    \quad $z = U_{reduce}' * x;$
  \end{algorithmic}
\end{algorithm}

% сделать ссылкой внизу страницы!!
{\bf svd} stays for ``singular value decomposition'' - works like {\bf
  eig(Sigma)}, but a bit more stable.

\subsection{Reconstruction from Compressed Representation}
\label{sec:14-6}

The idea is to reconstruct data from reduced dimension data set: $z
\in \mathbf{R} \rightarrow x \in \mathbf{R}^2$, so if $z =
U^T_{reduce} x$, then \[ \underbrace{x_{approx}}_{\mathbf{R}^n} =
\underbrace{U_{reduce}}_{n \times k} \underbrace{z}_{k \times 1} \]

\subsection{Applying PCA}
\label{sec:14-7}

\subsubsection{Speedup supervised learning}
\label{sec:14-7-1}

Given $(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}),\dots, (x^{(m)},
y^{(m})$, and $(x^{(i)} \in \mathbf{R}^{10000}$ (i.e. for image
analysis of $100 \times 100$ pixels. This going to be quite slow, so:
\begin{itemize}
\item Extract input x's to unlabelled dataset $x^{(1)},x^{(2)},\dots,
  x^{(m)} \in \mathbf{R}^{10000}$
\item apply PCA so that dataset becomes $z^{(1)},z^{(2)},\dots,
  z^{(m)} \in \mathbf{R}^{1000}$
Got a new training set:  $(z^{(1)}, y^{(1)}),(z^{(2)}, y^{(2)}),\dots, (z^{(m)},
y^{(m})$; train algorithm using $h_\theta(z) = \frac{1}{1 +
  e^{-\theta^Tz}}$
\end{itemize}

When given a new sample x:
\begin{itemize}
\item calculate $x \rightarrow z$ using $U_{reduce}$
\item use z for making prediction based on $h_\theta(z)$
\end{itemize}
%find out how to separate "Notes":

\subparagraph{Note:}
Mapping $x^{(i)} \rightarrow z^{(i)}$ should be defined by running PCA
only on the training set. This mapping can be applied as well to the
examples $x_{cv}^{(i)}$ and $x_{test}^{(i)}$ in the cross validation
and test sets.

\paragraph{Application of PCA}

\begin{itemize}
\item Compression
  \begin{itemize}
  \item Reduce memory/disk needed to store data
  \item Speed up learning algorithm
  \end{itemize}
Here choose k by \% of variance retained. Good (typical) result - up
to 99\%
\item Visualisation
k = 2 or k = 3
\end{itemize}

\paragraph{Bad use of PCA: To prevent outfitting} 

Like use $z^{(i)}$ instead of $x^{(i)}$ to reduce the number of
features to k < n. Thus, fewer features => less likely to overfit.

Bad idea (despite sometimes works). PCA throws away some information
without knowing the values of $y^{(i)}$. Use regularisation instead.

Quite often the whole ML system can go without any dimension reducing
at all. Always need to test on original/raw data before considering
PCA. 

\section{Anomaly Detection}
\label{sec:15}

Example: aircraft engine features.
Formally, we're given example (training) set $\{ x^{(1)}, x^{(2)},
\dots, x^{(m)} \}$, we need to estimate is $x_{test}$ anomalous? 

We build a model $p(x)$ - probability model and ``small threshold''
$\epsilon$ such that:
\begin{itemize}
\item $p(x_{test} < \epsilon \rightarrow$ flag anomaly
\item $p(x_{test} \geq \epsilon \to$ OK
\end{itemize}

Another example - fraud detection:
\begin{itemize}
\item $x^{(i)}$ - features of user i' s activities
\item Model $p(x)$ from data (based on features like typing speed,
  pages visited etc).
\item Identify unusual users by checking which have $p(x) , \epsilon$
\end{itemize}
Manufacturing
Monitoring computers in a data centre: $x^(i)$ = features of machine
i: $x_1$ = memory use; $x_2$ = number of disk access/sec; $x_3$ = CPU
load; $x_4$ = network traffic etc. Check $p(x_{test} < \epsilon$

\subsubsection{Gaussian Distribution (Normal Distribution)}
\label{sec:15-2}
Say $x \in \mathbf{R}$. If x is distributed Gaussian with mean $\mu$,
variance $\sigma^2$, it is written as $x \sim \aleph(\mu, \sigma^2)$
(read ``x distributed as Gaussian from mu and sigma''); $\aleph$, or
``N scripted'' stands for ``normal''; $\sigma$ is called ``the
standard deviation''; $\sigma^2$ is a variance:
\[ p(x; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi} \sigma }
exp(-\frac{(x-\mu)^2 }{2\sigma^2}) \]
-p(x) is parametrized by $\mu$ and $\sigma$-squared.

The total square of probability curve integrates to 1; it changes
shape depending on parameters.

\subsubsection{Parameter estimation}
Given dataset $\{x^{(1)}, x^{(2)}, \dots, x^{(m)}\}, x^{(i)} \in
\mathbf{R}$, find out $\mu, \sigma$:
\[\mu = \frac{1}{m} \sum \limits_{i=1}^m x^{(i)} ; \sigma^2 =
\frac{1}{m} \sum \limits_{i=1}^m (x^{(i)} - \mu)^2 \]

Sometimes $\frac{1}{m-1}$ variant is used (with slightly different
mathematical properties), but in practice m is usually quite large so
no much difference.

\subsubsection{Algorithm}
\label{sec:15-3}
Movie rating example - see page 2 of slides 16.

We assume we have NOT all data for given - so use $x^{(i)}$ only for
available features (so that we do have a value for feature i from
person j: $r(i, j) = 1$:

Given $\theta^{(1)}, \theta^{(2)}, \dots, \theta^{(n_u)}$, to learn
$x^{(i)}$: \[ 
\min \limits_{x^{(i)}} \frac{1}{2} \sum \limits_{j:r(i, j) = 1}
((\theta^{(i)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum
\limits_{k=1}^n (x_k^{(i)})^2 \]

- actually, minimising the square error:
\begin{itemize}
\item $(\theta^{(i)})^T x^{(i)}$ - the predicted value (rating)
\item $y^{(i,j)}$ - actual value given by $j^{th}$ person to $i^{th}$
  movie
\item $\frac{\lambda}{2} \sum \limits_{k=1}^n (x_k^{(i)})^2$ -
  regularisation term
\end{itemize}

Expanding on the idea of learning the whole dataset (the data for all
m movies, $x^{(1)},x^{(2)}, \dots,  x^{(n_m)}$):

\[ \min \limits_{x^{(1)},x^{(2)}, \dots,  x^{(n_m)}} \frac{1}{2} \sum
\limits_{i=1}^{n_m}  \sum \limits_{j:r(i, j) = 1}
((\theta^{(i)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum
\limits_{i=1}^{n_m} \sum \limits_{k=1}^n (x_k^{(i)})^2 \]

% 16-3  06:30

\section{Recommender System}
\label{sec:16}

\end{document}
