%\documentclass{ncc}
%\documentclass{article}
\documentclass{scrartcl}

\usepackage{amsmath,amssymb,amsfonts} % Typical maths resource packages
\usepackage{graphics}                 % Packages to allow inclusion of graphics
\usepackage{color}                    % For creating coloured text and background
\usepackage{hyperref}                 % For creating hyperlinks in
                                % cross references 
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}
\usepackage{listings}
\lstloadlanguages {[LaTeX]TeX, Octave}
\lstset {language=[LaTeX]TeX,
  extendedchars=true ,escapechar=|}

\begin{document}
\label{Chapter 1}
\section {Вводная}
\subsection* {Supervised Learning} - where the ``right (correct)
answers'' for
each example of data is given. \\
Usually we have one of two:\\
{\bf Regression:} predict continuous valued output \\
{\bf Classification:} assign discrete valued output (0, 1 etc) \\

\subsection* {Unsupervised Learning} - finding structure in ``blind''
set of
data. \\
{\bf Clustering} algorithms  \\
Example: {\bf Cocktail Party Problem} (разделение записей на двух
микрофонах от двух источниках и восстановление сигналов от каждого
источника
индивидуально). \\

\label{Chapter 2}
\section {Linear Regression with One Variable}

\subsection {Model Representation}
Notation: \\
{\bf m} =  Number of training examples \\
{\bf x}'s = ``input'' values / features \\
{\bf y}'s = ``output''variable / ``target'' variable \\
{\bf (x, y)} = one single training example \\
{\bf $(x^{(i)}, y^{(i)})$} = $i^{th}$ training example \\

Обычный алгоритм: берем Training Set, скармливаем его Learning
Algorithm. Оный строит ф-ю h (иначе hypothesis), которая по входному
значению $x$ строит Estimation. Иначе говоря, \\
h maps from x's to y's: \\
$h_\theta(x)=\theta_0 + \theta_1x$; shorthand: h(x) \\
- т.е. полагаем что ф-я линейная \\
Другие названия:
\begin{itemize}
\item Linear regression with one variable (x)
\item Univariate(single variable) linear regression
\end{itemize}
\subsection {Cost Function}
\label{2-2}
Идея - минимизировать \[ J(\theta_0, \theta_1) = \frac{1}{2m}
\sum_{i=1}^m\left( h_\theta (x^{(i)}) - y^{(i)} \right)^2
\]
Оная $J(\theta_0, \theta_1)$ и называется Cost Function, она же
Squared Error Function

\subsection {Gradient Descent}
\label {2-5}
Задача: имея функцию $J(\theta_0, \theta_1)$, заполучить ее
${\min \atop \theta_0, \theta_1} J(\theta_0, \theta_1)$ \\
Outline:
\begin {itemize}
\item Start with some $\theta_0, \theta_1$
\item Keep changing $\theta_0, \theta_1$ to reduce J($\theta_0,
  \theta_1$) until we hopefully end up at a minimum
\end{itemize}
Работает также для бОльшего количества параметров ( $\theta_0,
\theta_1,..., \theta_n$)
\subsubsection {Gradient descent algorithm}
repeat until convergence \{ \\
$\theta_j := \theta_j - {\alpha} \underbrace{ \frac {\delta}{\delta
    \theta_j} J(\theta_0,
  \theta_1)}_{derivative}$ (for j=0 and j=1) \\
\}, where: \\
$\alpha$ - learning rate \\
$\frac {\delta}{\delta \theta_j} J(\theta_0, \theta_1)$ - derivative
(производная, точнее, частная производная)

\subparagraph {Correct: Simultaneous upgrade}

temp0 := $\theta_0 - \alpha \frac {\delta}{\delta \theta_0}
J(\theta_0,
\theta_1)$ \\
temp1 := $\theta_1 - \alpha \frac {\delta}{\delta \theta_1}
J(\theta_0,
\theta_1)$ \\
$\theta_0$ := temp0 \\
$\theta_1$ := temp1 \\

\subsection {Gradient Descent Algorithm For Linear Regression }
\label {2-7}
После подстановок - получаем частные производные:
\[ \frac {\delta}{\delta \theta_j} J(\theta_0, \theta_1) = \frac
{\delta}{\delta \theta_j} \frac{1}{2m} \sum\limits_{i=1}^m\left(
  h_\theta (x^{(i)}) - y^{(i)} \right)^2 = \frac {\delta}{\delta
  \theta_j} \frac{1}{2m} \sum\limits_{i=1}^m\left(\theta_0 + \theta_1
  x^{(i)} - y^{(i)} \right)^2
\] \\
Тогда:
\[
\theta_0 : \frac {\delta}{\delta \theta_0} J(\theta_0, \theta_1) =
\frac{1}{m} \sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)}
\right) \]
\[
\theta_1 : \frac {\delta}{\delta \theta_1} J(\theta_0, \theta_1) =
\frac{1}{m} \sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)}
\right) * x^{(i)}
\]
Тогда алгоритм будет: \\
repeat until convergence \{ \\
\
\[\theta_0 := \theta_0 - {\alpha} \frac{1}{m}
\sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)} \right) \]
\[\theta_1 := \theta_1 - {\alpha} \frac{1}{m}
\sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)} \right) *
x^{(i)} \] \} Утверждает, что Cost Function для Linear Regression
всегда является ``bow shaped'', или ``Convex function'' - т.е. не
имеет локальных минимумов; единственный минимум - глобальный.

Дальше, он называет сей процесс ``Batch'' Gradient Descent - т.к. Each
step of gradient descent uses all the training examples.

\subsection {Extensions}
\label {2-8}
1. Существует прямой (не - итерационный) метод обсчета $\theta_0,
\theta_1$ \\
2. БОльшее количество параметров (features): $y = F(x_1, x_2, ...,
x_n)$

\label{Chapter 3}
\section {Linear Algebra Review}
Ничего интересного, обозначения:
\[
\left( \begin{array}{cc}
    1402 & 191 \\
    1371 & 821 \\ 949 & 1437 \\ 147 & 1448 \\
  \end{array} \right) = \mathbb{R}^{4\times2}
\]
Identity Matrix - единичная матрица вроде $\left( I
  = \begin{array}{cc}
    1 & 0 \\ 0 & 1 \\
  \end{array}
\right)$

Свойства: $A I = I A = A$ Обычно умножение матриц некомутативно, но
ассоциативно: $A B \neq B A$, но $(A B) C = A (B C)$

Inverse matrix - обратная матрица Transpose matrix - транспонированная
матрица

\label{Chapter 4}
\section {Linear Regression with Multiple Variables}
Notation: \\
\begin{itemize}
\item {m} = number of examples (size of training set)
\item {n} = number of features
\item{$x^{(i)}$} = input (features) of $i^{th}$ training example
  (vector of size n)
\item {$x_j^{(i)}$} = value of feature j in $i^{th}$ training example
\end{itemize}
Hypothesis: \\
\[
h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + ...
\theta_nx_n
\]
For convenience in notation , define $x_0=1$. $(x_0^{(i)}=1)$: Feature
vector (values for a single feature) is of dimention n+1:
\[
x = \left[ \begin{array}{c} x_0 \\ x_1 \\ x_2 \\ \vdots \\ x_n
  \end{array} \right] \in \mathbf{R^{n+1}}
\]
The same story (n+1 dimentional vector) - for parameters:
\[
\theta = \left[ \begin{array}{c} \theta_0 \\ \theta_1 \\ \theta_2 \\
    \vdots \\ \theta_n
  \end{array} \right] \in \mathbf{R^{n+1}}
\]
(another n+1 dimentional vector) Then hypothesis can be written as:
\[
h_\theta(x) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 +
... \theta_nx_n = \\ \theta^TX
\]
where: $\theta^T = [\theta_0 \theta_1 ... \theta_n]$ - so-called {\bf
  raw vector}. \\

This form is also called {\bf Multivariate linear regression }.

\subsection {Gradient Descent for Multiple Variables}
Cost function:
\[
J(\theta_0, \theta_1, ..., \theta_n) = \frac{1}{2m}
\sum\limits_{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)} \right)^2
\]
Gradient descent: \\
Repeat \{
\[ \theta_j := \theta_j - \alpha \frac
{\delta}{\delta\theta_j}J(\theta) \\
= \theta_j - \alpha \frac{1}{m}
\sum\limits_{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)}
\right)x_j^{(i)} \]
\} - simultaneously update for every j = 0,1,...n \\
for j = 0, the formula is the same as of Single-Variable Descent (as
$x_0 = 1$)
\label {4-3}
\subsection {Gradient Descent in practice}
Предлагается масштабировать features так, чтобы избегать ``вытянутых''
контуров на $\theta_1(\theta_2)$ диаграммах, т.к. на оных вытянутых
диаграммах процесс сходится плохо. Сие называется

\subsubsection{ Feature Scaling} 
- get every feature into approximately a $-1 \leq x_i \leq 1$ range.
\subsubsection {Mean normalisation}
- replace $x_i$ with $\frac{x_i - \mu_i}{S_i}$ to make features have
approximately zero mean (do not apply to $x_0 = 1$):
\begin{itemize}
\item{$\mu_i$} - mean value
\item{$S_i$} - the range (max - min) or standard deviation (would be
  fine too)
\end{itemize}
Example:
\begin{equation*}
  \begin{split}
    x_1 = \frac{size-1000}{2000} \\
    x_2 = \frac{\#bedrooms-2}{5} \\
    -0.5 \leq x_1 \leq 0.5, -0.5 \leq x_2 \leq 0.5
  \end{split}
\end{equation*}

\subsection {Gradient Descent in Practice - Learning Rate}
\label {4-4}
Gradient descent works correctly if $J(\theta)$ decrease after every
iteration. Usually convergence is declared if $J(\theta)$ decreases by
less then some constant (i.e. $10^-{3}$) in one iteration.

If $J(\theta)$ increases (oscillates up and down) - this usually means
that learning rate $\alpha$ is too big (but convergence becomes
slower).

So it is recommended to try \[ ..., 0.001,0.003, 0.01, 0.03, 0.1, 0.3,
1, ...
\]
 
\subsection{Features and Polynomial Regression}
\label{4-5}
\subsubsection{Polynomial Regression}
In polynomial regression formulas like $h_\theta(x)=\theta_0 +
\theta_qx + \theta_2x^2 + \theta^3x_3$ we can re-write the expression
as $h_\theta(x) = \theta_0 + \theta_qx_1 + \theta_2x_2 + \theta_3x_3$
where \begin{itemize}
\item{$x_1=x$}
\item{$x_2=x^2$}
\item{$x_1=x^3$}
\end{itemize}
and use the mechanics of the ``normal''linear regression. In addition,
it is usually a better idea to use some sort of ``inside'' information
- in order to choose the ``right'' set of feature. Such selection
process can also be automated.

\subsection {Normal Equation}
\label{4-6}
To solve the $\theta$ analytically. For 1-D features we can use
derivatives: $\frac{d}{d\theta}J(\theta) = 0$, solve it for $\theta$.
But if $\theta \in \mathbb{R}^{n+1}$, we need to solve \[
\frac{\delta}{\delta \theta_j}J(\theta) = \frac{\delta}{\delta
  \theta_j} \left( \frac{1}{2m}
  \sum\limits_{i=1}^m\big(h_\theta(x^{(i)}) - y^{(i)}\big)^2\right) =
0
\] for every j

The idea is: pack the training set into matrix $X^{m \times (n + 1)}$
(so-called design matrix) for features (n features + ones for $x_0$)
and m - dimentional vector y for results. Then:
\[ \theta = \mathbf{\left( X^TX \right)^{-1}X^T}y \] values in
$\theta$ will minimise the cost function. Feature scaling in this case
is NOT actual.

Octave: \emph{ pinv(X'*X)*X'*y }

\paragraph {Normal Equation Gradient Descent)} No need to choose
$\alpha$, don't need to iterate.

\paragraph {Gradient Descent vs Normal Equation} Gradient Descent
works well with large $n$, while calculation $n \times n$ matrix for
large n could be painful.

\subsection {Normal Equation Non-invertibility}
Idea: what if $\mathbf{X^TX}$ is non- invertible (singular)?
Causes:\begin{itemize}
\item Redundant features (linearly dependent). \\
  E.g. $x_1$ = size in $feet^2$, $x_2$ = size in $m^2$
\item Too many features ($e.g. m \leq n$). \\
  Delete some features or use regularisation.

\end{itemize}

In Octave, \emph{pinv(X'*X)*X'*y} will end with some meaningful result
- this is different from ``just'' \emph{inv} function (see Octave doc
for difference between the ``normal'' inversion inv and ``pseudo'' -
inversion pinv functions).

\section {Octave Tutorial}
\label {Chapter 5}
Add ``;'' to make a command (not output it immediately)\\
Commands can be comma - chained Commands / operators:\begin{itemize}
\item {PS1('>> ');} set up the command prompt
\item {help command} prints help on command
\item {\verb!~=!} works as ``not equal'' logical operator
\item {constants}: pi,
\item {output} \verb!disp(sprintf('2 decimals: %0.2f', a))!
\item {switch default output format}: format long / format short
\end{itemize}
\subsubsection {matrix}
\begin{verbatim}
A = [1 2; 3 4;
> 5 6]

>> V = [1 2 3]
\end{verbatim}
Automated init:
\begin{verbatim}
>>v = 1:0.1:2 % creates vector with 11 values: from 1.0 step 0.1 to 2.0
>>v = 1:6 % creates vector [1..6]
>> ones(2, 3) % generates 2 * 3 matrix filled with 1
>> w = zeros(1,3) % generates vector of "o"
>> rand(3,3) % generates matrix, inits with random (normal
distribution values in 0..1 range)
>> rand(1, 3) % generates matrix, inits with random (Gaussian
distribution with mean = 0 and sigma = 1)
>> eye(6) % identity matrix 6 * 6
\end{verbatim}
\verb!hist(w)! builds a histogram for given vector w

\subsection {Moving data around}
\label{5-2}
\begin{verbatim}
>> size (A) % returns dimensions of matrix A
>> size (A, 1) % returns the value for first dimension of A (rows)
>> size (A, 2) % returns the value for second dimension of A (columns)
>> length(v) % returns length of vector. For matrix - returns the
% longer dimension of matrix
>> pwd, cd, ls,   % CO
>> load filename.ext / load('filename.ext') % loads text files with
% data into relevant variable
>> who % lists variables in memory
>> whos % more detailed 'who'
>> clear <variable name> % deletes variable from memory. If called
% without parameter - clears all variables
>> v = V(1:10) % get the first 10 elements of V
>> save hello.mat v; % saves vector v to file hello.mat (binary
% format) to save in human - readable format, use -ascii option

>> A(3, 2) % returns element of matrix A
>> A(2, :) % fetch everything in the second row. Works for columns as
% well; can be used for assigning
>> A([1 3], :) % get everything from rows 1 and 3
>> A = [A , [100; 101; 102]]; % appends the column vector to the
% matrix (notice ';' as delimiter for elements in column)
>> A(:) % put all elements of A into a single vector
>> C = [A B] % concatenate matrices into a new one (horizontally).
% Same is [A, B] (comma instead of space)
>> C = [A;B] % concatenate matrices vertically 
\end{verbatim}

\subsection{Computing on Data}
\label{5-3}
\begin{verbatim}
>> A*C % matrix multiplication
>> A .* B % element-wise operations: each element of A is being
% multiplied by corresponding element of B
>> A .^ 2 % element-wise power
>> 1 ./ v % element-wise reciprocal of v (every element of a new
% vector is 1 / corresponding element of v)
>> log(v), exp(v), abs(a), -v % element-wise log, exp, abs, negative
>> v + ones(length(v), 1) % adds 1 to every element of v (one can do
% it simpler v + 1)
>> A' % A transposed
>> max(v) % maximum element of v. For matrix - give "per-row" maximum
% by default. Trick to calculate the maximum element of matrix:
% max(A(:))
>> [val, ind] = max(v) % assigns val to max value, ind, to the index
% of max element from v
>> v < 3 % element-wise comparing (builds matrix with 0/1 per element)
>> find (v < 3) % returns vector with elements satisfying the
% condition
>> magic(3) % returns "magic square"
>> sum(A), prod(A) % sum and product of all elements of matrix. By
% default - "per-column" (1st dimention - just like sum(A,1). Per-row
% will be calculated by sum(A(:))
>>sum(sum(A.*eye(9))) % trick to calculate the sum of all elements on
% the matrix diagonal. For other diagonal: sum(sum(A.*flipud(eye(9))))
>> floor(A), ceil(A) % rounding all elements
>> rand(3) % create random matrix 3x3
>> max(a, b) % generate matrix with values = element-wise max of both.
% Also exist "per-row" and "per-column" form
>> pinv(A) %"pseudo-inverse matrix 
\end{verbatim}

\subsection{Plotting Data}
\label{5-4}
\begin{itemize}
\item{plot(t, y)} - plot graph with points $t_i, y_i$ from vectors
\item{hold on} - plot 1st graph, ``hold on'', plot another one
\item{xlabel('time'), ylabel('value')} - set labelx for x and y
\item{legend('sin', 'cos'), title('my plot')} - legend and title
\item{print -dpng 'myPlot.png'} - save to file
\item{close} - remove plot
\item{figure(1);plot(t,y1);figure(2);plot(t,y2);} - builds two plots
  on different widows
\item{subplot(1, 2, 1)} - divide plot a 1x2 grid, access first element
\item{axis([0.5 1 -1 1])} - set x- and y- ranges
\item{clf;} - clear
\item{imagesc(A)} - draw image based on goven matrix A
\item{imagesc(A), colorbar, colormap grey} - as before, add colorbar
  legend, make legend grey-scale
\end{itemize}

\subsection{Control Statement}
\label{5-5}
\subsubsection{Control Statements}
\begin{verbatim}
for i=1:10
 v(i) = 2^i;
end;
% or, having a vector like indices=1:10; we can do similar:
for i=indices,
 disp(i);
end;
% _break_ and _continue_ apply
================
i = 1;
while i <= 5,
 v(i) = 100;
 i = i + 1;
end;
========
if i == 6, break;
=======
if i == 6, disp('oops');
elseif v(i) == 2, disp('aah');
else disp('grgrg');
end;
\end{verbatim}
\subsubsection{functions}
Create file named <function-name>.m (text). Example:
\begin{verbatim}
function y = squareThisNumber(x)

y = x^2;
\end{verbatim}
Then use it in Octave:
\begin{verbatim}
>> cd <folder with file>
>> squareThisNumber(5)
===== or use addpath ('some\path\to\function') - see search path idea
\end{verbatim}
Multiple-variable returning functions
\begin{verbatim}
function [y1, y2] = squareAndCubeThisNumber(x)

y1 = x^2;
y2 = x^3;
\end{verbatim}
Variant: $J(\theta)$
\begin{verbatim}
function j = costFunctionJ(X, y, theta)
% X - "design matrix" with training examples y - class labesls

m = size(X, 1)  % number of training examples
predictions = X*theta % predictionsof hypothesis on all m examples
sqrError = (predicitons-y).^2; %squared error (element-wise)

J = 1/(2*m) * sum(sqeErrors);
\end{verbatim}
\subsection{Vectorization}
\label{5-6}
Idea: instead of using cycles (loops) over data matrices or vectors,
use matrix(vector) multiplication operations.

\label {Chapter 6}
\section {Logistic Regression}
\subsection{Classification}
Idea is to separate the input into cathegories like Spam/Ham,
Malignant / Benign tumor etc. Separation: \begin{itemize}
\item {Two-class} (``binary class'') classification $y \in \{0, 1\}$
\item {Multi-class} classification $y \in \{0, 1, ... n\}$
\end{itemize}
Idea: to use Linear Regression with $h_\theta(x) = \theta^T x$ and
threshold which checks: \begin{itemize}
\item{if $h_\theta(x) \geq 0.5$} predict 1
\item{if $h_\theta(x) < 0.5$} predict 0
\end{itemize}
Problem emerges when the training set is not lineary separated, so
``if paramater greater ... then ...'' does not work. In addition, h
can be > 1 or < 0. As such , we need something like \[ Logistic
Regression: 0 \leq h_\theta(x) \leq 1
\]
It is named ``regression'' for historical reason, but this is actually
a classification algorithm.

\subsection{Hypothesis Representation}
We use Linear Regression Model ($h_\theta(x) = \theta^Tx$ ) with
so-called {\bf Sigmoid function} or {\bf Logistic function} : \[
h_\theta(x) = g(\theta^Tx), where \]
\[g(z) = \frac{1}{1 + e^{-z}}\] so
\begin{equation*}
  h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}
\end{equation*}
(see function graph for this)
\subsubsection{Interpretaion}
$h_\theta(x) = $ estimated probability that y=1 on input x
\begin{equation*}
  \textrm{Example: if }  x = \left[ \begin{array}{c} x_0 \\
      x_1  \end{array} \right] = \left[ \begin{array}{c} 1 \\
      tumorSize  \end{array} \right]
\end{equation*}
\begin{equation*} h_\theta(x) = 0.7 \end{equation*} - then patient has
70\% chance of tumor being malignant. Or: $ h_\theta(x) = \frac{1}{1 +
  e^{-\theta^Tx}}$ - ``probability that y = 1, given
x, parametrized by $\theta$'' \\
As y = 0 or 1, then:
\[P( y = 0 | x;\theta) + P(y=1|x;\theta) = 1\]
\[P( y = 0 | x;\theta) = 1 - P(y=1|x;\theta)\]

\subsection{Decision Boundary}
Suppose predict ``y = 1'' if $h_\theta(x) \geq 0.5$ \\
and ``y = 0'' if $h_\theta(x) < 0.5$ \\

as $g(z) \geq 0.5 \ when \ Z \geq 0 \Rightarrow h_\theta(x) =
g(\theta^Tx) \geq 0.5 \ whenever \ \theta^Tx \geq 0$
\label {page 10 from pdf}

Decision boundary - a line that separates regions with different
predictions. It is a property of a hypothesis (not training set!).

\subsection{Cost Function}
\label{sec:6-4}
Training set: $\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \dots ,
(x^{(m)},y^{(m)})\}$ of m examples; x is a vector of
$\mathbf{R^{n+1}}$, where $x_0 = 1, y \in \{0, 1\}$. We're choosing
the parameters $\theta$ to minimise
\[ \textrm{hypothesis:} h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}} \]
\[ \textrm{Modify Linear regression:} J(\theta) = \frac{1}{m}
\sum\limits_{i=1}^m Cost (h_\theta(x^{(i)}, y^{(i)}) \]
\[ \textrm{where } Cost (h_\theta(x^{(i)}, y^{(i)}) =
\frac{1}{2}(h_\theta(x^{(i)}) - y^{(i)})^2 \]

Logistic cost function is ``non-convex'', which means it can have a
number of local minimums, as a result we can not work with derivative.
Instead, we use iteration method based on \[ \textrm{hypothesis:}
Cost(h_\theta(x), y) = \left\{ \begin{array}
    {rr} -log(h_\theta(x)) & \textrm{if y = 1} \\
    -log(1- h_\theta(x)) & \textrm{if y = 0}
  \end{array} \right. \]

\paragraph{for y=1:}
Cost = 0 if y = 1, $h_\theta(x) = 1$
but as $h_\theta(x) \to 0 \Rightarrow \textrm{Cost } \to \infty $ \\
That captures intuition that if $h_\theta(x) = 0$, predict
$P(y=1|x;\theta) = 0$, but for y=1 we'll penaltize algorithm by a very
large cost.

\paragraph{for y=0:}
Cost = 0 for $h_\theta(x) = 0$, penalty for $h_\theta(x) = 1$

Together it gives convex function suitable for iterative processing.

\subsection{Gradient Descent}
\label{sec:6-5}
\[\textrm{minimising } J(\theta) = \frac{1}{m} \sum\limits_{i=1}^m
Cost (h_\theta(x^{(i)}, y^{(i)}) \] Initial Cost function is being
re-written as \[ Cost(h_\theta(x), y) = -ylog(h_\theta(x)) -
(1-y)log(1-h_\theta(x))
\]

Then J can be expressed as \[ J(\theta) = -\frac{1}{m} \left[
  \sum\limits_{i=1}^m y^{(i)} \log (h_\theta(x^{(i)})) + (1-y^{(i)})
  \log (1 - h_\theta(x^{(i)})) \right] \]

To make a prediction given new x: \[ \textrm{Output } h_\theta(x) =
\frac{1}{1 + e^{-\theta^Tx}} \quad \textrm{which means probability :}
p(y=1|x; \theta)
\]

\paragraph{Gradient Descent}
Calculating $\min_\theta J (\theta)$: \\
\[ \begin{array}{l}
  \textrm{Repeat }  \{ \\
  \theta_j := \theta_j - \alpha \frac {\delta}{\delta \theta} J
  (\theta) \\
  \textrm{where } \frac {\delta}{\delta \theta} J(\theta) =
  \frac{1}{m} \sum\limits_{i=1}^m (h_\theta(x^{(i)}) -
  y^{(i)})x_j^{(i)} \textrm{ -
    partial derivative} \\
  \textrm{so } \theta_j := \theta_j - \alpha \sum\limits_{i=1}^m
  (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\
  \}
\end{array} \]
{\bf simultaneously update all $\theta_j$} - looks exactly like the Linear
Regression algorithm (just hypothesis $h_\theta(x)$ is different), so
implementation can use both loop from 0 to m or vector form as well as
feature scaling idea.

\subsection{Optimisation}
\label{sec:6-6}
The idea is to play with different control algorithms (Gradient
Descent, Conjugate Descent, BFGS, L-BFGS) supplying them ways to
compute $J(\theta)$ and $\frac{\delta}{\delta \theta_j}J(\theta)$
(through some sort of plug-ins - ?).

The last three algorithms are more complex then the ``normal''
Gradient descent but:
\begin{itemize}
\item No need to manually pick $\alpha$ (learning rate)
\item Often faster then ``normal'' gradient descent
\end{itemize}

In Octave, this is implemented through generalised function and
function reference (delegate ?):

Example: \[\theta = \left[ \begin{array}{c} \theta_1 \\ \theta_2
  \end{array} \right]; J(\theta) = (\theta_1 - 5)^2 + (\theta_2- 5)^2 \]
\[\textrm{then } \frac{\delta}{\delta \theta_1}J(\theta) = 2(\theta_1
-5) ;\frac{\delta}{\delta \theta_2}J(\theta) = 2(\theta_2 - 5)
\]
in Octave:
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  function [jVal, gradient] = costFunction(theta) jVal = (theta(1) -
  5)^2 + (theta(2)-5)^2; gradient = zeros(2,1); gradient(1) = 2 *
  (theta(1) - 5); gradient(2) = 2 * (theta(2) - 5);
\end{lstlisting}
then
\begin{lstlisting} [language=Octave, caption=={Algorithm Call }]
  % set up process options
  options = optimset('GradObj', 'on', 'MaxIter', '100'); initialTheta
  = zeros(2, 1); [optTheta, functionVal, exitFlag] =
  fminunc(@costFunction, initialTheta, options);
\end{lstlisting}
This returns optTheta (value of $\theta$ for optimal point),
fuctionVal - cost value in optimal point, and exitFlag (1 for normally
converged operation).

Works for $\theta \in \mathbf{R^d}; d \geq 2$

\subsection{Mutliclass Classification}
\label{sec:6-7}
When you need to classify by more then 2 categories (as in Binary
Classification).

Idea is called ``One-vs-All'' or ``One-vs-Rest''. Build classifiers
[$h^{(1)}(x), h^{(2)}(x), h^{(3)}(x)... $] for each class: \[
h^{(i)}(x) = P(y = i|x;\theta) \ (i = 1, 2, 3)
\]
On a new input x, to make a prediction, pick the class i that
maximises $\max_i h^{(i)}(x)$

\section{Regularisation}
\label{sec:7}

\subsection{The Problem of Overfitting}
\label{sec:7-1}
The ``underfitting' (or ``high bias'') problem takes place when we try
to fit, say, linear model to the task which is rather quadratic - so
we have too strong pre-conception to the idea of the linear model.

Over-fitting (``high variance'') is a vise-verse state: for instance,
we apply the model of power 4 to the problem which would be fine
described by quadratic model. The graph fits to the training data
really well, but the curve itself becomes too ``windy''.

So, if we have too many features, the learned hypothesis may fit the
training set very well ($J(\theta) = \frac{1}{2m} \sum
\limits{i=1}{m}(h_\theta(c^{(i)}) - y^{(i)})^2 \approx 0$) but fail to
generalise to new examples (predict output on new examples). Similar
story - with Logistic Regression.

Addressing over-fitting:
\begin{enumerate}
\item Reduce number of features
  \begin{itemize}
  \item Manually select which features to keep
  \item Model selection algorithm
  \end{itemize}
\item Regularisation
  \begin{itemize}
  \item Keep all the features, but reduce magnitude/values of
    parameters $\theta_j$
  \item Works well when we have a lot of features, each of which
    contributes a bit to predicting y.
  \end{itemize}
\end{enumerate}

\subsection{Cost Function}
\label{sec:7-2}
The idea is to have small values high-order parameters. This will make
our hypothesis simpler and less prone to over-fitting. This is done
through ``penalising'' values in our cost function (keep in mind that
for our case $x^n$ is equivalent to $\theta_n$ so we can use the same
idea for both multi-feature training set and for high-order polynom:

\[J(\theta) = \frac{1}{2m} \left[ \sum \limits{i=1}{m}
  (h_\theta(x^{(i)}) - y^{(i)})^2 + \underbrace{ \lambda \sum
    \limits_{i=1}^{n}\theta_j^2}_{penalty} \right] \] Notice the
regularisation term (a ``penalty'') does not include $\theta_0$ - this
is a sort of convention. $\lambda$ is called ``regularisation
parameter''; it provides balance between two tasks:
\begin{itemize}
\item fitting the training set well
\item to keep the parameter values small
\end{itemize}

If $\lambda$ is too high, all $\theta_i$ end up near 0, so the
hypothesis becomes too flat - i.e. ``under-fit'': $h_\theta(x) =
\theta_0$.

\subsection{Regularised Linear Regression}
\label{sec:7-3}
For ``updated'' Linear Regression (with regularisation parameter), the
Gradient Descent looks a bit different:
\[ \begin{array}{l}
  \textrm{Repeat }  \{ \\
  \theta_0 := \theta_0 - \alpha \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_0^{(i)} \\
  \theta_j := \theta_j - \alpha \left[ \frac {1}{m} \sum
    \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
    \underbrace{+
      \frac{\lambda}{m}\theta_j} _{penalty} \right] \\
  (j= 1, 2, 3, \dots , n) \textrm{ - no zero here!} \\
  \}
\end{array}
\]

- actually, the good old partial derivative on regularised hypothesis.
After some transformations: \[ \theta_j := \theta_j(1 -
\alpha\frac{\lambda}{m}) - \alpha \frac{1}{m} \sum
\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
\]
and $1 - \alpha \frac{\lambda}{m} < 1$, usually about 0.99 (roughly).
Then first part looks like $\theta_j * 0.99$. While second part is a
``normal'' regression update, each step of the algorithm reduces
$\theta_j$.

\subsubsection{Normal Equation}
\label{sec:7-2-2}
\begin{equation*}
  \begin{array}{ll}
    \mathbf{X} = \left[ \begin{array}{c} (x^{(1)})^T \\ (x^{(2)})^T \\
        \vdots \\ (x^{(m)})^T \end{array} \right] & y =
    \left[ \begin{array}{c} y^{(1)}  \\ y^{(2)}  \\  \vdots \\ y^{(m)} \end{array} \right]
  \end{array}
\end{equation*}
where X is $M \times (n+1)$ matrix, y is vector of length m. To
minimise cost function $J(\theta)$: \[ \frac{\delta}{\delta
  \theta_j}J(\theta) \Rightarrow 0 \]
\[
\theta = \Big(X^T X + \lambda \left[ \begin{array}{ccccc} 0 & 0 & 0 &
    \dots & 0 \\ 0 & 1 & 0 & \dots & 0 \\ 0 & 0 & 1 & \dots & 0 \\
    \vdots & \vdots & \vdots & \ddots & 0 \\ 0 & 0 & 0 & \dots &
    1 \end{array} \right] \Big)^{-1} X^Ty
\]
This terrible matrix is of $(n+1) \times (n+1)$ size.

\paragraph{Non-invertibility}
Suppose $m \leq n$, for our $\theta = (X^T X)^{-1}X^Ty$, if $\lambda >
0$:

Then $(X^T X)$ is non-invertible (singular). ``pinv'' function could
still give some sane result, but, if working with different language
(or using regular inverse - inv),this would not work.

It is possible to prove that sum of $(X^T X)$ and this ugly matrix is
invertible (non-singular) for $\lambda > 0$

E.g. Regularisation helps in this pathetic case.

\subsection{Regularised Logistic Regression}
\label{sec:7-4}
Regularised cost function; \[ J(\theta) = -\left[ \frac{1}{m} \sum
  \limits_{i=1}^{m} y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)})
  \log (1 - h_\theta(x^{(i)})) \right] \underbrace{+
  \frac{\lambda}{2m} \sum \limits_{j=1}^{n} \theta_j^2} _{penalty}
\]

Cosmetically, the algorithm looks the same:
\[ \begin{array}{l}
  \textrm{Repeat }  \{ \\
  \theta_0 := \theta_0 - \alpha \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_0^{(i)} \\
  \theta_j := \theta_j - \alpha \underbrace{ \left[ \frac {1}{m} \sum
      \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
      \underbrace{+ \frac{\lambda}{m}\theta_j } _{penalty}
    \right]}_{\frac{\delta}{\delta \theta_j}J(\theta)} \\
  (j= 1, 2, 3, \dots , n) \textrm{ - no zero here!} \\
  \}
\end{array}
\]
where hypothesis is a bit different: $h_\theta(x) = \frac{1}{1 +
  e^{-\theta^Tx}}$

\subsubsection{Advanced Optimisation}
\label{sec:7-4-2}
In Octave:
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  function [jVal, gradient] = costFunction(theta) jVal = [code to
  compute |$J(\theta)$|]; |\[\textrm{becomes } J(\theta) = \left[
    -\frac{1}{m} \sum \limits_{i=1}^{m} y^{(i)} \log h_\theta(x^{(i)})
    + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)})) \right] +
  \frac{\lambda}{2m} \sum \limits_{j=1}^{n} \theta_j^2 \]| gradient(1)
  = [code to compute |$\frac{\delta}{\delta \theta_0} J(\theta)$|];
  |\[\frac {1}{m} \sum \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})
  x_0^{(i)} \]| gradient(2) = [code to compute |$\frac{\delta}{\delta
    \theta_1} J(\theta)$|]; |\[ \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_1^{(i)} +
  \frac{\lambda}{m}\theta_1 \]| gradient(3) = [code to compute
  |$\frac{\delta}{\delta \theta_2} J(\theta)$|]; |\[ \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_2^{(i)} +
  \frac{\lambda}{m}\theta_2 \]| ..... gradient(n+1) = [code to compute
  |$\frac{\delta}{\delta \theta_n} J(\theta)$|]; |\[ \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_n^{(i)} +
  \frac{\lambda}{m}\theta_n \]|
\end{lstlisting}

\section{Neural Networks: Representation}
\label{sec:8}
- to cope with the problems of too large feature sets. - to give ``one
more'' approach to the learning problem

\subsection{Neurons and Brain}
\label{sec:8-2}
The ``one learning algorithm'' hypothesis was quite popular 80-90s.
The idea was to re-use the ``almost universal'' algorithms to ``almost
any'' sensors.

\subsection{Model Representation}
\label{sec:8-3}
Simulating neurons in the brain:
\begin{itemize}
\item ``Input'' wires - dendrites
\item ``Output'' wire - axon
\item dendrite on one neuron is connected to the axon on another. The
  neuron itself makes some ``computations'' when passing signal.
\end{itemize}

Neuron model represents a neuron as a logistic unit with a number of
inputs and single output function (hypothesis) like $ h_\theta(x) $
like: \[ x = \left[ \begin{array}{c} x_0 \\ x_1 \\ x_2 \\
    x_3 \end{array} \right];
\theta = \left[ \begin{array}{c}  \theta_0 \\ \theta_1 \\ \theta_2 \\
    \theta_3 \end{array} \right] \textrm{(sometimaes called
  "weights")} \] and $h_\theta(x)=\frac{1}{1 + e^{-\theta^Tx}}$ is
called ``Sigmoid (logistic) activation function''. Sometimes $x_0$ is
referred as ``bias unit'' (``bias neuron'' on the diagram). It is
always equal to 1.

\subsubsection{Neural Network}
The Neural Network is a group of neurons like the one on a picture.

Layer 1 neurons are ``input neurons''; intermediate (Layer 2) is also
called ``the hidden layer'' (contains $a_0$ - the ``bias unit'' ; the
final one (Layer 3) - ``output layer''. Notations:
\begin{itemize}
\item {$a_j^{(j)}$} = ``activation'' of unit i in layer j
\item {$\theta^{(j)}$} = matrix of weights controlling function
  mapping from layer j to layer j + 1.
\end{itemize}
Example: for network with three input units and three hidden ones
(assuming g(z) is the sigmoid function):
\begin{equation}
  \label{eq:8-4}
  \begin{array}[c]{c}
    a_1^{(2)} = g(\theta_{10}^{(1)}x_0 + \theta_{11}^{(1)}x_1 +
    \theta_{12}^{(1)}x_2 + \theta_{13}^{(1)}x_3)  \\
    a_2^{(2)} = g(\theta_{20}^{(1)}x_0 + \theta_{21}^{(1)}x_1 +
    \theta_{22}^{(1)}x_2 + \theta_{23}^{(1)}x_3)  \\
    a_3^{(2)} = g(\underbrace{\theta_{30}^{(1)}x_0 + \theta_{31}^{(1)}x_1 +
      \theta_{32}^{(1)}x_2 + \theta_{33}^{(1)}x_3}_{z_3^{(2)}})
  \end{array}
\end{equation}
Finally,
\[ h_\theta(x) = a_1^{(3)} = g(\theta_{10}^{(2)}a_0^{(2)} +
\theta_{11}^{(2)}a_1^{(2)} + \theta_{12}^{(2)}a_2^{(2)} +
\theta_{13}^{(2)}a_3^{(2)} )\]
Dimensions: \\
If network has $s_j$ units in layer j, $s_{j+1}$ units in layer j+1,
then $\theta^{(j)}$ will be of dimension $s_{j+1} \times (s_j + 1)$.
\\
The whole hypothesis looks like $h_\theta (X)$ (in vectors!) -
``parametrized'' by $\theta$
\label{8-4}
 
\subsubsection{Calcualtion and Optimisation}
Terminology: The $\theta_{10}^{(1)}x_0 + \theta_{11}^{(1)}x_1 +
\theta_{12}^{(1)}x_2 + \theta_{13}^{(1)}x_3$ part is represented as
$z_1^{(2)}$ so that $a_1^{(2)} = g(z_1^{(2)})$.
% \label {8-4: 02:01}
The whole set of $z_1^{(i)}$ from \ref{eq:8-4} can be written as
$\theta^{(1)} \times x$

Further vectorization (with $z_0^{(2)} = 1$ always):
\begin{equation*}
  \label{eq:8-4:02:25}
  \begin{array}[c]{l}
    x = \left[  \begin{array}[c]{c}   x_0 \\ x_1 \\ x_2 \\ x_3 \end{array} \right]
    z^{(2)} = \left[\begin{array}[c]{c} z_1^{(2)} \\ z_2^{(2)}
        \\ z_3^{(2)} \end{array} \right] 
    \\ \\    z^{(2)} = \theta^{(1)}x 
    \\    a^{(2)} = g(z^{(2)})
  \end{array}
\end{equation*}
- both $a^{(2)}$ and $z^{(2)}$ are $\mathbf{R}^3$

Then , as $x = a^{(1)}$ (activation for the first layer), we can
generalise: \[ z^{(2)} = \theta^{(1)} a^{(1)}\]

With bias unit: $a_0^{(2)} = 1 \Rightarrow a^{(2)} \in
\mathbf{R}^4$: \[ z^{(3)} = \theta^{(2)}a^{(2)} \]
\[ h_\theta(x) = a^{(3)} = g(z^{(3)})\]

The whole story is called {\bf Forward Propagation}. At each level,
the process is our good old Logistic Regression, ``learning its own
features'', ergo, we can play with different architectures instead of
playing with high - order features as in Logistic Regression.

\subsection{Examples and Intuition}
\label{sec:8-5}
Exotic example 1: hypothesis like $y = x_1 \textrm{ XOR } x_2$ or $y =
x_1 \textrm{ NXOR } x_2$

Simple example: AND:
\[ x_1, x_2 \in \{0, 1\}\] \[ y = x_1 \textrm{ AND } x_2 \] The whole
network consists of 3 input parameters ($x_1, x_2$ and bias unit
``+1'') and single hidden unit (single-neuron network). We could solve
the task by assigning: $ \theta = [-30, +20, +20]$ - that is,
$h_\theta(x0) = g(-30 + 20 x_1 + 20 x_2)$

OR function is built similarly: $ \theta = [-10, +20, +20]$

Negation (NOT): $ \theta = [+10, -20]$ (2 input parameters in total)

Returning to $x_1 \textrm{ XNOR } x_2 = (x_1 \textrm{ AND } x_2)
\textrm{ OR } ((\textrm{NOT } x_1) \textrm{ AND } (\textrm{NOT }
x_2))$:
\[
\begin{array}{l}
  a_1^{(2)} = (-30 + 20 x_1 + 20 x_2) \\  
  a_2^{(2)} = (+10 - 20 x_1 - 20 x_2)  \\
  \textrm{ level 3: } a_1^{(3)} = (-10 + 20 a_1^{(2)} + 20 a_2^{(2)})
  \Rightarrow h_\theta(x)
\end{array}
\]
\label{8-6}

\subsection{Multiclass Classification}
\label{sec:8-7}
Output layer consists of more then one unit: n units each representing
probability of relevant output. As such, training examples will
be \[(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), (x^{(3)}, y^{(3)}), \dots
(x^{(m)}, y^{(m)}) \], where $x^{(i)}$ are images, and each $y^{(i)}$
is one of \[ \begin{array}{cccc} \left[ \begin{array}{c} 1 \\ 0 \\ 0
      \\ 0 \end{array} \right], & \left[ \begin{array}{c} 0 \\ 1 \\ 0
      \\ 0 \end{array} \right], & \left[ \begin{array}{c} 0 \\ 0 \\ 1
      \\ 0 \end{array} \right], & \left[ \begin{array}{c} 0 \\ 0 \\ 0
      \\ 1 \end{array} \right]
\end{array}\]
and use ``one-vs-all'' approach to training


\section{Neural Networks: Learning}
\label {sec:9}

\subsection{Cost Function}
\label{sec:9-1}
Notation:
\[ \{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}) \dots (x^{(m)}, y^{(m)})
\} \textrm{ - samples} \]
\[ \mathbf{L} \textrm{ = total no of layers in network} \]
\[ s_l \textrm{ = no of units (not counting bias unit in layer l}\]

We have 2 variants of the classification problem:

\subsubsection{Binary Classification}
y = 0 or 1; 1 output unit; heuristic will be: the single real number`;
$h_\theta(x) \in \mathbf{R}$, number of output units $S_L = 1$

\subsubsection{Multi-class Classification (K classification)}
\label{sec:9-1}
K output units
\[y \in \mathbf{R^K} \textrm{ e.g.} \begin{array}{cccc}
  \left[ \begin{array}{c} 1 \\ 0 \\ 0 \\ 0 \end{array} \right], &
  \left[ \begin{array}{c} 0 \\ 1 \\ 0 \\ 0 \end{array} \right], &
  \left[ \begin{array}{c} 0 \\ 0 \\ 1 \\ 0 \end{array} \right], &
  \left[ \begin{array}{c} 0 \\ 0 \\ 0
      \\ 1 \end{array} \right] \\
  pedesterian & car & motorcycle & truck
\end{array} \]
$h_\theta(x) \in \mathbf{R^K}$ and $S_L = K, K \geq 3$.  

The concrete cost function is generalisation of logistic regression:

\[ J(\theta) = -\frac{1}{m} \left[\sum \limits_{i=1}^{m} y^{(i)} \log
  h_\theta(x^{(i)}) + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)}))
\right] + \frac{\lambda}{2m} \sum \limits_{j=1}^{n} \theta_j^2 \]

instead of single $y^{(i)}$ we now have a vector of K such elements,
so, for $h_\theta(x) \in \mathbf{R^K} \textrm{ and output }
(h_\theta(x))_i = i^{th}$ :

\[ J(\theta) = - \frac{1}{m} \left[ \sum \limits_{i=1}^{m} \sum
  \limits_{k=1}^{K} y_k^{(i)} \log (h_\theta(x^{(i)})_k) + (1 -
  y_k^{(i)}) \log (1 - (h_\theta(x^{(i)}))_k) \right] +
\frac{\lambda}{2m} \sum \limits_{l=1}^{L-1} \sum \limits_{i=1}^{s_l}
\sum \limits_{j=1}^{s_{l+1}} (\theta_{ji}^{(l)})^2 \]
 
For binary classification (binary output) K = 1. Regularisation term
does NOT include the bias terms $\theta_{i0}x_0$

\subsection{Backpropagation Algorithm}
\label{sec:9-2}
To minimise Cost function we need a code to compute $J(\theta)$ and
$-\frac{\delta}{\delta \theta^{(l)}_{ij}} J(\theta)$

Intuition: $\delta_j^{(l)}$ = ``error'' of node $j$ in layer $l$.

For each output layer (layer L = 4) $ \delta_j^{(4)} =
\underbrace{a_j^{(4)}}_{(h_\theta(x))_j} - y_j$, in vector form: $\vec
\delta^{(4)} = \vec a^{(4)} - \vec y$

Further:
\[\delta^{(3)} = (\theta^{(3)})^T \delta^{(4)} .*
\underbrace{g'(Z^{(3)})}_{a^{(3)} .* (1-a^{(3)})} \] where .* -
element-wise multiplication; $a^{(3)}$ is activation vector at level
3.

Similarly:
\[\delta^{(2)} = (\theta^{(2)})^T \delta^{(3)} .* g'(Z^{(2)}) \]
no $\delta^{(1)}$ term, because the first layer corresponds to
features observed in training set.

The term {\bf back Propagation} comes from the fact that initially we
calculate $\delta^{(4)}$, then use it for calculating $\delta^{(3)}$ -
so we ``back-propagating'' the error from high level to level 1.

After quite complex derivation: \[\frac{\delta}{\delta
  \theta_{ij}^{(l)}} J(\theta) = a_j^{(l)}\delta_i^{(l+1)} \]
(ignoring $\lambda$ - regularisation!)

So, the algorithm itself:

Get training set $\{(x^{(1)}, y^{(1)}), \dots, (x^{(m)}, y^{(m)})\}$

Set $\Delta_{ij}^{(l)} = 0$ (for all $l, i, j$). This will be used to
compute the partial derivatives.
\begin{tabbing}
  For\quad \=$i$ = 1 to $m$  \\
  \>Set $a^{(1)} = x{(i)}$ \\
  \>Perform forward propagation to compute $a^{(l)}$ for $l = 2, 3,
  \dots, L $ \\
  \>Using $y^{(i)}$, compute $\delta^{(L)} = a^{(L)} - y^{(i)}$ \\
  \>Compute $\delta^{(L-1)}, \delta^{(L-2)}, \dots, \delta^{(2)}$ \\
  \>$\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + a_j^{(l)}
  \delta_i^{(l+1)}$ - possible to vectorise as $\Delta^{(l)} :=
  \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$ \\
\end{tabbing}   
After the loop:  \\
$[\mathbf{D}_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)} + \lambda
\theta_{ij}^{(l)} \textrm{ if }j \ne 0$ \\
$\mathbf{D}_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)} \textrm{ if }j
= 0$ \\

finally: $\frac{\delta}{\delta \theta_{ij}^{(l)}} J(\theta) =
\mathbf{D}_{ij}^{(l)}$

\subsection{Backpropagation Intuition}
\label{sec:9-3}

The idea is that $\delta_j^{(l)}$ is an ``error'' of cost for
$a_j^{(l)}$ (unit $j$ in layer $l$). Formally, $\delta_j^{(l)} =
\frac{\delta}{\delta z_j^{(l)}} \, cost(i)$ where $cost(i) = y^{(i)}
\log h_\theta(x^{(i)}) + (1 - y^{(i)}) \log h_\theta(x^{(i)})$

\subsection{Implementation: Unrolling Parameters}
\label{sec:9-4}
Traditional ``Advanced optimisation'' way s built using vectors for
weight values and gradients. In the neural networks we use{\bf
  matrices} for $\theta^{(l)}$ and $D^{(l)}$ - so need to ``unroll''
these into vectors.

Example: 10 parameters, 1 hidden layer (10 units), 1 output unit:
\[ s_1 = 10, s_2 = 10, s_3 = 1\]
\[ \Theta^{(1)} \in \mathbf{R}^{10\times11}, \Theta^{(2)} \in
\mathbf{R}^{10\times11}, \Theta^{(2)} \in \mathbf{R}^{1\times 11} \]
\[ D^{(1)} \in \mathbf{R}^{10\times11}, D^{(2)} \in
\mathbf{R}^{10\times11}, D^{(2)} \in \mathbf{R}^{1\times 11} \] In
Octave:
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  thetaVec = [ Theta1(:); Theta2 (:); Theta3(:)]; DVec = [D1(:);
  D2(:); D3(:) ];

  Theta1 = reshape(thetaVec(1:110), 10, 11); Theta2 =
  reshape(thetaVec(111:220), 10, 11); Theta3 =
  reshape(thetaVec(221:231), 1, 11);
\end{lstlisting}

\subsubsection{Learning Algorithm}
Have initial parameters $\Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}$. \\
Unroll to get {\it initialTheta} to pass to
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  fminunc(@costFunction,initialTheta,options)
\end{lstlisting}

Now for {\it costFunction}:
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  function [jval, gradientVec] = costFunction(thetaVec)
\end{lstlisting}
\begin{tabbing}
  \quad \quad \= From {\it thetaVec}, get $\Theta^{(1)}, \Theta^{(2)},
  \Theta^{(3)}$ (use {\it reshape}). \\
  \>Use forward prop/back prop to compute $D^{(1)}, D^{(2)}, D^{(3)}$
  and $J(\Theta)$. \\
  \>Unroll  $D^{(1)}, D^{(2)}, D^{(3)}$ to get {\it gradientVec}\\
\end{tabbing}

\subsection{Gradient Checking}
\label{sec:9-5}
This is an idea to eliminate possible bugs in back prop
implementation. We calculate some sort of ``derivative approximation''
in $\Theta$ using small value $\epsilon$ to calculate approximation:
\[ \frac{\delta}{\delta \Theta} J(\Theta) \approx \frac{J(\Theta +
  \epsilon) - J(\Theta - \epsilon)} { 2 \epsilon} \]

Usually value for $\epsilon$ is quite small: $\epsilon \approx
10^{-4}$ (but beware of too small value - this can case the numerical
problem).

Implementation:
\begin{lstlisting}[language=Octave]
  gradApprox = (J(theta + EPSILON) - J(theta - EPSILON)) / (2*EPSILON)
\end{lstlisting}
% 9-5 05:00
For general case (parameter vector $\theta$ can even be unrolled
version of $\Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}$, so that $\Theta
\in \mathbf{R}^n$ and $\theta = \left[\theta_1, \theta_2, \theta_3,
  \dots, \theta_n \right]$. Then:
\[ \frac{\delta}{\delta \theta_1} J(\theta) \approx \frac{J(\theta_1 +
  \epsilon, \theta_2, \theta_3, \dots, \theta_n) - J(\theta_1 -
  \epsilon, \theta_2, \theta_3, \dots, \theta_n)}{2 \epsilon} \]
\[ \frac{\delta}{\delta \theta_1} J(\theta) \approx \frac{J(\theta_1,
  \theta_2 + \epsilon, \theta_3, \dots, \theta_n) - J(\theta_1,
  \theta_2 - \epsilon, \theta_3, \dots, \theta_n)}{2 \epsilon} \]
\[ \frac{\delta}{\delta \theta_1} J(\theta) \approx \frac{J(\theta_1,
  \theta_2, \theta_3 + \epsilon, \dots, \theta_n) - J(\theta_1,
  \theta_2, \theta_3 - \epsilon, \dots, \theta_n)}{2 \epsilon} \]
\[ \vdots \]
\[ \frac{\delta}{\delta \theta_n} J(\theta) \approx \frac{J(\theta_1,
  \theta_2, \theta_3, \dots, \theta_n + \epsilon) - J(\theta_1,
  \theta_2, \theta_3, \dots, \theta_n - \epsilon)}{2 \epsilon} \] in
Octave:
\begin{lstlisting}[language=Octave]
  for i = 1 : n, thetaPlus = theta; thetaPlus(i) = thetaPlus(i) +
  EPSILON; thetaMinus = theta; thetaMinus(i) = thetaMinus(i) -
  EPSILON; gradApprox(i) = (J(thetaPlus) - J(thetaMinus)) /
  (2*EPSILON); end;
\end{lstlisting}

The criteria: check that $gradApprox \approx \underbrace{
  DVec}_{\textrm{derivatives from back prop}}$. The idea is to compare
the numerically obtained value for derivatives with formulated ones.

\subsubsection{Implementation Node}
\begin{itemize}
\item Implement backprop to compute {\it DVec} (unrolled $D^{(1)},
  D^{(2)}, D^{(3)}$).
\item Implement numerical gradient check to compute {\it gradApprox}.
\item Make sure they give similar values.
\item Turn off gradient checking. Use backprop code for learning.
\end{itemize}

\paragraph{Important.}
Be sure to disable the gradient checking code before training
classifier!

\subsection{Random Initialisation}
\label{sec:9-6}
For gradient descent and advanced optimisation method, need initial
value for $\Theta$:
\begin{lstlisting}[language=Octave]
  otpTheta = fminunc(@costFunciton, initialTheta, options)
\end{lstlisting}
Consider gradient descent: Set {\it initialTheta = zeros(n, 1)}? This
does not work for neural network as $\Theta_{ij}^{(l)} = 0$ for all
$i, j, l$ and $a_1^{(2)} = a_2^{(2)}$, also $\delta_1^{(2)} =
\delta_2^{(2)}$ and partial derivatives are equal to each other. After
each update, parameters corresponding to inputs going into each of two
hidden units are identical. As a result, all hidden units are
computing the same feature (the same function from the input).

\subsubsection{Random Initialisation: Symmetry breaking}
\label{sec:9-6-1}
Initialise each $\Theta_{ij}^{(l)}$ to a random value in $[ -\epsilon,
\epsilon ]$ so that $-\epsilon \leq \Theta_{ij}^{(l)} \leq \epsilon$.
\\
E.g.
\begin{lstlisting}[language=Octave]
  Theta1 = rand(10, 11) * (2*INIT_EPSILON) - INIT_EPSILON; Theta2 =
  rand(1, 11) * (2*INIT_EPSILON) - INIT_EPSILON;
\end{lstlisting}

\subsection{Putting it Together}
\label{sec:9-7}
\begin{itemize}
\item Pick a network architecture:\\
  No of input units: Dimension of features $x^{(i)}$ \\
  No of output units: Number of classes \\
  Reasonable default: 1 hidden layer, or if >1 hidden layer, have same
  no. of hidden units in every layer (usually the more the better, but
  this will be computation expensive).
\item Train a neural network
  \begin{enumerate}
  \item Randomly initialise weights
  \item Implement forward propagation to get $h_\Theta(x^{(i)})$ for
    any $x^{(i)}$
  \item Implement code to compute cost function $J(\Theta)$
  \item Implement back prop to compute partial derivatives
    $\frac{\delta}{\delta\Theta_{jk}^{(l)}} J(\Theta)$
  \end{enumerate}
  \begin{lstlisting}[language=Octave]
    for i = 1:m
  \end{lstlisting}
  \begin{tabbing}
    \quad \quad \quad \quad \= Perform forward propagation and back propagation using
    example ($x^{(i)}, y^{(i)}$)  \\
    \>Get activations $a^{(l)}$ and delta terms $\delta^{(l)}$ for $l=2,
    \dots, L$)\\
    \>Compute accumulation terms $\Delta^{(l)} := \Delta^{(l)} +
    \delta^{(li)}(a^{(l)})^T$
  \end{tabbing} 
  Compute $\frac{\delta}{\delta \Theta_{jk}^{(l)}} J(\Theta)$ (outside
  loop on 1:m)
\item Training a neural network
  \begin{enumerate}
  \item Use gradient checking to compare $\frac{\delta}{\delta
      \Theta_{jk}^{(l)}} J(\Theta)$ computed using back propagation
    vs. using numerical estimate of gradient of $J(\Theta)$. \\
    Then disable gradient checking code.
  \item Use gradient descent or advanced optimisation method with back
    propagation to try to minimise $J(\Theta)$ as a function of
    parameters $\Theta$
  \end{enumerate}
\end{itemize}

BTW, for neural networks, the function $J(\Theta)$ is non-convex so it
is possible it will have the local minimum (at least in theory). In
practice, it usually diverges quite OK.

\subsection{Automonous Driving}
\label{sec:9-8}
Баловство одно. But the idea of using more then one training set and
selecting the most ``confident'' output worth noting.


\section{Advice for Applying ML}
\label{sec:10}
Imagine we're debugging a regularised linear regression which predicts
the house prices:
\[J(\Theta) = \frac{1}{2m} \left[ \sum\limits_{i=1}^{m}
  (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum
  \limits_{i=1}^{n}\theta_j^2 \right] \] But, on new set of houses, we
get unacceptable large errors in predictions. Oops.
Possible variants;
\begin{itemize}
\item get more training examples - works sometimes (we'll see when)
\item try smaller set of features (to avoid over- fitting)
\item try getting additional features
\item adding polynomial features
\item try decreasing / increasing $\lambda$;
\end{itemize}
The idea is to filter out the approaches which are not going to work:

\subsubsection{ML diagnostic}
Diagnostic: A test that you can run to gain insight what is/isn't
working with a learning algorithm, and gain guidance as to how best to
improve its performance.

Sometimes takes time to implement, but usually it is a good spent time.

\subsection{Evaluating a Hypothesis}
\label{sec:10-2}
Idea is to check how  hypothesis adapts the new data. We split the
data into two portions; first (near 70\%) used as the training set;
the second is the test set ($m_{test}$). 

It is better to select testing set randomly (to avoid any ordering
data effects).

\paragraph{Training / test procedures for linear regression}
\label{sec:10-2-1}
\begin{itemize}
\item Learn parameter $\theta$ from training data (minimising training
  error $J(\theta)$).
\item Compute test set error:
\[J_{test}(\theta) = \frac{1}{2m_{test}}\sum \limits_{i=1}^{m_{test}}
  \left(h_\theta(x_{test}^{(i)}) - y_{test}^{(i)} \right)^2  \]
\end{itemize}

\paragraph{Training / test procedures for logistic regression}
\label{sec:10-2-2}
\begin{itemize}
\item Learn parameter $\theta$ from training data.
\item Compute test set error:
\[J_{test}(\theta) = \frac{1}{2m_{test}}\sum \limits_{i=1}^{m_{test}}
  y_{test}^{(i)} \log h_\theta(x_{test}^{(i)}) + (1 - y_{test}^{(i)})
  \log (1 - h_\theta (x_{test}^{(i)}))  \]
\item Miss- classification error (0/1 misclassification error): often
  used fr evaluation: \[
err(h_\theta^{(c)}, y) = \left\{ \begin{array}
    {lll} 1 & \textrm{ if } h_\theta(x) \leq 0.5, & y = 0  \\
& \textrm{ or } h_\theta(x) < 0.5, & y = 1  \\
0   & \textrm{ otherwise } & 
  \end{array} \right.\]
Test error = $\frac{1}{m_{test}} \sum \limits_{i=1}^{m_{test}}
err(h_\theta(x_{test}^{(i)}), y_{test}^{(i)})$.
\end{itemize}

\subsection{Model Selection and Train Validation Test Sets}
\label{sec:10-3}

\subsubsection{Model Selection}
Introduce a parameter d (a ``degree of polynomial'') which is, well,
the max power of x in hypothesis. We can compute the relevant
$\Theta^{(i)}$ and corresponding test errors:
\[ d = 1 \rightarrow h_\theta(x) = \theta_0 + \theta_1x \rightarrow
\Theta^{(1)} \rightarrow J_{test}(\Theta^{(1)}) \] 
\[ d = 1 \rightarrow h_\theta(x) = \theta_0 + \theta_1x + \theta_2x^2 \rightarrow
\Theta^{(2)} \rightarrow J_{test}(\Theta^{(2)}) \] 
\[ \vdots\]

\[ d = 1 \rightarrow h_\theta(x) = \theta_0 + \theta_1x \rightarrow +
\theta_2x^2 + \dots + \theta_{10}x^{10} \rightarrow \Theta^{(10)}  \rightarrow J_{test}(\Theta^{(10)}) \] 

Now we can take the bets performing (i.e. returning minimal
$J_{test}$). Assume for our example we have the 5th order polynomial.
We could report $J_{test}$ as estimated but this is likely to be an
optimistic estimate of generalization error, i.e. our extra parameter
(d = degree of polynomial) is fit to test set (in other words, we
optimised the d value as a model parameter on the test set). 

So we divide the data on {\bf three} parts:
\begin{itemize}
\item training set (about 60\%)
\item cross validation set (about 20\%) values are $(x_{cv}^{(i)},y_{cv}^{(i)})$
\item test set (20\%)
\end{itemize}
The we calculate:
Training error:
\[ J_{training}(\theta) = \frac{1}{2m} \sum
\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 \]
Cross Validation error:
\[ J_{cv}(\theta) = \frac{1}{2m_{cv}} \sum
\limits_{i=1}^{m_{cv}}(h_\theta(x_{cv}^{(i)}) - y_{cv}^{(i)})^2 \]
Test error:
\[ J_{test}(\theta) = \frac{1}{2m_{test}} \sum
\limits_{i=1}^{m_{test}}(h_\theta(x_{test}^{(i)}) - y_{test}^{(i)})^2 \]
Returning to Model Selection task, we:
\begin{itemize}
\item calculate $\Theta^{(d)}$ for each reasonable d using the
  training data set;
\item calculate $J_{cv}(\Theta^{(d)})$ for each $\Theta^{(d)}$; select
 d with minimal $J_{cv}$ - assume this is for d=4
\item estimate generalisation error for test set $J_{test}(\theta^{(4)})$.
\end{itemize}
Sometimes CV set us used to report generalisation error (i.e. CV set
is used as the test set too. It is only acceptable is the test set is
sufficiently large, but usually it is a bad practice.

\subsection{Diagnosing Bias vs Variance}
\label{sec:10-4}

Usual reason for poor performance is either Bias (underfitting) or
high variance (over-fitting). To check this, compare values of Cross
Validation vs Training errors.
%see 10-4, Diagnosing bias vs. variance diagram
High values for $J_{cv}$ correspond to either Bias or High Variance
situation. Plus:
\begin{itemize}
\item Bias (underfit):
$J_{train}(\theta)$ will be high; \\
$J_{cv}(\theta) \approx J_{train}(\theta) $ - also high
\item Variance (overfit)
$J_{train}(\theta)$ will be low; \\
$J_{cv}(\theta) >> J_{train}(\theta) $ 
\end{itemize}

\subsection{Regularisation and Bias / Variance}
\label{sec:10-5}

Large $\lambda$ values lead to High bias (underfit) training results;
small $\lambda$ - lead to high variance (overfit). The idea is to
select $\lambda$ automatically.

Despite the ``normal'' cost function $J(\theta)$ now contains
$\lambda$:
\[J(\theta) = \frac{1}{2m} \sum \limits_{i=1}^{m}(h_\theta(x^{(i)}) -
y^{(i)})^2 + \frac{\lambda}{2m} \sum \limits_{j=1}^{m} \theta_j^2  \]

our error values are calculated without one - ``as usual''
Training error:
\[ J_{train}(\theta) = \frac{1}{2m} \sum
\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 \]
Cross Validation error:
\[ J_{cv}(\theta) = \frac{1}{2m_{cv}} \sum
\limits_{i=1}^{m_{cv}}(h_\theta(x_{cv}^{(i)}) - y_{cv}^{(i)})^2 \]
Test error:
\[ J_{test}(\theta) = \frac{1}{2m_{test}} \sum
\limits_{i=1}^{m_{test}}(h_\theta(x_{test}^{(i)}) - y_{test}^{(i)})^2 \]

So, to chose the value for regularisation parameter $\lambda$, we step
with powers of 2:
\begin{enumerate}
\item 0.01
\item 0.02
\item 0.04
\item 0.08

$\vdots$
\item[12] 10 (almost 10.24)
\end{enumerate}

On each step minimise $J(\Theta)$, getting $\Theta^{(i)}$. Determine
cross validation error $J_{cv}(\theta^{d})$. Then check the test
error.

For $\lambda$, the picture differs a bit from the Model Selection: we
have high variance (high $J_{cv}(\theta)$ / low $J_{test}(\theta)$)
for low value of $\lambda$ and high Bias (high values for both
$J_{cv}(\theta)$ and $J_{test}(\theta)$) for high values of $\lambda$.

\subsection{Learning Curves}
\label{sec:10-6}

Used for sanity check or performance proves. Usually the plots of
$J_{training}(\theta)$ and $J_{cv}(\theta)$ are plotted as the
functions of m (training set size).

With increasing m, the training error increases. The Cross Validation
(CV) error decreases with increase in training set size.

In High Bias case, the training error slightly increases but
stabilises at some plato; CV error decreases to the similar value. The
resulting error is quite high, and this {\bg error does not decreases
  with increasing training set size }.

In High Variance case, the training error increases slightly with
increasing m, while CV error decreases to quite high error value (well
above the training error). This gap (between training error and Cross
Validation error) works as indicative diagnostic for the case of High
Variance (too low $\lambda$). Adding more data is likely to help in
this case.
%see pictures from PDF!

\subsection{Deciding What to Do Next Revisited}
\label{sec:10-7}

So, if the ML algorithm does not provide sufficient performance, what
should you try next?
\begin{itemize}
\item get more training examples - to fix high variance
\item try smaller set of features - foxes high variance
\item try getting additional features - fixes high bias problem
\item adding polynomial features - for fixing the high bias problem
\item try decreasing - fixes high bias
\item try decreasing - fixes high variance
\end{itemize}

\subsubsection{Neural Networks and Overfitting}
\label{sec:10-7-1}
\begin{itemize}
\item ``Small'' neural networks (fewer parameters; more prone to
  under-fitting) are computationally cheaper 
\item ``large'' network (more hidden unit, more hidden layers) - more
  prone to overfitting. Computationally more expensive; use $\lambda$
  to address over-fitting.
\end{itemize}
In any case, use $L_{cv}$ to find out the best network config (namely,
the amount of hidden layers and amount of hidden element).

\section{ML System Design}
\label{sec:11}



\end{document}