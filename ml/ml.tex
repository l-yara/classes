%\documentclass{ncc}
%\documentclass{article}
\documentclass{scrartcl}

\usepackage{amsmath,amssymb,amsfonts} % Typical maths resource packages
\usepackage{graphics}                 % Packages to allow inclusion of graphics
\usepackage{color}                    % For creating coloured text and background
\usepackage{hyperref}                 % For creating hyperlinks in
                                % cross references 
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}
\usepackage{listings}
\lstloadlanguages {[LaTeX]TeX, Octave}
\lstset {language=[LaTeX]TeX,
  extendedchars=true ,escapechar=|}

\begin{document}
\label{Chapter 1}
\section {Вводная}
\subsection* {Supervised Learning} - where the ``right (correct)
answers'' for
each example of data is given. \\
Usually we have one of two:\\
{\bf Regression:} predict continuous valued output \\
{\bf Classification:} assign discrete valued output (0, 1 etc) \\

\subsection* {Unsupervised Learning} - finding structure in ``blind''
set of
data. \\
{\bf Clustering} algorithms  \\
Example: {\bf Cocktail Party Problem} (разделение записей на двух
микрофонах от двух источниках и восстановление сигналов от каждого
источника
индивидуально). \\

\label{Chapter 2}
\section {Linear Regression with One Variable}

\subsection {Model Representation}
Notation: \\
{\bf m} =  Number of training examples \\
{\bf x}'s = ``input'' values / features \\
{\bf y}'s = ``output''variable / ``target'' variable \\
{\bf (x, y)} = one single training example \\
{\bf $(x^{(i)}, y^{(i)})$} = $i^{th}$ training example \\

Обычный алгоритм: берем Training Set, скармливаем его Learning
Algorithm. Оный строит ф-ю h (иначе hypothesis), которая по входному
значению $x$ строит Estimation. Иначе говоря, \\
h maps from x's to y's: \\
$h_\theta(x)=\theta_0 + \theta_1x$; shorthand: h(x) \\
- т.е. полагаем что ф-я линейная \\
Другие названия:
\begin{itemize}
\item Linear regression with one variable (x)
\item Univariate(single variable) linear regression
\end{itemize}
\subsection {Cost Function}
\label{2-2}
Идея - минимизировать \[ J(\theta_0, \theta_1) = \frac{1}{2m}
\sum_{i=1}^m\left( h_\theta (x^{(i)}) - y^{(i)} \right)^2
\]
Оная $J(\theta_0, \theta_1)$ и называется Cost Function, она же
Squared Error Function

\subsection {Gradient Descent}
\label {2-5}
Задача: имея функцию $J(\theta_0, \theta_1)$, заполучить ее
${\min \atop \theta_0, \theta_1} J(\theta_0, \theta_1)$ \\
Outline:
\begin {itemize}
\item Start with some $\theta_0, \theta_1$
\item Keep changing $\theta_0, \theta_1$ to reduce J($\theta_0,
  \theta_1$) until we hopefully end up at a minimum
\end{itemize}
Работает также для бОльшего количества параметров ( $\theta_0,
\theta_1,..., \theta_n$)
\subsubsection {Gradient descent algorithm}
repeat until convergence \{ \\
$\theta_j := \theta_j - {\alpha} \underbrace{ \frac {\delta}{\delta
    \theta_j} J(\theta_0,
  \theta_1)}_{derivative}$ (for j=0 and j=1) \\
\}, where: \\
$\alpha$ - learning rate \\
$\frac {\delta}{\delta \theta_j} J(\theta_0, \theta_1)$ - derivative
(производная, точнее, частная производная)

\subparagraph {Correct: Simultaneous upgrade}

temp0 := $\theta_0 - \alpha \frac {\delta}{\delta \theta_0}
J(\theta_0,
\theta_1)$ \\
temp1 := $\theta_1 - \alpha \frac {\delta}{\delta \theta_1}
J(\theta_0,
\theta_1)$ \\
$\theta_0$ := temp0 \\
$\theta_1$ := temp1 \\

\subsection {Gradient Descent Algorithm For Linear Regression }
\label {2-7}
После подстановок - получаем частные производные:
\[ \frac {\delta}{\delta \theta_j} J(\theta_0, \theta_1) = \frac
{\delta}{\delta \theta_j} \frac{1}{2m} \sum\limits_{i=1}^m\left(
  h_\theta (x^{(i)}) - y^{(i)} \right)^2 = \frac {\delta}{\delta
  \theta_j} \frac{1}{2m} \sum\limits_{i=1}^m\left(\theta_0 + \theta_1
  x^{(i)} - y^{(i)} \right)^2
\] \\
Тогда:
\[
\theta_0 : \frac {\delta}{\delta \theta_0} J(\theta_0, \theta_1) =
\frac{1}{m} \sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)}
\right) \]
\[
\theta_1 : \frac {\delta}{\delta \theta_1} J(\theta_0, \theta_1) =
\frac{1}{m} \sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)}
\right) * x^{(i)}
\]
Тогда алгоритм будет: \\
repeat until convergence \{ \\
\
\[\theta_0 := \theta_0 - {\alpha} \frac{1}{m}
\sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)} \right) \]
\[\theta_1 := \theta_1 - {\alpha} \frac{1}{m}
\sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)} \right) *
x^{(i)} \] \} Утверждает, что Cost Function для Linear Regression
всегда является ``bow shaped'', или ``Convex function'' - т.е. не
имеет локальных минимумов; единственный минимум - глобальный.

Дальше, он называет сей процесс ``Batch'' Gradient Descent - т.к. Each
step of gradient descent uses all the training examples.

\subsection {Extensions}
\label {2-8}
1. Существует прямой (не - итерационный) метод обсчета $\theta_0,
\theta_1$ \\
2. БОльшее количество параметров (features): $y = F(x_1, x_2, ...,
x_n)$

\label{Chapter 3}
\section {Linear Algebra Review}
Ничего интересного, обозначения:
\[
\left( \begin{array}{cc}
    1402 & 191 \\
    1371 & 821 \\ 949 & 1437 \\ 147 & 1448 \\
  \end{array} \right) = \mathbb{R}^{4\times2}
\]
Identity Matrix - единичная матрица вроде $\left( I
  = \begin{array}{cc}
    1 & 0 \\ 0 & 1 \\
  \end{array}
\right)$

Свойства: $A I = I A = A$ Обычно умножение матриц некомутативно, но
ассоциативно: $A B \neq B A$, но $(A B) C = A (B C)$

Inverse matrix - обратная матрица Transpose matrix - транспонированная
матрица

\label{Chapter 4}
\section {Linear Regression with Multiple Variables}
Notation: \\
\begin{itemize}
\item {m} = number of examples (size of training set)
\item {n} = number of features
\item{$x^{(i)}$} = input (features) of $i^{th}$ training example
  (vector of size n)
\item {$x_j^{(i)}$} = value of feature j in $i^{th}$ training example
\end{itemize}
Hypothesis: \\
\[
h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 + ...
\theta_nx_n
\]
For convenience in notation , define $x_0=1$. $(x_0^{(i)}=1)$: Feature
vector (values for a single feature) is of dimention n+1:
\[
x = \left[ \begin{array}{c} x_0 \\ x_1 \\ x_2 \\ \vdots \\ x_n
  \end{array} \right] \in \mathbf{R^{n+1}}
\]
The same story (n+1 dimentional vector) - for parameters:
\[
\theta = \left[ \begin{array}{c} \theta_0 \\ \theta_1 \\ \theta_2 \\
    \vdots \\ \theta_n
  \end{array} \right] \in \mathbf{R^{n+1}}
\]
(another n+1 dimentional vector) Then hypothesis can be written as:
\[
h_\theta(x) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 +
... \theta_nx_n = \\ \theta^TX
\]
where: $\theta^T = [\theta_0 \theta_1 ... \theta_n]$ - so-called {\bf
  raw vector}. \\

This form is also called {\bf Multivariate linear regression }.

\subsection {Gradient Descent for Multiple Variables}
Cost function:
\[
J(\theta_0, \theta_1, ..., \theta_n) = \frac{1}{2m}
\sum\limits_{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)} \right)^2
\]
Gradient descent: \\
Repeat \{
\[ \theta_j := \theta_j - \alpha \frac
{\delta}{\delta\theta_j}J(\theta) \\
= \theta_j - \alpha \frac{1}{m}
\sum\limits_{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)}
\right)x_j^{(i)} \]
\} - simultaneously update for every j = 0,1,...n \\
for j = 0, the formula is the same as of Single-Variable Descent (as
$x_0 = 1$)
\label {4-3}
\subsection {Gradient Descent in practice}
Предлагается масштабировать features так, чтобы избегать ``вытянутых''
контуров на $\theta_1(\theta_2)$ диаграммах, т.к. на оных вытянутых
диаграммах процесс сходится плохо. Сие называется

\subsubsection{ Feature Scaling} 
- get every feature into approximately a $-1 \leq x_i \leq 1$ range.
\subsubsection {Mean normalisation}
- replace $x_i$ with $\frac{x_i - \mu_i}{S_i}$ to make features have
approximately zero mean (do not apply to $x_0 = 1$):
\begin{itemize}
\item{$\mu_i$} - mean value
\item{$S_i$} - the range (max - min) or standard deviation (would be
  fine too)
\end{itemize}
Example:
\begin{equation*}
  \begin{split}
    x_1 = \frac{size-1000}{2000} \\
    x_2 = \frac{\#bedrooms-2}{5} \\
    -0.5 \leq x_1 \leq 0.5, -0.5 \leq x_2 \leq 0.5
  \end{split}
\end{equation*}

\subsection {Gradient Descent in Practice - Learning Rate}
\label {4-4}
Gradient descent works correctly if $J(\theta)$ decrease after every
iteration. Usually convergence is declared if $J(\theta)$ decreases by
less then some constant (i.e. $10^-{3}$) in one iteration.

If $J(\theta)$ increases (oscillates up and down) - this usually means
that learning rate $\alpha$ is too big (but convergence becomes
slower).

So it is recommended to try \[ ..., 0.001,0.003, 0.01, 0.03, 0.1, 0.3,
1, ...
\]
 
\subsection{Features and Polynomial Regression}
\label{4-5}
\subsubsection{Polynomial Regression}
In polynomial regression formulas like $h_\theta(x)=\theta_0 +
\theta_qx + \theta_2x^2 + \theta^3x_3$ we can re-write the expression
as $h_\theta(x) = \theta_0 + \theta_qx_1 + \theta_2x_2 + \theta_3x_3$
where \begin{itemize}
\item{$x_1=x$}
\item{$x_2=x^2$}
\item{$x_1=x^3$}
\end{itemize}
and use the mechanics of the ``normal''linear regression. In addition,
it is usually a better idea to use some sort of ``inside'' information
- in order to choose the ``right'' set of feature. Such selection
process can also be automated.

\subsection {Normal Equation}
\label{4-6}
To solve the $\theta$ analytically. For 1-D features we can use
derivatives: $\frac{d}{d\theta}J(\theta) = 0$, solve it for $\theta$.
But if $\theta \in \mathbb{R}^{n+1}$, we need to solve \[
\frac{\delta}{\delta \theta_j}J(\theta) = \frac{\delta}{\delta
  \theta_j} \left( \frac{1}{2m}
  \sum\limits_{i=1}^m\big(h_\theta(x^{(i)}) - y^{(i)}\big)^2\right) =
0
\] for every j

The idea is: pack the training set into matrix $X^{m \times (n + 1)}$
(so-called design matrix) for features (n features + ones for $x_0$)
and m - dimentional vector y for results. Then:
\[ \theta = \mathbf{\left( X^TX \right)^{-1}X^T}y \] values in
$\theta$ will minimise the cost function. Feature scaling in this case
is NOT actual.

Octave: \emph{ pinv(X'*X)*X'*y }

\paragraph {Normal Equation Gradient Descent)} No need to choose
$\alpha$, don't need to iterate.

\paragraph {Gradient Descent vs Normal Equation} Gradient Descent
works well with large $n$, while calculation $n \times n$ matrix for
large n could be painful.

\subsection {Normal Equation Non-invertibility}
Idea: what if $\mathbf{X^TX}$ is non- invertible (singular)?
Causes:\begin{itemize}
\item Redundant features (linearly dependent). \\
  E.g. $x_1$ = size in $feet^2$, $x_2$ = size in $m^2$
\item Too many features ($e.g. m \leq n$). \\
  Delete some features or use regularisation.

\end{itemize}

In Octave, \emph{pinv(X'*X)*X'*y} will end with some meaningful result
- this is different from ``just'' \emph{inv} function (see Octave doc
for difference between the ``normal'' inversion inv and ``pseudo'' -
inversion pinv functions).

\section {Octave Tutorial}
\label {Chapter 5}
Add ``;'' to make a command (not output it immediately)\\
Commands can be comma - chained Commands / operators:\begin{itemize}
\item {PS1('>> ');} set up the command prompt
\item {help command} prints help on command
\item {\verb!~=!} works as ``not equal'' logical operator
\item {constants}: pi,
\item {output} \verb!disp(sprintf('2 decimals: %0.2f', a))!
\item {switch default output format}: format long / format short
\end{itemize}
\subsubsection {matrix}
\begin{verbatim}
A = [1 2; 3 4;
> 5 6]

>> V = [1 2 3]
\end{verbatim}
Automated init:
\begin{verbatim}
>>v = 1:0.1:2 % creates vector with 11 values: from 1.0 step 0.1 to 2.0
>>v = 1:6 % creates vector [1..6]
>> ones(2, 3) % generates 2 * 3 matrix filled with 1
>> w = zeros(1,3) % generates vector of "o"
>> rand(3,3) % generates matrix, inits with random (normal
distribution values in 0..1 range)
>> rand(1, 3) % generates matrix, inits with random (Gaussian
distribution with mean = 0 and sigma = 1)
>> eye(6) % identity matrix 6 * 6
\end{verbatim}
\verb!hist(w)! builds a histogram for given vector w

\subsection {Moving data around}
\label{5-2}
\begin{verbatim}
>> size (A) % returns dimensions of matrix A
>> size (A, 1) % returns the value for first dimension of A (rows)
>> size (A, 2) % returns the value for second dimension of A (columns)
>> length(v) % returns length of vector. For matrix - returns the
% longer dimension of matrix
>> pwd, cd, ls,   % CO
>> load filename.ext / load('filename.ext') % loads text files with
% data into relevant variable
>> who % lists variables in memory
>> whos % more detailed 'who'
>> clear <variable name> % deletes variable from memory. If called
% without parameter - clears all variables
>> v = V(1:10) % get the first 10 elements of V
>> save hello.mat v; % saves vector v to file hello.mat (binary
% format) to save in human - readable format, use -ascii option

>> A(3, 2) % returns element of matrix A
>> A(2, :) % fetch everything in the second row. Works for columns as
% well; can be used for assigning
>> A([1 3], :) % get everything from rows 1 and 3
>> A = [A , [100; 101; 102]]; % appends the column vector to the
% matrix (notice ';' as delimiter for elements in column)
>> A(:) % put all elements of A into a single vector
>> C = [A B] % concatenate matrices into a new one (horizontally).
% Same is [A, B] (comma instead of space)
>> C = [A;B] % concatenate matrices vertically 
\end{verbatim}

\subsection{Computing on Data}
\label{5-3}
\begin{verbatim}
>> A*C % matrix multiplication
>> A .* B % element-wise operations: each element of A is being
% multiplied by corresponding element of B
>> A .^ 2 % element-wise power
>> 1 ./ v % element-wise reciprocal of v (every element of a new
% vector is 1 / corresponding element of v)
>> log(v), exp(v), abs(a), -v % element-wise log, exp, abs, negative
>> v + ones(length(v), 1) % adds 1 to every element of v (one can do
% it simpler v + 1)
>> A' % A transposed
>> max(v) % maximum element of v. For matrix - give "per-row" maximum
% by default. Trick to calculate the maximum element of matrix:
% max(A(:))
>> [val, ind] = max(v) % assigns val to max value, ind, to the index
% of max element from v
>> v < 3 % element-wise comparing (builds matrix with 0/1 per element)
>> find (v < 3) % returns vector with elements satisfying the
% condition
>> magic(3) % returns "magic square"
>> sum(A), prod(A) % sum and product of all elements of matrix. By
% default - "per-column" (1st dimention - just like sum(A,1). Per-row
% will be calculated by sum(A(:))
>>sum(sum(A.*eye(9))) % trick to calculate the sum of all elements on
% the matrix diagonal. For other diagonal: sum(sum(A.*flipud(eye(9))))
>> floor(A), ceil(A) % rounding all elements
>> rand(3) % create random matrix 3x3
>> max(a, b) % generate matrix with values = element-wise max of both.
% Also exist "per-row" and "per-column" form
>> pinv(A) %"pseudo-inverse matrix 
\end{verbatim}

\subsection{Plotting Data}
\label{5-4}
\begin{itemize}
\item{plot(t, y)} - plot graph with points $t_i, y_i$ from vectors
\item{hold on} - plot 1st graph, ``hold on'', plot another one
\item{xlabel('time'), ylabel('value')} - set labelx for x and y
\item{legend('sin', 'cos'), title('my plot')} - legend and title
\item{print -dpng 'myPlot.png'} - save to file
\item{close} - remove plot
\item{figure(1);plot(t,y1);figure(2);plot(t,y2);} - builds two plots
  on different widows
\item{subplot(1, 2, 1)} - divide plot a 1x2 grid, access first element
\item{axis([0.5 1 -1 1])} - set x- and y- ranges
\item{clf;} - clear
\item{imagesc(A)} - draw image based on goven matrix A
\item{imagesc(A), colorbar, colormap grey} - as before, add colorbar
  legend, make legend grey-scale
\end{itemize}

\subsection{Control Statement}
\label{5-5}
\subsubsection{Control Statements}
\begin{verbatim}
for i=1:10
 v(i) = 2^i;
end;
% or, having a vector like indices=1:10; we can do similar:
for i=indices,
 disp(i);
end;
% _break_ and _continue_ apply
================
i = 1;
while i <= 5,
 v(i) = 100;
 i = i + 1;
end;
========
if i == 6, break;
=======
if i == 6, disp('oops');
elseif v(i) == 2, disp('aah');
else disp('grgrg');
end;
\end{verbatim}
\subsubsection{functions}
Create file named <function-name>.m (text). Example:
\begin{verbatim}
function y = squareThisNumber(x)

y = x^2;
\end{verbatim}
Then use it in Octave:
\begin{verbatim}
>> cd <folder with file>
>> squareThisNumber(5)
===== or use addpath ('some\path\to\function') - see search path idea
\end{verbatim}
Multiple-variable returning functions
\begin{verbatim}
function [y1, y2] = squareAndCubeThisNumber(x)

y1 = x^2;
y2 = x^3;
\end{verbatim}
Variant: $J(\theta)$
\begin{verbatim}
function j = costFunctionJ(X, y, theta)
% X - "design matrix" with training examples y - class labesls

m = size(X, 1)  % number of training examples
predictions = X*theta % predictionsof hypothesis on all m examples
sqrError = (predicitons-y).^2; %squared error (element-wise)

J = 1/(2*m) * sum(sqeErrors);
\end{verbatim}
\subsection{Vectorization}
\label{5-6}
Idea: instead of using cycles (loops) over data matrices or vectors,
use matrix(vector) multiplication operations.

\label {Chapter 6}
\section {Logistic Regression}
\subsection{Classification}
Idea is to separate the input into cathegories like Spam/Ham,
Malignant / Benign tumor etc. Separation: \begin{itemize}
\item {Two-class} (``binary class'') classification $y \in \{0, 1\}$
\item {Multi-class} classification $y \in \{0, 1, ... n\}$
\end{itemize}
Idea: to use Linear Regression with $h_\theta(x) = \theta^T x$ and
threshold which checks: \begin{itemize}
\item{if $h_\theta(x) \geq 0.5$} predict 1
\item{if $h_\theta(x) < 0.5$} predict 0
\end{itemize}
Problem emerges when the training set is not lineary separated, so
``if paramater greater ... then ...'' does not work. In addition, h
can be > 1 or < 0. As such , we need something like \[ Logistic
Regression: 0 \leq h_\theta(x) \leq 1
\]
It is named ``regression'' for historical reason, but this is actually
a classification algorithm.

\subsection{Hypothesis Representation}
We use Linear Regression Model ($h_\theta(x) = \theta^Tx$ ) with
so-called {\bf Sigmoid function} or {\bf Logistic function} : \[
h_\theta(x) = g(\theta^Tx), where \]
\[g(z) = \frac{1}{1 + e^{-z}}\] so
\begin{equation*}
  h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}
\end{equation*}
(see function graph for this)
\subsubsection{Interpretaion}
$h_\theta(x) = $ estimated probability that y=1 on input x
\begin{equation*}
  \textrm{Example: if }  x = \left[ \begin{array}{c} x_0 \\
      x_1  \end{array} \right] = \left[ \begin{array}{c} 1 \\
      tumorSize  \end{array} \right]
\end{equation*}
\begin{equation*} h_\theta(x) = 0.7 \end{equation*} - then patient has
70\% chance of tumor being malignant. Or: $ h_\theta(x) = \frac{1}{1 +
  e^{-\theta^Tx}}$ - ``probability that y = 1, given
x, parametrized by $\theta$'' \\
As y = 0 or 1, then:
\[P( y = 0 | x;\theta) + P(y=1|x;\theta) = 1\]
\[P( y = 0 | x;\theta) = 1 - P(y=1|x;\theta)\]

\subsection{Decision Boundary}
Suppose predict ``y = 1'' if $h_\theta(x) \geq 0.5$ \\
and ``y = 0'' if $h_\theta(x) < 0.5$ \\

as $g(z) \geq 0.5 \ when \ Z \geq 0 \Rightarrow h_\theta(x) =
g(\theta^Tx) \geq 0.5 \ whenever \ \theta^Tx \geq 0$
\label {page 10 from pdf}

Decision boundary - a line that separates regions with different
predictions. It is a property of a hypothesis (not training set!).

\subsection{Cost Function}
\label{sec:6-4}
Training set: $\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \dots ,
(x^{(m)},y^{(m)})\}$ of m examples; x is a vector of
$\mathbf{R^{n+1}}$, where $x_0 = 1, y \in \{0, 1\}$. We're choosing
the parameters $\theta$ to minimise
\[ \textrm{hypothesis:} h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}} \]
\[ \textrm{Modify Linear regression:} J(\theta) = \frac{1}{m}
\sum\limits_{i=1}^m Cost (h_\theta(x^{(i)}, y^{(i)}) \]
\[ \textrm{where } Cost (h_\theta(x^{(i)}, y^{(i)}) =
\frac{1}{2}(h_\theta(x^{(i)}) - y^{(i)})^2 \]

Logistic cost function is ``non-convex'', which means it can have a
number of local minimums, as a result we can not work with derivative.
Instead, we use iteration method based on \[ \textrm{hypothesis:}
Cost(h_\theta(x), y) = \left\{ \begin{array}
    {rr} -log(h_\theta(x)) & \textrm{if y = 1} \\
    -log(1- h_\theta(x)) & \textrm{if y = 0}
  \end{array} \right. \]

\paragraph{for y=1:}
Cost = 0 if y = 1, $h_\theta(x) = 1$
but as $h_\theta(x) \to 0 \Rightarrow \textrm{Cost } \to \infty $ \\
That captures intuition that if $h_\theta(x) = 0$, predict
$P(y=1|x;\theta) = 0$, but for y=1 we'll penaltize algorithm by a very
large cost.

\paragraph{for y=0:}
Cost = 0 for $h_\theta(x) = 0$, penalty for $h_\theta(x) = 1$

Together it gives convex function suitable for iterative processing.

\subsection{Gradient Descent}
\label{sec:6-5}
\[\textrm{minimising } J(\theta) = \frac{1}{m} \sum\limits_{i=1}^m
Cost (h_\theta(x^{(i)}, y^{(i)}) \] Initial Cost function is being
re-written as \[ Cost(h_\theta(x), y) = -ylog(h_\theta(x)) -
(1-y)log(1-h_\theta(x))
\]

Then J can be expressed as \[ J(\theta) = -\frac{1}{m} \left[
  \sum\limits_{i=1}^m y^{(i)} \log (h_\theta(x^{(i)})) + (1-y^{(i)})
  \log (1 - h_\theta(x^{(i)})) \right] \]

To make a prediction given new x: \[ \textrm{Output } h_\theta(x) =
\frac{1}{1 + e^{-\theta^Tx}} \quad \textrm{which means probability :}
p(y=1|x; \theta)
\]

\paragraph{Gradient Descent}
Calculating $\min_\theta J (\theta)$: \\
\[ \begin{array}{l}
  \textrm{Repeat }  \{ \\
  \theta_j := \theta_j - \alpha \frac {\delta}{\delta \theta} J
  (\theta) \\
  \textrm{where } \frac {\delta}{\delta \theta} J(\theta) =
  \frac{1}{m} \sum\limits_{i=1}^m (h_\theta(x^{(i)}) -
  y^{(i)})x_j^{(i)} \textrm{ -
    partial derivative} \\
  \textrm{so } \theta_j := \theta_j - \alpha \sum\limits_{i=1}^m
  (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\
  \}
\end{array} \]
{\bf simultaneously update all $\theta_j$} - looks exactly like the Linear
Regression algorithm (just hypothesis $h_\theta(x)$ is different), so
implementation can use both loop from 0 to m or vector form as well as
feature scaling idea.

\subsection{Optimisation}
\label{sec:6-6}
The idea is to play with different control algorithms (Gradient
Descent, Conjugate Descent, BFGS, L-BFGS) supplying them ways to
compute $J(\theta)$ and $\frac{\delta}{\delta \theta_j}J(\theta)$
(through some sort of plug-ins - ?).

The last three algorithms are more complex then the ``normal''
Gradient descent but:
\begin{itemize}
\item No need to manually pick $\alpha$ (learning rate)
\item Often faster then ``normal'' gradient descent
\end{itemize}

In Octave, this is implemented through generalised function and
function reference (delegate ?):

Example: \[\theta = \left[ \begin{array}{c} \theta_1 \\ \theta_2
  \end{array} \right]; J(\theta) = (\theta_1 - 5)^2 + (\theta_2- 5)^2 \]
\[\textrm{then } \frac{\delta}{\delta \theta_1}J(\theta) = 2(\theta_1
-5) ;\frac{\delta}{\delta \theta_2}J(\theta) = 2(\theta_2 - 5)
\]
in Octave:
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  function [jVal, gradient] = costFunction(theta) jVal = (theta(1) -
  5)^2 + (theta(2)-5)^2; gradient = zeros(2,1); gradient(1) = 2 *
  (theta(1) - 5); gradient(2) = 2 * (theta(2) - 5);
\end{lstlisting}
then
\begin{lstlisting} [language=Octave, caption=={Algorithm Call }]
  % set up process options
  options = optimset('GradObj', 'on', 'MaxIter', '100'); initialTheta
  = zeros(2, 1); [optTheta, functionVal, exitFlag] =
  fminunc(@costFunction, initialTheta, options);
\end{lstlisting}
This returns optTheta (value of $\theta$ for optimal point),
fuctionVal - cost value in optimal point, and exitFlag (1 for normally
converged operation).

Works for $\theta \in \mathbf{R^d}; d \geq 2$

\subsection{Mutliclass Classification}
\label{sec:6-7}
When you need to classify by more then 2 categories (as in Binary
Classification).

Idea is called ``One-vs-All'' or ``One-vs-Rest''. Build classifiers
[$h^{(1)}(x), h^{(2)}(x), h^{(3)}(x)... $] for each class: \[
h^{(i)}(x) = P(y = i|x;\theta) \ (i = 1, 2, 3)
\]
On a new input x, to make a prediction, pick the class i that
maximises $\max_i h^{(i)}(x)$

\section{Regularisation}
\label{sec:7}

\subsection{The Problem of Overfitting}
\label{sec:7-1}
The ``underfitting' (or ``high bias'') problem takes place when we try
to fit, say, linear model to the task which is rather quadratic - so
we have too strong pre-conception to the idea of the linear model.

Over-fitting (``high variance'') is a vise-verse state: for instance,
we apply the model of power 4 to the problem which would be fine
described by quadratic model. The graph fits to the training data
really well, but the curve itself becomes too ``windy''.

So, if we have too many features, the learned hypothesis may fit the
training set very well ($J(\theta) = \frac{1}{2m} \sum
\limits{i=1}{m}(h_\theta(c^{(i)}) - y^{(i)})^2 \approx 0$) but fail to
generalise to new examples (predict output on new examples). Similar
story - with Logistic Regression.

Addressing over-fitting:
\begin{enumerate}
\item Reduce number of features
  \begin{itemize}
  \item Manually select which features to keep
  \item Model selection algorithm
  \end{itemize}
\item Regularisation
  \begin{itemize}
  \item Keep all the features, but reduce magnitude/values of
    parameters $\theta_j$
  \item Works well when we have a lot of features, each of which
    contributes a bit to predicting y.
  \end{itemize}
\end{enumerate}

\subsection{Cost Function}
\label{sec:7-2}
The idea is to have small values high-order parameters. This will make
our hypothesis simpler and less prone to over-fitting. This is done
through ``penalising'' values in our cost function (keep in mind that
for our case $x^n$ is equivalent to $\theta_n$ so we can use the same
idea for both multi-feature training set and for high-order polynom:

\[J(\theta) = \frac{1}{2m} \left[ \sum \limits{i=1}{m}
  (h_\theta(x^{(i)}) - y^{(i)})^2 + \underbrace{ \lambda \sum
    \limits_{i=1}^{n}\theta_j^2}_{penalty} \right] \] Notice the
regularisation term (a ``penalty'') does not include $\theta_0$ - this
is a sort of convention. $\lambda$ is called ``regularisation
parameter''; it provides balance between two tasks:
\begin{itemize}
\item fitting the training set well
\item to keep the parameter values small
\end{itemize}

If $\lambda$ is too high, all $\theta_i$ end up near 0, so the
hypothesis becomes too flat - i.e. ``under-fit'': $h_\theta(x) =
\theta_0$.

\subsection{Regularised Linear Regression}
\label{sec:7-3}
For ``updated'' Linear Regression (with regularisation parameter), the
Gradient Descent looks a bit different:
\[ \begin{array}{l}
  \textrm{Repeat }  \{ \\
  \theta_0 := \theta_0 - \alpha \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_0^{(i)} \\
  \theta_j := \theta_j - \alpha \left[ \frac {1}{m} \sum
    \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
    \underbrace{+
      \frac{\lambda}{m}\theta_j} _{penalty} \right] \\
  (j= 1, 2, 3, \dots , n) \textrm{ - no zero here!} \\
  \}
\end{array}
\]

- actually, the good old partial derivative on regularised hypothesis.
After some transformations: \[ \theta_j := \theta_j(1 -
\alpha\frac{\lambda}{m}) - \alpha \frac{1}{m} \sum
\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
\]
and $1 - \alpha \frac{\lambda}{m} < 1$, usually about 0.99 (roughly).
Then first part looks like $\theta_j * 0.99$. While second part is a
``normal'' regression update, each step of the algorithm reduces
$\theta_j$.

\subsubsection{Normal Equation}
\label{sec:7-2-2}
\begin{equation*}
  \begin{array}{ll}
    \mathbf{X} = \left[ \begin{array}{c} (x^{(1)})^T \\ (x^{(2)})^T \\
        \vdots \\ (x^{(m)})^T \end{array} \right] & y =
    \left[ \begin{array}{c} y^{(1)}  \\ y^{(2)}  \\  \vdots \\ y^{(m)} \end{array} \right]
  \end{array}
\end{equation*}
where X is $M \times (n+1)$ matrix, y is vector of length m. To
minimise cost function $J(\theta)$: \[ \frac{\delta}{\delta
  \theta_j}J(\theta) \Rightarrow 0 \]
\[
\theta = \Big(X^T X + \lambda \left[ \begin{array}{ccccc} 0 & 0 & 0 &
    \dots & 0 \\ 0 & 1 & 0 & \dots & 0 \\ 0 & 0 & 1 & \dots & 0 \\
    \vdots & \vdots & \vdots & \ddots & 0 \\ 0 & 0 & 0 & \dots &
    1 \end{array} \right] \Big)^{-1} X^Ty
\]
This terrible matrix is of $(n+1) \times (n+1)$ size.

\paragraph{Non-invertibility}
Suppose $m \leq n$, for our $\theta = (X^T X)^{-1}X^Ty$, if $\lambda >
0$:

Then $(X^T X)$ is non-invertible (singular). ``pinv'' function could
still give some sane result, but, if working with different language
(or using regular inverse - inv),this would not work.

It is possible to prove that sum of $(X^T X)$ and this ugly matrix is
invertible (non-singular) for $\lambda > 0$

E.g. Regularisation helps in this pathetic case.

\subsection{Regularised Logistic Regression}
\label{sec:7-4}
Regularised cost function; \[ J(\theta) = -\left[ \frac{1}{m} \sum
  \limits_{i=1}^{m} y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)})
  \log (1 - h_\theta(x^{(i)})) \right] \underbrace{+
  \frac{\lambda}{2m} \sum \limits_{j=1}^{n} \theta_j^2} _{penalty}
\]

Cosmetically, the algorithm looks the same:
\[ \begin{array}{l}
  \textrm{Repeat }  \{ \\
  \theta_0 := \theta_0 - \alpha \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_0^{(i)} \\
  \theta_j := \theta_j - \alpha \underbrace{ \left[ \frac {1}{m} \sum
      \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
      \underbrace{+ \frac{\lambda}{m}\theta_j } _{penalty}
    \right]}_{\frac{\delta}{\delta \theta_j}J(\theta)} \\
  (j= 1, 2, 3, \dots , n) \textrm{ - no zero here!} \\
  \}
\end{array}
\]
where hypothesis is a bit different: $h_\theta(x) = \frac{1}{1 +
  e^{-\theta^Tx}}$

\subsubsection{Advanced Optimisation}
\label{sec:7-4-2}
In Octave:
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  function [jVal, gradient] = costFunction(theta) jVal = [code to
  compute |$J(\theta)$|]; |\[\textrm{becomes } J(\theta) = \left[
    -\frac{1}{m} \sum \limits_{i=1}^{m} y^{(i)} \log h_\theta(x^{(i)})
    + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)})) \right] +
  \frac{\lambda}{2m} \sum \limits_{j=1}^{n} \theta_j^2 \]| gradient(1)
  = [code to compute |$\frac{\delta}{\delta \theta_0} J(\theta)$|];
  |\[\frac {1}{m} \sum \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})
  x_0^{(i)} \]| gradient(2) = [code to compute |$\frac{\delta}{\delta
    \theta_1} J(\theta)$|]; |\[ \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_1^{(i)} +
  \frac{\lambda}{m}\theta_1 \]| gradient(3) = [code to compute
  |$\frac{\delta}{\delta \theta_2} J(\theta)$|]; |\[ \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_2^{(i)} +
  \frac{\lambda}{m}\theta_2 \]| ..... gradient(n+1) = [code to compute
  |$\frac{\delta}{\delta \theta_n} J(\theta)$|]; |\[ \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_n^{(i)} +
  \frac{\lambda}{m}\theta_n \]|
\end{lstlisting}

\section{Neural Networks: Representation}
\label{sec:8}
- to cope with the problems of too large feature sets. - to give ``one
more'' approach to the learning problem

\subsection{Neurons and Brain}
\label{sec:8-2}
The ``one learning algorithm'' hypothesis was quite popular 80-90s.
The idea was to re-use the ``almost universal'' algorithms to ``almost
any'' sensors.

\subsection{Model Representation}
\label{sec:8-3}
Simulating neurons in the brain:
\begin{itemize}
\item ``Input'' wires - dendrites
\item ``Output'' wire - axon
\item dendrite on one neuron is connected to the axon on another. The
  neuron itself makes some ``computations'' when passing signal.
\end{itemize}

Neuron model represents a neuron as a logistic unit with a number of
inputs and single output function (hypothesis) like $ h_\theta(x) $
like: \[  
x = \left[ \begin{array}{c}  x_0 \\ x_1 \\ x_2 \\ x_3 \end{array}
\right];
\theta = \left[ \begin{array}{c}  \theta_0 \\ \theta_1 \\ \theta_2 \\
    \theta_3 \end{array} \right] \textrm{(sometimaes called "weights")} \]
and $h_\theta(x)=\frac{1}{1 + e^{-\theta^Tx}}$ is called ``Sigmoid
(logistic) activation function''.
Sometimes $x_0$ is referred as ``bias unit'' (``bias neuron'' on the
diagram). It is always equal to 1.

\subsubsection{Neural Network}
The Neural Network is a group of neurons like the one on a picture.

Layer 1 neurons are ``input neurons'' 
\label{06:00}


\end{document}
