%\documentclass{ncc}
%\documentclass{article}
\documentclass{scrartcl}

\usepackage{amsmath,amssymb,amsfonts} % Typical maths resource packages
\usepackage{graphics}                 % Packages to allow inclusion of graphics
\usepackage{color}                    % For creating coloured text and background
\usepackage{hyperref}                 % For creating hyperlinks in
                                % cross references 

\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}
\usepackage{listings}
\lstloadlanguages {[LaTeX]TeX, Octave}
\lstset {language=[LaTeX]TeX,
  extendedchars=true ,escapechar=|}

\begin{document}
\label{Chapter 1}
\section {Вводная}
\subsection* {Supervised Learning} - where the ``right (correct)
answers'' for each example of data is given.

Usually we have one of two:
\begin{itemize}
\item {\bf Regression:} predict continuous valued output
\item {\bf Classification:} assign discrete valued output (0, 1 etc)
\end{itemize}

\subsection* {Unsupervised Learning} - finding structure in ``blind''
set of data.

{\bf Clustering} algorithms


Example: {\bf Cocktail Party Problem} (разделение записей на двух
микрофонах от двух источниках и восстановление сигналов от каждого
источника индивидуально).

\label{Chapter 2}
\section {Linear Regression with One Variable}

\subsection {Model Representation}
Notation:
\begin{itemize}
\item {\bf m} = Number of training examples
\item {\bf x}'s = ``input'' values / features
\item {\bf y}'s = ``output''variable / ``target'' variable
\item {\bf (x, y)} = one single training example
\item {\bf $(x^{(i)}, y^{(i)})$} = $i^{th}$ training example
\end{itemize}

Обычный алгоритм: берем Training Set, скармливаем его Learning
Algorithm. Оный строит ф-ю h (иначе hypothesis), которая по входному
значению $x$ строит Estimation. Иначе говоря,

h maps from x's to y's: $h_\theta(x)=\theta_0 + \theta_1x$; shorthand:
h(x)

- т.е. полагаем что ф-я линейная

Другие названия:
\begin{itemize}
\item Linear regression with one variable (x)
\item Univariate(single variable) linear regression
\end{itemize}
\subsection {Cost Function}
\label{2-2}
Идея - минимизировать \[ J(\theta_0, \theta_1) = \frac{1}{2m}
\sum_{i=1}^m\left( h_\theta (x^{(i)}) - y^{(i)} \right)^2
\]
Оная $J(\theta_0, \theta_1)$ и называется Cost Function, она же
Squared Error Function

\subsection {Gradient Descent}
\label {2-5}
Задача: имея функцию $J(\theta_0, \theta_1)$, заполучить ее
${\min \atop \theta_0, \theta_1} J(\theta_0, \theta_1)$ \\
Outline:
\begin {itemize}
\item Start with some $\theta_0, \theta_1$
\item Keep changing $\theta_0, \theta_1$ to reduce J($\theta_0,
  \theta_1$) until we hopefully end up at a minimum
\end{itemize}
Работает также для бОльшего количества параметров ( $\theta_0,
\theta_1,..., \theta_n$)
\subsubsection {Gradient descent algorithm}
repeat until convergence \{ \\
$\theta_j := \theta_j - {\alpha} \underbrace{ \frac {\delta}{\delta
    \theta_j} J(\theta_0,
  \theta_1)}_{derivative}$ (for j=0 and j=1) \\
\}, where: \\
$\alpha$ - learning rate \\
$\frac {\delta}{\delta \theta_j} J(\theta_0, \theta_1)$ - derivative
(производная, точнее, частная производная)

\subparagraph {Correct: Simultaneous upgrade}

\[temp0 := \theta_0 - \alpha \frac {\delta}{\delta \theta_0}
J(\theta_0, \theta_1)\]

\[temp1 := \theta_1 - \alpha \frac {\delta}{\delta \theta_1}
J(\theta_0,\theta_1)\]
\[\theta_0 := temp0 \]
\[\theta_1 := temp1 \]

\subsection {Gradient Descent Algorithm For Linear Regression }
\label {2-7}
После подстановок - получаем частные производные:
\[ \frac {\delta}{\delta \theta_j} J(\theta_0, \theta_1) = \frac
{\delta}{\delta \theta_j} \frac{1}{2m} \sum\limits_{i=1}^m\left(
  h_\theta (x^{(i)}) - y^{(i)} \right)^2 = \frac {\delta}{\delta
  \theta_j} \frac{1}{2m} \sum\limits_{i=1}^m\left(\theta_0 + \theta_1
  x^{(i)} - y^{(i)} \right)^2
\] \\
Тогда:
\[
\theta_0 : \frac {\delta}{\delta \theta_0} J(\theta_0, \theta_1) =
\frac{1}{m} \sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)}
\right) \]
\[
\theta_1 : \frac {\delta}{\delta \theta_1} J(\theta_0, \theta_1) =
\frac{1}{m} \sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)}
\right) * x^{(i)} \]

Тогда алгоритм будет:


repeat until convergence \{


\[\theta_0 := \theta_0 - {\alpha} \frac{1}{m}
\sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)} \right) \]
\[\theta_1 := \theta_1 - {\alpha} \frac{1}{m}
\sum\limits_{i=1}^m\left(h_\theta (x^{(i)}) - y^{(i)} \right) *
x^{(i)} \] \} Утверждает, что Cost Function для Linear Regression
всегда является ``bow shaped'', или ``Convex function'' - т.е. не
имеет локальных минимумов; единственный минимум - глобальный.

Дальше, он называет сей процесс ``Batch'' Gradient Descent - т.к. Each
step of gradient descent uses all the training examples.

\subsection {Extensions}
\label {2-8}
\begin{enumerate}
\item Существует прямой (не - итерационный) метод обсчета $\theta_0,
  \theta_1$
\item БОльшее количество параметров (features): $y = F(x_1, x_2, ...,
  x_n)$
\end{enumerate}

\label{Chapter 3}
\section {Linear Algebra Review}
Ничего интересного, обозначения:
\[
\left( \begin{array}{cc}
    1402 & 191 \\
    1371 & 821 \\ 949 & 1437 \\ 147 & 1448 \\
  \end{array} \right) = \mathbb{R}^{4\times2}
\]
Identity Matrix - единичная матрица вроде $\left( I
  = \begin{array}{cc}
    1 & 0 \\ 0 & 1 \\
  \end{array}
\right)$

% Свойства: $A I = I A = A$ Обычно умножение матриц некомутативно, но
% ассоциативно: $A B \neq B A$, но $(A B) C = A (B C)$

Inverse matrix - обратная матрица Transpose matrix - транспонированная
матрица

\label{Chapter 4}
\section {Linear Regression with Multiple Variables}

Notation:
\begin{itemize}
\item {m} = number of examples (size of training set)
\item {n} = number of features
\item{$x^{(i)}$} = input (features) of $i^{th}$ training example
  (vector of size n)
\item {$x_j^{(i)}$} = value of feature j in $i^{th}$ training example
\end{itemize}

Hypothesis:
\[ h_\theta(x) = \theta_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 +
... \theta_nx_n \]

For convenience in notation , define $x_0=1$. $(x_0^{(i)}=1)$: Feature
vector (values for a single feature) is of dimention n+1:
\[ x = \left[ \begin{array}{c} x_0 \\ x_1 \\ x_2 \\ \vdots \\ x_n
  \end{array} \right] \in \mathbf{R^{n+1}} \]
The same story (n+1 dimentional vector) - for parameters:
\[
\theta = \left[ \begin{array}{c} \theta_0 \\ \theta_1 \\ \theta_2 \\
    \vdots \\ \theta_n
  \end{array} \right] \in \mathbf{R^{n+1}}
\]
(another n+1 dimentional vector) Then hypothesis can be written as:
\[
h_\theta(x) = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \theta_3x_3 +
... \theta_nx_n = \theta^TX \]

where: $\theta^T = [\theta_0 \theta_1 ... \theta_n]$ - so-called {\bf
  raw vector}.

This form is also called {\bf Multivariate linear regression }.

\subsection {Gradient Descent for Multiple Variables}
Cost function:
\[
J(\theta_0, \theta_1, ..., \theta_n) = \frac{1}{2m}
\sum\limits_{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)} \right)^2
\]
Gradient descent:

Repeat \{
\[ \theta_j := \theta_j - \alpha \frac
{\delta}{\delta\theta_j}J(\theta) \\
= \theta_j - \alpha \frac{1}{m}
\sum\limits_{i=1}^m\left(h_\theta(x^{(i)}) - y^{(i)}
\right)x_j^{(i)} \]
\} - simultaneously update for every j = 0,1,...n \\
for j = 0, the formula is the same as of Single-Variable Descent (as
$x_0 = 1$)
\label {4-3}
\subsection {Gradient Descent in practice}
Предлагается масштабировать features так, чтобы избегать ``вытянутых''
контуров на $\theta_1(\theta_2)$ диаграммах, т.к. на оных вытянутых
диаграммах процесс сходится плохо. Сие называется

\subsubsection{ Feature Scaling} 
- get every feature into approximately a $-1 \leq x_i \leq 1$ range.
\subsubsection {Mean normalisation}
- replace $x_i$ with $\frac{x_i - \mu_i}{S_i}$ to make features have
approximately zero mean (do not apply to $x_0 = 1$):
\begin{itemize}
\item{$\mu_i$} - mean value
\item{$S_i$} - the range (max - min) or standard deviation (would be
  fine too)
\end{itemize}
Example:
\begin{equation*}
  \begin{split}
    x_1 = \frac{size-1000}{2000} \\
    x_2 = \frac{\#bedrooms-2}{5} \\
    -0.5 \leq x_1 \leq 0.5, -0.5 \leq x_2 \leq 0.5
  \end{split}
\end{equation*}

\subsection {Gradient Descent in Practice - Learning Rate}
\label {4-4}
Gradient descent works correctly if $J(\theta)$ decrease after every
iteration. Usually convergence is declared if $J(\theta)$ decreases by
less then some constant (i.e. $10^-{3}$) in one iteration.

If $J(\theta)$ increases (oscillates up and down) - this usually means
that learning rate $\alpha$ is too big (but convergence becomes
slower).

So it is recommended to try \[ ..., 0.001,0.003, 0.01, 0.03, 0.1, 0.3,
1, ...
\]
 
\subsection{Features and Polynomial Regression}
\label{4-5}
\subsubsection{Polynomial Regression}
In polynomial regression formulas like $h_\theta(x)=\theta_0 +
\theta_qx + \theta_2x^2 + \theta^3x_3$ we can re-write the expression
as $h_\theta(x) = \theta_0 + \theta_qx_1 + \theta_2x_2 + \theta_3x_3$
where \begin{itemize}
\item{$x_1=x$}
\item{$x_2=x^2$}
\item{$x_1=x^3$}
\end{itemize}
and use the mechanics of the ``normal''linear regression. In addition,
it is usually a better idea to use some sort of ``inside'' information
- in order to choose the ``right'' set of feature. Such selection
process can also be automated.

\subsection {Normal Equation}
\label{4-6}
To solve the $\theta$ analytically. For 1-D features we can use
derivatives: $\frac{d}{d\theta}J(\theta) = 0$, solve it for $\theta$.
But if $\theta \in \mathbb{R}^{n+1}$, we need to solve \[
\frac{\delta}{\delta \theta_j}J(\theta) = \frac{\delta}{\delta
  \theta_j} \left( \frac{1}{2m}
  \sum\limits_{i=1}^m\big(h_\theta(x^{(i)}) - y^{(i)}\big)^2\right) =
0
\] for every j

The idea is: pack the training set into matrix $X^{m \times (n + 1)}$
(so-called design matrix) for features (n features + ones for $x_0$)
and m - dimentional vector y for results. Then:
\[ \theta = \mathbf{\left( X^TX \right)^{-1}X^T}y \] values in
$\theta$ will minimise the cost function. Feature scaling in this case
is NOT actual.

Octave: \emph{ pinv(X'*X)*X'*y }

\paragraph {Normal Equation Gradient Descent)} No need to choose
$\alpha$, don't need to iterate.

\paragraph {Gradient Descent vs Normal Equation} Gradient Descent
works well with large $n$, while calculation $n \times n$ matrix for
large n could be painful.

\subsection {Normal Equation Non-invertibility}
Idea: what if $\mathbf{X^TX}$ is non- invertible (singular)?
Causes:\begin{itemize}
\item Redundant features (linearly dependent). \\
  E.g. $x_1$ = size in $feet^2$, $x_2$ = size in $m^2$
\item Too many features ($e.g. m \leq n$). \\
  Delete some features or use regularisation.

\end{itemize}

In Octave, \emph{pinv(X'*X)*X'*y} will end with some meaningful result
- this is different from ``just'' \emph{inv} function (see Octave doc
for difference between the ``normal'' inversion inv and ``pseudo'' -
inversion pinv functions).

\section {Octave Tutorial}
\label {Chapter 5}
Add ``;'' to make a command (not output it immediately)\\
Commands can be comma - chained Commands / operators:\begin{itemize}
\item {PS1('>> ');} set up the command prompt
\item {help command} prints help on command
\item {\verb!~=!} works as ``not equal'' logical operator
\item {constants}: pi,
\item {output} \verb!disp(sprintf('2 decimals: %0.2f', a))!
\item {switch default output format}: format long / format short
\end{itemize}
\subsubsection {matrix}
\begin{verbatim}
A = [1 2; 3 4;
> 5 6]

>> V = [1 2 3]
\end{verbatim}
Automated init:
\begin{verbatim}
>>v = 1:0.1:2 % creates vector with 11 values: from 1.0 step 0.1 to 2.0
>>v = 1:6 % creates vector [1..6]
>> ones(2, 3) % generates 2 * 3 matrix filled with 1
>> w = zeros(1,3) % generates vector of "o"
>> rand(3,3) % generates matrix, inits with random (normal
distribution values in 0..1 range)
>> rand(1, 3) % generates matrix, inits with random (Gaussian
distribution with mean = 0 and sigma = 1)
>> eye(6) % identity matrix 6 * 6
\end{verbatim}
\verb!hist(w)! builds a histogram for given vector w

\subsection {Moving data around}
\label{5-2}
\begin{verbatim}
>> size (A) % returns dimensions of matrix A
>> size (A, 1) % returns the value for first dimension of A (rows)
>> size (A, 2) % returns the value for second dimension of A (columns)
>> length(v) % returns length of vector. For matrix - returns the
% longer dimension of matrix
>> pwd, cd, ls,   % CO
>> load filename.ext / load('filename.ext') % loads text files with
% data into relevant variable
>> who % lists variables in memory
>> whos % more detailed 'who'
>> clear <variable name> % deletes variable from memory. If called
% without parameter - clears all variables
>> v = V(1:10) % get the first 10 elements of V
>> save hello.mat v; % saves vector v to file hello.mat (binary
% format) to save in human - readable format, use -ascii option

>> A(3, 2) % returns element of matrix A
>> A(2, :) % fetch everything in the second row. Works for columns as
% well; can be used for assigning
>> A([1 3], :) % get everything from rows 1 and 3
>> A = [A , [100; 101; 102]]; % appends the column vector to the
% matrix (notice ';' as delimiter for elements in column)
>> A(:) % put all elements of A into a single vector
>> C = [A B] % concatenate matrices into a new one (horizontally).
% Same is [A, B] (comma instead of space)
>> C = [A;B] % concatenate matrices vertically 
\end{verbatim}

\subsection{Computing on Data}
\label{5-3}
\begin{verbatim}
>> A*C % matrix multiplication
>> A .* B % element-wise operations: each element of A is being
% multiplied by corresponding element of B
>> A .^ 2 % element-wise power
>> 1 ./ v % element-wise reciprocal of v (every element of a new
% vector is 1 / corresponding element of v)
>> log(v), exp(v), abs(a), -v % element-wise log, exp, abs, negative
>> v + ones(length(v), 1) % adds 1 to every element of v (one can do
% it simpler v + 1)
>> A' % A transposed
>> max(v) % maximum element of v. For matrix - give "per-row" maximum
% by default. Trick to calculate the maximum element of matrix:
% max(A(:))
>> [val, ind] = max(v) % assigns val to max value, ind, to the index
% of max element from v
>> v < 3 % element-wise comparing (builds matrix with 0/1 per element)
>> find (v < 3) % returns vector with elements satisfying the
% condition
>> magic(3) % returns "magic square"
>> sum(A), prod(A) % sum and product of all elements of matrix. By
% default - "per-column" (1st dimention - just like sum(A,1). Per-row
% will be calculated by sum(A(:))
>>sum(sum(A.*eye(9))) % trick to calculate the sum of all elements on
% the matrix diagonal. For other diagonal: sum(sum(A.*flipud(eye(9))))
>> floor(A), ceil(A) % rounding all elements
>> rand(3) % create random matrix 3x3
>> max(a, b) % generate matrix with values = element-wise max of both.
% Also exist "per-row" and "per-column" form
>> pinv(A) %"pseudo-inverse matrix 
\end{verbatim}

\subsection{Plotting Data}
\label{5-4}
\begin{itemize}
\item{plot(t, y)} - plot graph with points $t_i, y_i$ from vectors
\item{hold on} - plot 1st graph, ``hold on'', plot another one
\item{xlabel('time'), ylabel('value')} - set labelx for x and y
\item{legend('sin', 'cos'), title('my plot')} - legend and title
\item{print -dpng 'myPlot.png'} - save to file
\item{close} - remove plot
\item{figure(1);plot(t,y1);figure(2);plot(t,y2);} - builds two plots
  on different widows
\item{subplot(1, 2, 1)} - divide plot a 1x2 grid, access first element
\item{axis([0.5 1 -1 1])} - set x- and y- ranges
\item{clf;} - clear
\item{imagesc(A)} - draw image based on goven matrix A
\item{imagesc(A), colorbar, colormap grey} - as before, add colorbar
  legend, make legend grey-scale
\end{itemize}

\subsection{Control Statement}
\label{5-5}
\subsubsection{Control Statements}
\begin{verbatim}
for i=1:10
 v(i) = 2^i;
end;
% or, having a vector like indices=1:10; we can do similar:
for i=indices,
 disp(i);
end;
% _break_ and _continue_ apply
================
i = 1;
while i <= 5,
 v(i) = 100;
 i = i + 1;
end;
========
if i == 6, break;
=======
if i == 6, disp('oops');
elseif v(i) == 2, disp('aah');
else disp('grgrg');
end;
\end{verbatim}
\subsubsection{functions}
Create file named <function-name>.m (text). Example:
\begin{verbatim}
function y = squareThisNumber(x)

y = x^2;
\end{verbatim}
Then use it in Octave:
\begin{verbatim}
>> cd <folder with file>
>> squareThisNumber(5)
===== or use addpath ('some\path\to\function') - see search path idea
\end{verbatim}
Multiple-variable returning functions
\begin{verbatim}
function [y1, y2] = squareAndCubeThisNumber(x)

y1 = x^2;
y2 = x^3;
\end{verbatim}
Variant: $J(\theta)$
\begin{verbatim}
function j = costFunctionJ(X, y, theta)
% X - "design matrix" with training examples y - class labesls

m = size(X, 1)  % number of training examples
predictions = X*theta % predictionsof hypothesis on all m examples
sqrError = (predicitons-y).^2; %squared error (element-wise)

J = 1/(2*m) * sum(sqeErrors);
\end{verbatim}
\subsection{Vectorization}
\label{5-6}
Idea: instead of using cycles (loops) over data matrices or vectors,
use matrix(vector) multiplication operations.

\label {Chapter 6}
\section {Logistic Regression}
\subsection{Classification}
Idea is to separate the input into cathegories like Spam/Ham,
Malignant / Benign tumor etc. Separation: \begin{itemize}
\item {Two-class} (``binary class'') classification $y \in \{0, 1\}$
\item {Multi-class} classification $y \in \{0, 1, ... n\}$
\end{itemize}
Idea: to use Linear Regression with $h_\theta(x) = \theta^T x$ and
threshold which checks: \begin{itemize}
\item{if $h_\theta(x) \geq 0.5$} predict 1
\item{if $h_\theta(x) < 0.5$} predict 0
\end{itemize}
Problem emerges when the training set is not lineary separated, so
``if paramater greater ... then ...'' does not work. In addition, h
can be > 1 or < 0. As such , we need something like \[ Logistic
Regression: 0 \leq h_\theta(x) \leq 1
\]
It is named ``regression'' for historical reason, but this is actually
a classification algorithm.

\subsection{Hypothesis Representation}
We use Linear Regression Model ($h_\theta(x) = \theta^Tx$ ) with
so-called {\bf Sigmoid function} or {\bf Logistic function} : \[
h_\theta(x) = g(\theta^Tx), where \]
\[g(z) = \frac{1}{1 + e^{-z}}\] so
\begin{equation*}
  h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx}}
\end{equation*}
(see function graph for this)
\subsubsection{Interpretaion}
$h_\theta(x) = $ estimated probability that y=1 on input x
\begin{equation*}
  \textrm{Example: if }  x = \left[ \begin{array}{c} x_0 \\
      x_1  \end{array} \right] = \left[ \begin{array}{c} 1 \\
      tumorSize  \end{array} \right]
\end{equation*}
\begin{equation*} h_\theta(x) = 0.7 \end{equation*} - then patient has
70\% chance of tumor being malignant. Or: $ h_\theta(x) = \frac{1}{1 +
  e^{-\theta^Tx}}$ - ``probability that y = 1, given
x, parametrized by $\theta$'' \\
As y = 0 or 1, then:
\[P( y = 0 | x;\theta) + P(y=1|x;\theta) = 1\]
\[P( y = 0 | x;\theta) = 1 - P(y=1|x;\theta)\]

\subsection{Decision Boundary}
Suppose predict ``y = 1'' if $h_\theta(x) \geq 0.5$ and ``y = 0'' if
$h_\theta(x) < 0.5$

as $g(z) \geq 0.5 \ when \ Z \geq 0 \Rightarrow h_\theta(x) =
g(\theta^Tx) \geq 0.5 \ whenever \ \theta^Tx \geq 0$
\label {page 10 from pdf}

Decision boundary - a line that separates regions with different
predictions. It is a property of a hypothesis (not training set!).

\subsection{Cost Function}
\label{sec:6-4}
Training set: $\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), \dots ,
(x^{(m)},y^{(m)})\}$ of m examples; x is a vector of
$\mathbf{R^{n+1}}$, where $x_0 = 1, y \in \{0, 1\}$. We're choosing
the parameters $\theta$ to minimise
\[ \textrm{hypothesis:} h_\theta(x) = \frac{1}{1 + e^{-\theta^T x}} \]
\[ \textrm{Modify Linear regression:} J(\theta) = \frac{1}{m}
\sum\limits_{i=1}^m Cost (h_\theta(x^{(i)}, y^{(i)}) \]
\[ \textrm{where } Cost (h_\theta(x^{(i)}, y^{(i)}) =
\frac{1}{2}(h_\theta(x^{(i)}) - y^{(i)})^2 \]

Logistic cost function is ``non-convex'', which means it can have a
number of local minimums, as a result we can not work with derivative.
Instead, we use iteration method based on \[ \textrm{hypothesis:}
Cost(h_\theta(x), y) = \left\{ \begin{array}
    {rr} -log(h_\theta(x)) & \textrm{if y = 1} \\
    -log(1- h_\theta(x)) & \textrm{if y = 0}
  \end{array} \right. \]

\paragraph{for y=1:}
Cost = 0 if y = 1, $h_\theta(x) = 1$
but as $h_\theta(x) \to 0 \Rightarrow \textrm{Cost } \to \infty $ \\
That captures intuition that if $h_\theta(x) = 0$, predict
$P(y=1|x;\theta) = 0$, but for y=1 we'll penaltize algorithm by a very
large cost.

\paragraph{for y=0:}
Cost = 0 for $h_\theta(x) = 0$, penalty for $h_\theta(x) = 1$

Together it gives convex function suitable for iterative processing.

\subsection{Gradient Descent}
\label{sec:6-5}
\[\textrm{minimising } J(\theta) = \frac{1}{m} \sum\limits_{i=1}^m
Cost (h_\theta(x^{(i)}, y^{(i)}) \] Initial Cost function is being
re-written as \[ Cost(h_\theta(x), y) = -ylog(h_\theta(x)) -
(1-y)log(1-h_\theta(x))
\]

Then J can be expressed as \[ J(\theta) = -\frac{1}{m} \left[
  \sum\limits_{i=1}^m y^{(i)} \log (h_\theta(x^{(i)})) + (1-y^{(i)})
  \log (1 - h_\theta(x^{(i)})) \right] \]

To make a prediction given new x: \[ \textrm{Output } h_\theta(x) =
\frac{1}{1 + e^{-\theta^Tx}} \quad \textrm{which means probability :}
p(y=1|x; \theta)
\]

\paragraph{Gradient Descent}
Calculating $\min_\theta J (\theta)$:
\[ \begin{array}{l}
  \textrm{Repeat }  \{ \\
  \theta_j := \theta_j - \alpha \frac {\delta}{\delta \theta} J
  (\theta) \\
  \textrm{where } \frac {\delta}{\delta \theta} J(\theta) =
  \frac{1}{m} \sum\limits_{i=1}^m (h_\theta(x^{(i)}) -
  y^{(i)})x_j^{(i)} \textrm{ -
    partial derivative} \\
  \textrm{so } \theta_j := \theta_j - \alpha \sum\limits_{i=1}^m
  (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\
  \}
\end{array} \]
{\bf simultaneously update all $\theta_j$} - looks exactly like the Linear
Regression algorithm (just hypothesis $h_\theta(x)$ is different), so
implementation can use both loop from 0 to m or vector form as well as
feature scaling idea.

\subsection{Optimisation}
\label{sec:6-6}
The idea is to play with different control algorithms (Gradient
Descent, Conjugate Descent, BFGS, L-BFGS) supplying them ways to
compute $J(\theta)$ and $\frac{\delta}{\delta \theta_j}J(\theta)$
(through some sort of plug-ins - ?).

The last three algorithms are more complex then the ``normal''
Gradient descent but:
\begin{itemize}
\item No need to manually pick $\alpha$ (learning rate)
\item Often faster then ``normal'' gradient descent
\end{itemize}

In Octave, this is implemented through generalised function and
function reference (delegate ?):

Example: \[\theta = \left[ \begin{array}{c} \theta_1 \\ \theta_2
  \end{array} \right]; J(\theta) = (\theta_1 - 5)^2 + (\theta_2- 5)^2 \]
\[\textrm{then } \frac{\delta}{\delta \theta_1}J(\theta) = 2(\theta_1
-5) ;\frac{\delta}{\delta \theta_2}J(\theta) = 2(\theta_2 - 5)
\]
in Octave:
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  function [jVal, gradient] = costFunction(theta) 

jVal = (theta(1) -  5)^2 + (theta(2)-5)^2; 

gradient = zeros(2,1); 
gradient(1) = 2 * (theta(1) - 5); 
gradient(2) = 2 * (theta(2) - 5);
\end{lstlisting}
then
\begin{lstlisting} [language=Octave, caption=={Algorithm Call }]
% set up process options
  options = optimset('GradObj', 'on', 'MaxIter', '100'); 

initialTheta  = zeros(2, 1); 

[optTheta, functionVal, exitFlag] =  
  fminunc(@costFunction, initialTheta, options);
\end{lstlisting}
This returns optTheta (value of $\theta$ for optimal point),
fuctionVal - cost value in optimal point, and exitFlag (1 for normally
converged operation).

Works for $\theta \in \mathbf{R^d}; d \geq 2$

\subsection{Mutliclass Classification}
\label{sec:6-7}
When you need to classify by more then 2 categories (as in Binary
Classification).

Idea is called ``One-vs-All'' or ``One-vs-Rest''. Build classifiers
[$h^{(1)}(x), h^{(2)}(x), h^{(3)}(x)... $] for each class: \[
h^{(i)}(x) = P(y = i|x;\theta) \ (i = 1, 2, 3)
\]
On a new input x, to make a prediction, pick the class i that
maximises $\max_i h^{(i)}(x)$

\section{Regularisation}
\label{sec:7}

\subsection{The Problem of Overfitting}
\label{sec:7-1}
The ``underfitting' (or ``high bias'') problem takes place when we try
to fit, say, linear model to the task which is rather quadratic - so
we have too strong pre-conception to the idea of the linear model.

Over-fitting (``high variance'') is a vise-verse state: for instance,
we apply the model of power 4 to the problem which would be fine
described by quadratic model. The graph fits to the training data
really well, but the curve itself becomes too ``windy''.

So, if we have too many features, the learned hypothesis may fit the
training set very well ($J(\theta) = \frac{1}{2m} \sum
\limits{i=1}{m}(h_\theta(c^{(i)}) - y^{(i)})^2 \approx 0$) but fail to
generalise to new examples (predict output on new examples). Similar
story - with Logistic Regression.

Addressing over-fitting:
\begin{enumerate}
\item Reduce number of features
  \begin{itemize}
  \item Manually select which features to keep
  \item Model selection algorithm
  \end{itemize}
\item Regularisation
  \begin{itemize}
  \item Keep all the features, but reduce magnitude/values of
    parameters $\theta_j$
  \item Works well when we have a lot of features, each of which
    contributes a bit to predicting y.
  \end{itemize}
\end{enumerate}

\subsection{Cost Function}
\label{sec:7-2}
The idea is to have small values high-order parameters. This will make
our hypothesis simpler and less prone to over-fitting. This is done
through ``penalising'' values in our cost function (keep in mind that
for our case $x^n$ is equivalent to $\theta_n$ so we can use the same
idea for both multi-feature training set and for high-order polynom:

\[J(\theta) = \frac{1}{2m} \left[ \sum \limits{i=1}{m}
  (h_\theta(x^{(i)}) - y^{(i)})^2 + \underbrace{ \lambda \sum
    \limits_{i=1}^{n}\theta_j^2}_{penalty} \right] \] Notice the
regularisation term (a ``penalty'') does not include $\theta_0$ - this
is a sort of convention. $\lambda$ is called ``regularisation
parameter''; it provides balance between two tasks:
\begin{itemize}
\item fitting the training set well
\item to keep the parameter values small
\end{itemize}

If $\lambda$ is too high, all $\theta_i$ end up near 0, so the
hypothesis becomes too flat - i.e. ``under-fit'': $h_\theta(x) =
\theta_0$.

\subsection{Regularised Linear Regression}
\label{sec:7-3}
For ``updated'' Linear Regression (with regularisation parameter), the
Gradient Descent looks a bit different:
\[ \begin{array}{l}
  \textrm{Repeat }  \{ \\
  \theta_0 := \theta_0 - \alpha \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_0^{(i)} \\
  \theta_j := \theta_j - \alpha \left[ \frac {1}{m} \sum
    \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
    \underbrace{+
      \frac{\lambda}{m}\theta_j} _{penalty} \right] \\
  (j= 1, 2, 3, \dots , n) \textrm{ - no zero here!} \\
  \}
\end{array}
\]

- actually, the good old partial derivative on regularised hypothesis.
After some transformations: \[ \theta_j := \theta_j(1 -
\alpha\frac{\lambda}{m}) - \alpha \frac{1}{m} \sum
\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}
\]
and $1 - \alpha \frac{\lambda}{m} < 1$, usually about 0.99 (roughly).
Then first part looks like $\theta_j * 0.99$. While second part is a
``normal'' regression update, each step of the algorithm reduces
$\theta_j$.

\subsubsection{Normal Equation}
\label{sec:7-2-2}
\begin{equation*}
  \begin{array}{ll}
    \mathbf{X} = \left[ \begin{array}{c} (x^{(1)})^T \\ (x^{(2)})^T \\
        \vdots \\ (x^{(m)})^T \end{array} \right] & y =
    \left[ \begin{array}{c} y^{(1)}  \\ y^{(2)}  \\  \vdots \\ y^{(m)} \end{array} \right]
  \end{array}
\end{equation*}
where X is $M \times (n+1)$ matrix, y is vector of length m. To
minimise cost function $J(\theta)$: \[ \frac{\delta}{\delta
  \theta_j}J(\theta) \Rightarrow 0 \]
\[
\theta = \Big(X^T X + \lambda \left[ \begin{array}{ccccc} 0 & 0 & 0 &
    \dots & 0 \\ 0 & 1 & 0 & \dots & 0 \\ 0 & 0 & 1 & \dots & 0 \\
    \vdots & \vdots & \vdots & \ddots & 0 \\ 0 & 0 & 0 & \dots &
    1 \end{array} \right] \Big)^{-1} X^Ty
\]
This terrible matrix is of $(n+1) \times (n+1)$ size.

\paragraph{Non-invertibility}
Suppose $m \leq n$, for our $\theta = (X^T X)^{-1}X^Ty$, if $\lambda >
0$:

Then $(X^T X)$ is non-invertible (singular). ``pinv'' function could
still give some sane result, but, if working with different language
(or using regular inverse - inv),this would not work.

It is possible to prove that sum of $(X^T X)$ and this ugly matrix is
invertible (non-singular) for $\lambda > 0$

E.g. Regularisation helps in this pathetic case.

\subsection{Regularised Logistic Regression}
\label{sec:7-4}
Regularised cost function; \[ J(\theta) = -\left[ \frac{1}{m} \sum
  \limits_{i=1}^{m} y^{(i)} \log h_\theta(x^{(i)}) + (1 - y^{(i)})
  \log (1 - h_\theta(x^{(i)})) \right] \underbrace{+
  \frac{\lambda}{2m} \sum \limits_{j=1}^{n} \theta_j^2} _{penalty}
\]

Cosmetically, the algorithm looks the same:
\[ \begin{array}{l}
  \textrm{Repeat }  \{ \\
  \theta_0 := \theta_0 - \alpha \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_0^{(i)} \\
  \theta_j := \theta_j - \alpha \underbrace{ \left[ \frac {1}{m} \sum
      \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)}
      \underbrace{+ \frac{\lambda}{m}\theta_j } _{penalty}
    \right]}_{\frac{\delta}{\delta \theta_j}J(\theta)} \\
  (j= 1, 2, 3, \dots , n) \textrm{ - no zero here!} \\
  \}
\end{array}
\]
where hypothesis is a bit different: $h_\theta(x) = \frac{1}{1 +
  e^{-\theta^Tx}}$

\subsubsection{Advanced Optimisation}
\label{sec:7-4-2}
In Octave:
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  function [jVal, gradient] = costFunction(theta) 
jVal = [code to compute |$J(\theta)$|]; 
|\[\textrm{becomes } J(\theta) = \left[
    -\frac{1}{m} \sum \limits_{i=1}^{m} y^{(i)} \log h_\theta(x^{(i)})
    + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)})) \right] +
  \frac{\lambda}{2m} \sum \limits_{j=1}^{n} \theta_j^2 \]

| gradient(1)  = [code to compute |$\frac{\delta}{\delta \theta_0} J(\theta)$|];
  |\[\frac {1}{m} \sum \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})
  x_0^{(i)} \]| gradient(2) = [code to compute |$\frac{\delta}{\delta
  \theta_1} J(\theta)$|]; |\[ \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_1^{(i)} +
  \frac{\lambda}{m}\theta_1 \]| gradient(3) = [code to compute
  |$\frac{\delta}{\delta \theta_2} J(\theta)$|]; |\[ \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_2^{(i)} +
  \frac{\lambda}{m}\theta_2 \]| ..... gradient(n+1) = [code to compute
  |$\frac{\delta}{\delta \theta_n} J(\theta)$|]; |\[ \frac {1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)}) x_n^{(i)} +
  \frac{\lambda}{m}\theta_n \]|
\end{lstlisting}

\section{Neural Networks: Representation}
\label{sec:8}
- to cope with the problems of too large feature sets. - to give ``one
more'' approach to the learning problem

\subsection{Neurons and Brain}
\label{sec:8-2}
The ``one learning algorithm'' hypothesis was quite popular 80-90s.
The idea was to re-use the ``almost universal'' algorithms to ``almost
any'' sensors.

\subsection{Model Representation}
\label{sec:8-3}
Simulating neurons in the brain:
\begin{itemize}
\item ``Input'' wires - dendrites
\item ``Output'' wire - axon
\item dendrite on one neuron is connected to the axon on another. The
  neuron itself makes some ``computations'' when passing signal.
\end{itemize}

Neuron model represents a neuron as a logistic unit with a number of
inputs and single output function (hypothesis) like $ h_\theta(x) $
like: \[ x = \left[ \begin{array}{c} x_0 \\ x_1 \\ x_2 \\
    x_3 \end{array} \right];
\theta = \left[ \begin{array}{c}  \theta_0 \\ \theta_1 \\ \theta_2 \\
    \theta_3 \end{array} \right] \textrm{(sometimaes called
  "weights")} \] and $h_\theta(x)=\frac{1}{1 + e^{-\theta^Tx}}$ is
called ``Sigmoid (logistic) activation function''. Sometimes $x_0$ is
referred as ``bias unit'' (``bias neuron'' on the diagram). It is
always equal to 1.

\subsubsection{Neural Network}
The Neural Network is a group of neurons like the one on a picture.

Layer 1 neurons are ``input neurons''; intermediate (Layer 2) is also
called ``the hidden layer'' (contains $a_0$ - the ``bias unit'' ; the
final one (Layer 3) - ``output layer''. Notations:
\begin{itemize}
\item {$a_j^{(j)}$} = ``activation'' of unit i in layer j
\item {$\theta^{(j)}$} = matrix of weights controlling function
  mapping from layer j to layer j + 1.
\end{itemize}
Example: for network with three input units and three hidden ones
(assuming g(z) is the sigmoid function):
\begin{equation}
  \label{eq:8-4}
  \begin{array}[c]{c}
    a_1^{(2)} = g(\theta_{10}^{(1)}x_0 + \theta_{11}^{(1)}x_1 +
    \theta_{12}^{(1)}x_2 + \theta_{13}^{(1)}x_3)  \\
    a_2^{(2)} = g(\theta_{20}^{(1)}x_0 + \theta_{21}^{(1)}x_1 +
    \theta_{22}^{(1)}x_2 + \theta_{23}^{(1)}x_3)  \\
    a_3^{(2)} = g(\underbrace{\theta_{30}^{(1)}x_0 + \theta_{31}^{(1)}x_1 +
      \theta_{32}^{(1)}x_2 + \theta_{33}^{(1)}x_3}_{z_3^{(2)}})
  \end{array}
\end{equation}
Finally,
\[ h_\theta(x) = a_1^{(3)} = g(\theta_{10}^{(2)}a_0^{(2)} +
\theta_{11}^{(2)}a_1^{(2)} + \theta_{12}^{(2)}a_2^{(2)} +
\theta_{13}^{(2)}a_3^{(2)} )\]
Dimensions: \\
If network has $s_j$ units in layer j, $s_{j+1}$ units in layer j+1,
then $\theta^{(j)}$ will be of dimension $s_{j+1} \times (s_j + 1)$.
\\
The whole hypothesis looks like $h_\theta (X)$ (in vectors!) -
``parametrized'' by $\theta$
\label{8-4}
 
\subsubsection{Calcualtion and Optimisation}
Terminology: The $\theta_{10}^{(1)}x_0 + \theta_{11}^{(1)}x_1 +
\theta_{12}^{(1)}x_2 + \theta_{13}^{(1)}x_3$ part is represented as
$z_1^{(2)}$ so that $a_1^{(2)} = g(z_1^{(2)})$.
% \label {8-4: 02:01}
The whole set of $z_1^{(i)}$ from \ref{eq:8-4} can be written as
$\theta^{(1)} \times x$

Further vectorization (with $z_0^{(2)} = 1$ always):
\begin{equation*}
  \label{eq:8-4:02:25}
  \begin{array}[c]{l}
    x = \left[  \begin{array}[c]{c}   x_0 \\ x_1 \\ x_2 \\ x_3 \end{array} \right]
    z^{(2)} = \left[\begin{array}[c]{c} z_1^{(2)} \\ z_2^{(2)}
        \\ z_3^{(2)} \end{array} \right] 
    \\ \\    z^{(2)} = \theta^{(1)}x 
    \\    a^{(2)} = g(z^{(2)})
  \end{array}
\end{equation*}
- both $a^{(2)}$ and $z^{(2)}$ are $\mathbf{R}^3$

Then , as $x = a^{(1)}$ (activation for the first layer), we can
generalise: \[ z^{(2)} = \theta^{(1)} a^{(1)}\]

With bias unit: $a_0^{(2)} = 1 \Rightarrow a^{(2)} \in
\mathbf{R}^4$: \[ z^{(3)} = \theta^{(2)}a^{(2)} \]
\[ h_\theta(x) = a^{(3)} = g(z^{(3)})\]

The whole story is called {\bf Forward Propagation}. At each level,
the process is our good old Logistic Regression, ``learning its own
features'', ergo, we can play with different architectures instead of
playing with high - order features as in Logistic Regression.

\subsection{Examples and Intuition}
\label{sec:8-5}
Exotic example 1: hypothesis like $y = x_1 \textrm{ XOR } x_2$ or $y =
x_1 \textrm{ NXOR } x_2$

Simple example: AND:
\[ x_1, x_2 \in \{0, 1\}\] \[ y = x_1 \textrm{ AND } x_2 \] The whole
network consists of 3 input parameters ($x_1, x_2$ and bias unit
``+1'') and single hidden unit (single-neuron network). We could solve
the task by assigning: $ \theta = [-30, +20, +20]$ - that is,
$h_\theta(x0) = g(-30 + 20 x_1 + 20 x_2)$

OR function is built similarly: $ \theta = [-10, +20, +20]$

Negation (NOT): $ \theta = [+10, -20]$ (2 input parameters in total)

Returning to $x_1 \textrm{ XNOR } x_2 = (x_1 \textrm{ AND } x_2)
\textrm{ OR } ((\textrm{NOT } x_1) \textrm{ AND } (\textrm{NOT }
x_2))$:
\[
\begin{array}{l}
  a_1^{(2)} = (-30 + 20 x_1 + 20 x_2) \\  
  a_2^{(2)} = (+10 - 20 x_1 - 20 x_2)  \\
  \textrm{ level 3: } a_1^{(3)} = (-10 + 20 a_1^{(2)} + 20 a_2^{(2)})
  \Rightarrow h_\theta(x)
\end{array}
\]
\label{8-6}

\subsection{Multiclass Classification}
\label{sec:8-7}
Output layer consists of more then one unit: n units each representing
probability of relevant output. As such, training examples will
be \[(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), (x^{(3)}, y^{(3)}), \dots
(x^{(m)}, y^{(m)}) \], where $x^{(i)}$ are images, and each $y^{(i)}$
is one of \[ \begin{array}{cccc} \left[ \begin{array}{c} 1 \\ 0 \\ 0
      \\ 0 \end{array} \right], & \left[ \begin{array}{c} 0 \\ 1 \\ 0
      \\ 0 \end{array} \right], & \left[ \begin{array}{c} 0 \\ 0 \\ 1
      \\ 0 \end{array} \right], & \left[ \begin{array}{c} 0 \\ 0 \\ 0
      \\ 1 \end{array} \right]
\end{array}\]
and use ``one-vs-all'' approach to training


\section{Neural Networks: Learning}
\label {sec:9}

\subsection{Cost Function}
\label{sec:9-1}
Notation:
\[ \{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}) \dots (x^{(m)}, y^{(m)})
\} \textrm{ - samples} \]
\[ \mathbf{L} \textrm{ = total no of layers in network} \]
\[ s_l \textrm{ = no of units (not counting bias unit in layer l}\]

We have 2 variants of the classification problem:

\subsubsection{Binary Classification}
y = 0 or 1; 1 output unit; heuristic will be: the single real number`;
$h_\theta(x) \in \mathbf{R}$, number of output units $S_L = 1$

\subsubsection{Multi-class Classification (K classification)}
\label{sec:9-1}
K output units
\[y \in \mathbf{R^K} \textrm{ e.g.} \begin{array}{cccc}
  \left[ \begin{array}{c} 1 \\ 0 \\ 0 \\ 0 \end{array} \right], &
  \left[ \begin{array}{c} 0 \\ 1 \\ 0 \\ 0 \end{array} \right], &
  \left[ \begin{array}{c} 0 \\ 0 \\ 1 \\ 0 \end{array} \right], &
  \left[ \begin{array}{c} 0 \\ 0 \\ 0
      \\ 1 \end{array} \right] \\
  pedesterian & car & motorcycle & truck
\end{array} \]
$h_\theta(x) \in \mathbf{R^K}$ and $S_L = K, K \geq 3$.  

The concrete cost function is generalisation of logistic regression:

\[ J(\theta) = -\frac{1}{m} \left[\sum \limits_{i=1}^{m} y^{(i)} \log
  h_\theta(x^{(i)}) + (1 - y^{(i)}) \log (1 - h_\theta(x^{(i)}))
\right] + \frac{\lambda}{2m} \sum \limits_{j=1}^{n} \theta_j^2 \]

instead of single $y^{(i)}$ we now have a vector of K such elements,
so, for $h_\theta(x) \in \mathbf{R^K} \textrm{ and output }
(h_\theta(x))_i = i^{th}$ :

\[ J(\theta) = - \frac{1}{m} \left[ \sum \limits_{i=1}^{m} \sum
  \limits_{k=1}^{K} y_k^{(i)} \log (h_\theta(x^{(i)})_k) + (1 -
  y_k^{(i)}) \log (1 - (h_\theta(x^{(i)}))_k) \right] +
\frac{\lambda}{2m} \sum \limits_{l=1}^{L-1} \sum \limits_{i=1}^{s_l}
\sum \limits_{j=1}^{s_{l+1}} (\theta_{ji}^{(l)})^2 \]
 
For binary classification (binary output) K = 1. Regularisation term
does NOT include the bias terms $\theta_{i0}x_0$

\subsection{Backpropagation Algorithm}
\label{sec:9-2}
To minimise Cost function we need a code to compute $J(\theta)$ and
$-\frac{\delta}{\delta \theta^{(l)}_{ij}} J(\theta)$

Intuition: $\delta_j^{(l)}$ = ``error'' of node $j$ in layer $l$.

For each output layer (layer L = 4) $ \delta_j^{(4)} =
\underbrace{a_j^{(4)}}_{(h_\theta(x))_j} - y_j$, in vector form: $\vec
\delta^{(4)} = \vec a^{(4)} - \vec y$

Further:
\[\delta^{(3)} = (\theta^{(3)})^T \delta^{(4)} .*
\underbrace{g'(Z^{(3)})}_{a^{(3)} .* (1-a^{(3)})} \] where .* -
element-wise multiplication; $a^{(3)}$ is activation vector at level
3.

Similarly:
\[\delta^{(2)} = (\theta^{(2)})^T \delta^{(3)} .* g'(Z^{(2)}) \]
no $\delta^{(1)}$ term, because the first layer corresponds to
features observed in training set.

The term {\bf back Propagation} comes from the fact that initially we
calculate $\delta^{(4)}$, then use it for calculating $\delta^{(3)}$ -
so we ``back-propagating'' the error from high level to level 1.

After quite complex derivation: \[\frac{\delta}{\delta
  \theta_{ij}^{(l)}} J(\theta) = a_j^{(l)}\delta_i^{(l+1)} \]
(ignoring $\lambda$ - regularisation!)

So, the algorithm itself:

Get training set $\{(x^{(1)}, y^{(1)}), \dots, (x^{(m)}, y^{(m)})\}$

Set $\Delta_{ij}^{(l)} = 0$ (for all $l, i, j$). This will be used to
compute the partial derivatives.
\begin{tabbing}
  For\quad \=$i$ = 1 to $m$  \\
  \>Set $a^{(1)} = x{(i)}$ \\
  \>Perform forward propagation to compute $a^{(l)}$ for $l = 2, 3,
  \dots, L $ \\
  \>Using $y^{(i)}$, compute $\delta^{(L)} = a^{(L)} - y^{(i)}$ \\
  \>Compute $\delta^{(L-1)}, \delta^{(L-2)}, \dots, \delta^{(2)}$ \\
  \>$\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + a_j^{(l)}
  \delta_i^{(l+1)}$ - possible to vectorise as $\Delta^{(l)} :=
  \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$ \\
\end{tabbing}   

After the loop:
\[ \mathbf{D}_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)} + \lambda
\theta_{ij}^{(l)} \textrm{ if }j \ne 0 \]
\[ \mathbf{D}_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)} \textrm{ if
}j = 0\]

finally: $\frac{\delta}{\delta \theta_{ij}^{(l)}} J(\theta) =
\mathbf{D}_{ij}^{(l)}$

\subsection{Backpropagation Intuition}
\label{sec:9-3}

The idea is that $\delta_j^{(l)}$ is an ``error'' of cost for
$a_j^{(l)}$ (unit $j$ in layer $l$). Formally, $\delta_j^{(l)} =
\frac{\delta}{\delta z_j^{(l)}} \, cost(i)$ where $cost(i) = y^{(i)}
\log h_\theta(x^{(i)}) + (1 - y^{(i)}) \log h_\theta(x^{(i)})$

\subsection{Implementation: Unrolling Parameters}
\label{sec:9-4}
Traditional ``Advanced optimisation'' way s built using vectors for
weight values and gradients. In the neural networks we use{\bf
  matrices} for $\theta^{(l)}$ and $D^{(l)}$ - so need to ``unroll''
these into vectors.

Example: 10 parameters, 1 hidden layer (10 units), 1 output unit:
\[ s_1 = 10, s_2 = 10, s_3 = 1\]
\[ \Theta^{(1)} \in \mathbf{R}^{10\times11}, \Theta^{(2)} \in
\mathbf{R}^{10\times11}, \Theta^{(2)} \in \mathbf{R}^{1\times 11} \]
\[ D^{(1)} \in \mathbf{R}^{10\times11}, D^{(2)} \in
\mathbf{R}^{10\times11}, D^{(2)} \in \mathbf{R}^{1\times 11} \] In
Octave:
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  thetaVec = [ Theta1(:); Theta2 (:); Theta3(:)]; DVec = [D1(:);
  D2(:); D3(:) ];

  Theta1 = reshape(thetaVec(1:110), 10, 11); Theta2 =
  reshape(thetaVec(111:220), 10, 11); Theta3 =
  reshape(thetaVec(221:231), 1, 11);
\end{lstlisting}

\subsubsection{Learning Algorithm}
Have initial parameters $\Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}$. \\
Unroll to get {\it initialTheta} to pass to
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  fminunc(@costFunction,initialTheta,options)
\end{lstlisting}

Now for {\it costFunction}:
\begin{lstlisting}[language=Octave, caption=={Plugin function }]
  function [jval, gradientVec] = costFunction(thetaVec)
\end{lstlisting}
\begin{tabbing}
  \quad \quad \= From {\it thetaVec}, get $\Theta^{(1)}, \Theta^{(2)},
  \Theta^{(3)}$ (use {\it reshape}). \\
  \>Use forward prop/back prop to compute $D^{(1)}, D^{(2)}, D^{(3)}$
  and $J(\Theta)$. \\
  \>Unroll  $D^{(1)}, D^{(2)}, D^{(3)}$ to get {\it gradientVec}\\
\end{tabbing}

\subsection{Gradient Checking}
\label{sec:9-5}
This is an idea to eliminate possible bugs in back prop
implementation. We calculate some sort of ``derivative approximation''
in $\Theta$ using small value $\epsilon$ to calculate approximation:
\[ \frac{\delta}{\delta \Theta} J(\Theta) \approx \frac{J(\Theta +
  \epsilon) - J(\Theta - \epsilon)} { 2 \epsilon} \]

Usually value for $\epsilon$ is quite small: $\epsilon \approx
10^{-4}$ (but beware of too small value - this can case the numerical
problem).

Implementation:
\begin{lstlisting}[language=Octave]
  gradApprox = (J(theta + EPSILON) - J(theta - EPSILON)) / (2*EPSILON)
\end{lstlisting}
% 9-5 05:00
For general case (parameter vector $\theta$ can even be unrolled
version of $\Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}$, so that $\Theta
\in \mathbf{R}^n$ and $\theta = \left[\theta_1, \theta_2, \theta_3,
  \dots, \theta_n \right]$. Then:
\[ \frac{\delta}{\delta \theta_1} J(\theta) \approx \frac{J(\theta_1 +
  \epsilon, \theta_2, \theta_3, \dots, \theta_n) - J(\theta_1 -
  \epsilon, \theta_2, \theta_3, \dots, \theta_n)}{2 \epsilon} \]
\[ \frac{\delta}{\delta \theta_1} J(\theta) \approx \frac{J(\theta_1,
  \theta_2 + \epsilon, \theta_3, \dots, \theta_n) - J(\theta_1,
  \theta_2 - \epsilon, \theta_3, \dots, \theta_n)}{2 \epsilon} \]
\[ \frac{\delta}{\delta \theta_1} J(\theta) \approx \frac{J(\theta_1,
  \theta_2, \theta_3 + \epsilon, \dots, \theta_n) - J(\theta_1,
  \theta_2, \theta_3 - \epsilon, \dots, \theta_n)}{2 \epsilon} \]
\[ \vdots \]
\[ \frac{\delta}{\delta \theta_n} J(\theta) \approx \frac{J(\theta_1,
  \theta_2, \theta_3, \dots, \theta_n + \epsilon) - J(\theta_1,
  \theta_2, \theta_3, \dots, \theta_n - \epsilon)}{2 \epsilon} \] in
Octave:
\begin{lstlisting}[language=Octave]
  for i = 1 : n, thetaPlus = theta; thetaPlus(i) = thetaPlus(i) +
  EPSILON; thetaMinus = theta; thetaMinus(i) = thetaMinus(i) -
  EPSILON; gradApprox(i) = (J(thetaPlus) - J(thetaMinus)) /
  (2*EPSILON); end;
\end{lstlisting}

The criteria: check that $gradApprox \approx \underbrace{
  DVec}_{\textrm{derivatives from back prop}}$. The idea is to compare
the numerically obtained value for derivatives with formulated ones.

\subsubsection{Implementation Node}
\begin{itemize}
\item Implement backprop to compute {\it DVec} (unrolled $D^{(1)},
  D^{(2)}, D^{(3)}$).
\item Implement numerical gradient check to compute {\it gradApprox}.
\item Make sure they give similar values.
\item Turn off gradient checking. Use backprop code for learning.
\end{itemize}

\paragraph{Important.}
Be sure to disable the gradient checking code before training
classifier!

\subsection{Random Initialisation}
\label{sec:9-6}
For gradient descent and advanced optimisation method, need initial
value for $\Theta$:
\begin{lstlisting}[language=Octave]
  otpTheta = fminunc(@costFunciton, initialTheta, options)
\end{lstlisting}
Consider gradient descent: Set {\it initialTheta = zeros(n, 1)}? This
does not work for neural network as $\Theta_{ij}^{(l)} = 0$ for all
$i, j, l$ and $a_1^{(2)} = a_2^{(2)}$, also $\delta_1^{(2)} =
\delta_2^{(2)}$ and partial derivatives are equal to each other. After
each update, parameters corresponding to inputs going into each of two
hidden units are identical. As a result, all hidden units are
computing the same feature (the same function from the input).

\subsubsection{Random Initialisation: Symmetry breaking}
\label{sec:9-6-1}
Initialise each $\Theta_{ij}^{(l)}$ to a random value in $[ -\epsilon,
\epsilon ]$ so that $-\epsilon \leq \Theta_{ij}^{(l)} \leq \epsilon$.
\\
E.g.
\begin{lstlisting}[language=Octave]
  Theta1 = rand(10, 11) * (2*INIT_EPSILON) - INIT_EPSILON; Theta2 =
  rand(1, 11) * (2*INIT_EPSILON) - INIT_EPSILON;
\end{lstlisting}

\subsection{Putting it Together}
\label{sec:9-7}
\begin{itemize}
\item Pick a network architecture:\\
  No of input units: Dimension of features $x^{(i)}$ \\
  No of output units: Number of classes \\
  Reasonable default: 1 hidden layer, or if >1 hidden layer, have same
  no. of hidden units in every layer (usually the more the better, but
  this will be computation expensive).
\item Train a neural network
  \begin{enumerate}
  \item Randomly initialise weights
  \item Implement forward propagation to get $h_\Theta(x^{(i)})$ for
    any $x^{(i)}$
  \item Implement code to compute cost function $J(\Theta)$
  \item Implement back prop to compute partial derivatives
    $\frac{\delta}{\delta\Theta_{jk}^{(l)}} J(\Theta)$
  \end{enumerate}
\begin{lstlisting}[language=Octave]
    for i = 1:m
\end{lstlisting}
  \begin{tabbing}
    \quad \quad \quad \quad \= Perform forward propagation and back propagation using
    example ($x^{(i)}, y^{(i)}$)  \\
    \>Get activations $a^{(l)}$ and delta terms $\delta^{(l)}$ for $l=2,
    \dots, L$)\\
    \>Compute accumulation terms $\Delta^{(l)} := \Delta^{(l)} +
    \delta^{(li)}(a^{(l)})^T$
  \end{tabbing} 
  Compute $\frac{\delta}{\delta \Theta_{jk}^{(l)}} J(\Theta)$ (outside
  loop on 1:m)
\item Training a neural network
  \begin{enumerate}
  \item Use gradient checking to compare $\frac{\delta}{\delta
      \Theta_{jk}^{(l)}} J(\Theta)$ computed using back propagation
    vs. using numerical estimate of gradient of $J(\Theta)$. \\
    Then disable gradient checking code.
  \item Use gradient descent or advanced optimisation method with back
    propagation to try to minimise $J(\Theta)$ as a function of
    parameters $\Theta$
  \end{enumerate}
\end{itemize}

BTW, for neural networks, the function $J(\Theta)$ is non-convex so it
is possible it will have the local minimum (at least in theory). In
practice, it usually diverges quite OK.

\subsection{Automonous Driving}
\label{sec:9-8}
Баловство одно. But the idea of using more then one training set and
selecting the most ``confident'' output worth noting.


\section{Advice for Applying ML}
\label{sec:10}
Imagine we're debugging a regularised linear regression which predicts
the house prices:
\[J(\Theta) = \frac{1}{2m} \left[ \sum\limits_{i=1}^{m}
  (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum
  \limits_{i=1}^{n}\theta_j^2 \right] \] But, on new set of houses, we
get unacceptable large errors in predictions. Oops. Possible variants;
\begin{itemize}
\item get more training examples - works sometimes (we'll see when)
\item try smaller set of features (to avoid over- fitting)
\item try getting additional features
\item adding polynomial features
\item try decreasing / increasing $\lambda$;
\end{itemize}
The idea is to filter out the approaches which are not going to work:

\subsubsection{ML diagnostic}
Diagnostic: A test that you can run to gain insight what is/isn't
working with a learning algorithm, and gain guidance as to how best to
improve its performance.

Sometimes takes time to implement, but usually it is a good spent
time.

\subsection{Evaluating a Hypothesis}
\label{sec:10-2}
Idea is to check how hypothesis adapts the new data. We split the data
into two portions; first (near 70\%) used as the training set; the
second is the test set ($m_{test}$).

It is better to select testing set randomly (to avoid any ordering
data effects).

\paragraph{Training / test procedures for linear regression}
\label{sec:10-2-1}
\begin{itemize}
\item Learn parameter $\theta$ from training data (minimising training
  error $J(\theta)$).
\item Compute test set error:
  \[J_{test}(\theta) = \frac{1}{2m_{test}}\sum
  \limits_{i=1}^{m_{test}} \left(h_\theta(x_{test}^{(i)}) -
    y_{test}^{(i)} \right)^2 \]
\end{itemize}

\paragraph{Training / test procedures for logistic regression}
\label{sec:10-2-2}
\begin{itemize}
\item Learn parameter $\theta$ from training data.
\item Compute test set error:
  \[J_{test}(\theta) = \frac{1}{2m_{test}}\sum
  \limits_{i=1}^{m_{test}} y_{test}^{(i)} \log
  h_\theta(x_{test}^{(i)}) + (1 - y_{test}^{(i)}) \log (1 - h_\theta
  (x_{test}^{(i)})) \]
\item Miss- classification error (0/1 misclassification error): often
  used fr evaluation: \[ err(h_\theta^{(c)}, y) =
  \left\{ \begin{array}
      {lll} 1 & \textrm{ if } h_\theta(x) \leq 0.5, & y = 0  \\
      & \textrm{ or } h_\theta(x) < 0.5, & y = 1  \\
      0 & \textrm{ otherwise } &
    \end{array} \right.\]
  Test error = $\frac{1}{m_{test}} \sum \limits_{i=1}^{m_{test}}
  err(h_\theta(x_{test}^{(i)}), y_{test}^{(i)})$.
\end{itemize}

\subsection{Model Selection and Train Validation Test Sets}
\label{sec:10-3}

\subsubsection{Model Selection}
Introduce a parameter d (a ``degree of polynomial'') which is, well,
the max power of x in hypothesis. We can compute the relevant
$\Theta^{(i)}$ and corresponding test errors:
\[ d = 1 \rightarrow h_\theta(x) = \theta_0 + \theta_1x \rightarrow
\Theta^{(1)} \rightarrow J_{test}(\Theta^{(1)}) \]
\[ d = 1 \rightarrow h_\theta(x) = \theta_0 + \theta_1x + \theta_2x^2
\rightarrow \Theta^{(2)} \rightarrow J_{test}(\Theta^{(2)}) \]
\[ \vdots\]

\[ d = 1 \rightarrow h_\theta(x) = \theta_0 + \theta_1x \rightarrow +
\theta_2x^2 + \dots + \theta_{10}x^{10} \rightarrow \Theta^{(10)}
\rightarrow J_{test}(\Theta^{(10)}) \]

Now we can take the bets performing (i.e. returning minimal
$J_{test}$). Assume for our example we have the 5th order polynomial.
We could report $J_{test}$ as estimated but this is likely to be an
optimistic estimate of generalization error, i.e. our extra parameter
(d = degree of polynomial) is fit to test set (in other words, we
optimised the d value as a model parameter on the test set).

So we divide the data on {\bf three} parts:
\begin{itemize}
\item training set (about 60\%)
\item cross validation set (about 20\%) values are
  $(x_{cv}^{(i)},y_{cv}^{(i)})$
\item test set (20\%)
\end{itemize}
The we calculate: Training error:
\[ J_{training}(\theta) = \frac{1}{2m} \sum
\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 \] Cross Validation
error:
\[ J_{cv}(\theta) = \frac{1}{2m_{cv}} \sum
\limits_{i=1}^{m_{cv}}(h_\theta(x_{cv}^{(i)}) - y_{cv}^{(i)})^2 \]
Test error:
\[ J_{test}(\theta) = \frac{1}{2m_{test}} \sum
\limits_{i=1}^{m_{test}}(h_\theta(x_{test}^{(i)}) -
y_{test}^{(i)})^2 \] Returning to Model Selection task, we:
\begin{itemize}
\item calculate $\Theta^{(d)}$ for each reasonable d using the
  training data set;
\item calculate $J_{cv}(\Theta^{(d)})$ for each $\Theta^{(d)}$; select
  d with minimal $J_{cv}$ - assume this is for d=4
\item estimate generalisation error for test set
  $J_{test}(\theta^{(4)})$.
\end{itemize}
Sometimes CV set us used to report generalisation error (i.e. CV set
is used as the test set too. It is only acceptable is the test set is
sufficiently large, but usually it is a bad practice.

\subsection{Diagnosing Bias vs Variance}
\label{sec:10-4}

Usual reason for poor performance is either Bias (underfitting) or
high variance (over-fitting). To check this, compare values of Cross
Validation vs Training errors.
% see 10-4, Diagnosing bias vs. variance diagram
High values for $J_{cv}$ correspond to either Bias or High Variance
situation. Plus:
\begin{itemize}
\item Bias (underfit):
  $J_{train}(\theta)$ will be high; \\
  $J_{cv}(\theta) \approx J_{train}(\theta) $ - also high
\item Variance (overfit)
  $J_{train}(\theta)$ will be low; \\
  $J_{cv}(\theta) >> J_{train}(\theta) $
\end{itemize}

\subsection{Regularisation and Bias / Variance}
\label{sec:10-5}

Large $\lambda$ values lead to High bias (underfit) training results;
small $\lambda$ - lead to high variance (overfit). The idea is to
select $\lambda$ automatically.

Despite the ``normal'' cost function $J(\theta)$ now contains
$\lambda$:
\[J(\theta) = \frac{1}{2m} \sum \limits_{i=1}^{m}(h_\theta(x^{(i)}) -
y^{(i)})^2 + \frac{\lambda}{2m} \sum \limits_{j=1}^{m} \theta_j^2 \]

our error values are calculated without one - ``as usual'' Training
error:
\[ J_{train}(\theta) = \frac{1}{2m} \sum
\limits_{i=1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2 \] Cross Validation
error:
\[ J_{cv}(\theta) = \frac{1}{2m_{cv}} \sum
\limits_{i=1}^{m_{cv}}(h_\theta(x_{cv}^{(i)}) - y_{cv}^{(i)})^2 \]
Test error:
\[ J_{test}(\theta) = \frac{1}{2m_{test}} \sum
\limits_{i=1}^{m_{test}}(h_\theta(x_{test}^{(i)}) -
y_{test}^{(i)})^2 \]

So, to chose the value for regularisation parameter $\lambda$, we step
with powers of 2:
\begin{enumerate}
\item 0.01
\item 0.02
\item 0.04
\item 0.08

  $\vdots$
\item[12] 10 (almost 10.24)
\end{enumerate}

On each step minimise $J(\Theta)$, getting $\Theta^{(i)}$. Determine
cross validation error $J_{cv}(\theta^{d})$. Then check the test
error.

For $\lambda$, the picture differs a bit from the Model Selection: we
have high variance (high $J_{cv}(\theta)$ / low $J_{test}(\theta)$)
for low value of $\lambda$ and high Bias (high values for both
$J_{cv}(\theta)$ and $J_{test}(\theta)$) for high values of $\lambda$.

\subsection{Learning Curves}
\label{sec:10-6}

Used for sanity check or performance proves. Usually the plots of
$J_{training}(\theta)$ and $J_{cv}(\theta)$ are plotted as the
functions of m (training set size).

With increasing m, the training error increases. The Cross Validation
(CV) error decreases with increase in training set size.

In High Bias case, the training error slightly increases but
stabilises at some plato; CV error decreases to the similar value. The
resulting error is quite high, and this {\bf error does not decreases
  with increasing training set size }.

In High Variance case, the training error increases slightly with
increasing m, while CV error decreases to quite high error value (well
above the training error). This gap (between training error and Cross
Validation error) works as indicative diagnostic for the case of High
Variance (too low $\lambda$). Adding more data is likely to help in
this case.
% see pictures from PDF!

\subsection{Deciding What to Do Next Revisited}
\label{sec:10-7}

So, if the ML algorithm does not provide sufficient performance, what
should you try next?
\begin{itemize}
\item get more training examples - to fix high variance
\item try smaller set of features - foxes high variance
\item try getting additional features - fixes high bias problem
\item adding polynomial features - for fixing the high bias problem
\item try decreasing - fixes high bias
\item try decreasing - fixes high variance
\end{itemize}

\subsubsection{Neural Networks and Overfitting}
\label{sec:10-7-1}
\begin{itemize}
\item ``Small'' neural networks (fewer parameters; more prone to
  under-fitting) are computationally cheaper
\item ``large'' network (more hidden unit, more hidden layers) - more
  prone to overfitting. Computationally more expensive; use $\lambda$
  to address over-fitting.
\end{itemize}
In any case, use $L_{cv}$ to find out the best network config (namely,
the amount of hidden layers and amount of hidden element).

\section{ML System Design}
\label{sec:11}

\subsubsection{Prioritising}
\label{sec:11-1}
Example: building a spam filter.

Naive approach: choose 100 words indicative of spam/not spam, like
deal, buy, discount, andrew (name of addressee), now...

then build a training set $\mathbf{x} \in \mathbf{R}^{100}$ such as \[
x_j = \left\{
  \begin{array}{ll}
    1 & \textrm{ if word j appears in email } \\
    0 & \textrm{ otherwise}
  \end{array} \right. \]

In practice, take most frequently occurring n words (10,000 to 50,000)
in training set, rather then manually pick 100 words.

Variants to spend time working here:
\begin{itemize}
\item Collect lots of data (like in ``honeypot'' project): not always
  work as we see previously;
\item Develop sophisticated features based on email routing
  information (from email header).
\item Develop sophisticated features for message body, e.g. should
  ``discount'' and ``discounts'' be treated as the same word? How
  about ``deal'' and ``Dealer''? Features about punctuation?
\item Develop algorithm to detect misspellings like m0rtgage,
  med1cine, w4tches
\end{itemize}
The selection is often made based on the ``gut feeling'' - not a best
way to make decisions.

\subsubsection{Error Analysis}
\label{sec:11-2}
Recommended approach:
\begin{itemize}
\item Start with a simple algorithm that you can implement quickly (in
  24 hours). Implement it and test on the cross - validation data.
\item Plot learning curves to decide if more data, more features, etc.
  are likely to help.
\item Error analysis: Manually examine the examples (in cross
  validation set) that your algorithm made errors on. See if you spot
  any systematic trend in what type of examples it is making errors
  on.
\end{itemize}

\paragraph{Example}
Let $m_{CV} = 500$ examples in cross validation set. \\
Algorithm miss-classified 100 emails. \\
Manually examine 100 errors and categorise them based on:
\begin{enumerate}
\item What type of email it is
\item What cues (features) you thing would have helped the algorithm
  classify them correctly
\end{enumerate}
For instance, imagine out of 100 emails, the ones which where
miss-classified as ``normal'' are:
\begin{itemize}
\item contains word ``Pharma'': 12
\item contains word ``Replica/fake'': 4
\item tries to steal password : 53
\item Other: 31
\end{itemize}
It become obvious that we need to look more carefully at password -
stealing emails and find out how we can add features like ``tries to
steal password''.

Other possible features:
\begin{itemize}
\item Deliberate misspelling (m0rgage, med1cine, etc.)
\item Unusual email routing
\item Unusual (spamming) punctuation
\end{itemize}

\paragraph{The importance of numerical evaluation}

E.g. should discount/discounts/discounted/disccounting be treated as
the same word?

For NLP projects it is often good idea to use ``stemming'' software -
like ``Porter stemming''. The problem is that it can hurt (like in
case universe/university). We need the numerical evaluation (e.g.
cross validation error) of algorithms performance with and without
stemming. Similar story - for distinguishing upper vs. lower case
(Mom/mom - should it be treated as the same or different words?)

\subsection{Error Metrics for Skewed Classes}
\label{sec:11-3}

\paragraph{Cancel classification example}
Train logistic regression model $h_\theta(x)$: y=1 if cancer, y = 0
otherwise.

Find that you got 1\% error on your test set (99\% correct diagnoses).
Looks good, but:

If only 0.50\% of patients actually do have cancer, our 1\% error does
not look that good: function which always return 0 will be even better
(it will fail in 0.5\% of cases - better then our 1\%).

Skewed classes - classification when the number of examples is too
close to one of extremes (so that returning constant will give
reasonably good result)..

In this case we need a different evaluation metric, like:

\subparagraph{Precision/Recall} if y = 1 in presence of rare class
that we want to detect (which is pure convention versus y = 0 for rare
case):

\begin{tabular}{cc}
  \quad & Actual class \\
  Predicted class &
  \begin{tabular}{c|c|c}
    \quad & 1 & 0 \\
    \hline
    1 & True positive & False positive \\
    0 & False negative & True negative 
  \end{tabular}
\end{tabular}

We calculate two parameters:
\begin{itemize}
\item Precision \\
  of all cases when we predicted y = 1, what fraction actually should
  be classified 1? $\frac{\textrm{True positives}}{\textrm{\#predicted
      pos}} = \frac{\textrm{True pos}}{\textrm{True pos + False pos}}$
  Shows how accurate our prediction is.
\item Recall \\
  Of all cases actually having 1, what fraction did we correctly
  detect as y = 1? $\frac{\textrm{True pos}}{\textrm{\#actual pos}} =
  \frac{\textrm{True pos}}{\textrm{True pos + False pos}}$ Gives a
  rough sense of how well is our classifier.
\end{itemize}
The idea is that good classified gives high both Precision and Recall,
while ``cheating'' one (e.g. the one which always returns 0) will have
one of these parameters quite low.

\subsection{Trading Off Precision and Recall}
\label{sec:11-4}

Let's say we work on logistic regression: $0 \leq h_\theta(x) \leq 1$ \\
Predict 1 if $h_\theta(x) \geq 0.5$ \\ Predict 0 if $h_\theta(x) <
0.5$ \\

Suppose we want to predict y = 1 (e.g. cancer) only when confident. In
this case we could set a threshold to 0.7. We'll end up with {\bf high
  precision } but {\bf lower recall}.

If we want to avoid missing too many cases of cancer (avoid false
negatives - conservative approach), we set threshold to 0.3. We'll get
a {\bf high recall} (correctly flagging a higher fraction of patients
actually having cancer) but {\bf lower precision} (higher fraction of
patients we've diagnosed with cancer actually do not have one).

More generally: see lecture at 06:50 for dependency between Precision
and Recall.

Next step - to find way to choose this threshold automatically -
particularly with different learning algorithms (or the same algorithm
with different parameters). We introduce some single - value metric
named:

\subsubsection{F Score ($F_1$ Score)}
\label{sec:11-4-1}
% 08:30
\begin{tabular}{c|cc|c|c}
  \quad & Precision (P) & Recall (R) & Average & $F_1$ Score\\
  \hline
  Algorithm 1 & 0.5 & 0.4 & 0.45 & 0.444 \\
  Algorithm 2 & 0.7 & 0.1 & 0.4 & 0.175\\
  Algorithm 3 & 0.02 & 1.0 & 0.51 & 0.0392
\end{tabular} \\

Variants:
\begin{itemize}
\item Average: $\frac{P + R}{2}$ - is not a good idea, because the
  variants with R = 1 (predict 1 all times) will get high level - like
  for Algorithm 3.
\item $F_1$ Score: $2\frac{PR}{P + R}$ - works much better. P = 0 or R
  = 0 $ \rightarrow F_1$ Score = 0; P = 1 and R = 1 $\rightarrow F_1$
  Score = 1;
\end{itemize}

\subsection{Data for Machine Learning}
\label{sec:11-5}

... It is not who has the best algorithm that wins, it's who has the
most data.

\subsubsection{Large data rationale}
\label{sec:11-5-1}

Assume feature $x \in \mathbf{R}^{n+1}$ has sufficient information to
predict y accurately

Example: For breakfast I ate (to?/too?/two?) eggs.

Counterexample: predict housing price from only size (feet) and no
other features.

Useful test: Given the input x, can a human expert confidently predict
y?

Better to use a learning algorithm with many parameters (e.g. logistic
regression/linear regression) neural network with many hidden units).
Use a very large training set (unlikely yo overfit):
$J_{train}(\Theta) \approx J_{test}(\Theta)$ and $J_{test}(\Theta)$
will be small.

Or: in good algorithm we do not want either high bias or high
variance. The bias problem is solved by selecting algorithm with many
parameters; the variance problem is solved by using the large training
set.


\section{Support Vector Machines (SVM)}
\label{sec:SVM}
The last supervised learning algorithms in this course.
\subsection{Optimisation Objective}
\label{sec:12-1}
Alternative view to logistic regression. We use $h_\theta = \frac{1}{1
  + e ^{-\theta^Tx}}$ to express: if $y=1$, we want $h_\theta(x)
\approx 1, \theta^{T} >> 0$ and $h_\theta(x) \approx 0, \theta^{T} <<
0$. Cost of example (for single example):
\[-(y \log h_\theta(x) + (1-y) \log(1-h_\theta(x))) = -y \log
\frac{1}{1 + e^{-\theta ^T x}} - (1-y) \log (1 - \frac{1}{1 +
  e^{-\theta^Tx}}) \]

This is, really, the generalisation of idea that when $y=1$ we use [y
* ]$-\log \frac{1}{1 + e^{-z}}, z = \theta^Tx; \theta^T >> 0$ part
while for $y=0$ we use (1-y) * $-\log (1 - \frac{1}{1 + e^{-z}}), z =
\theta^Tx; \theta^T << 0 $ part. For SVM we change the cost function:
\begin{itemize}
\item for $y = 1$: $cost_1(z)$ equals 0 for $z \geq 1$ and linearly
  decreases from ``somewhere'' to 0 for $z < 1$.
\item for $y = 0$: $cost_0(z)$ equals 0 for $z \leq 1$ and linearly
  increases to ``somewhere'' form 0 for $z > -1$.
\end{itemize}

Now re-write the cost function, replacing as shown in Logistic
regression:
\[\min \limits _\theta \frac{1}{m} \left[\sum \limits_{i=1}^{m}
  y^{(i)} \underbrace{\left(- \log h_\theta
      (x^{(i)})\right)}_{cost_1(\Theta^Tx^{(i)})} + (1-y^{(i)})
  \underbrace{\left(-\log(1 - h\theta(x^{(i)})
    \right)}_{cost_0(\Theta^Tx^{(i)})} \right] + \frac{\lambda}{2m}
\sum \limits_{j=1}^{n}\theta_j^2 \]

so in Support Vector Machine:
\[\min \limits _\theta \frac{1}{m} \left[\sum \limits_{i=1}^{m}
  y^{(i)}cost_1(\Theta^Tx^{(i)}) + (1-y^{(i)}) cost_0(\Theta^Tx^{(i)})
\right] + \frac{\lambda}{2m} \sum \limits_{j=1}^{n}\theta_j^2 \]

By convection, the regularisation parameter is written slightly
differently: we remove $\frac{1}{m}$ (as gradient descent will find
the minimum of cost function regardless of concrete values). Then,
instead of (roughly) using cost function as $A + \lambda B$ (where A
is the ``main'' part of formula, B - regularisation member; we play
with $\lambda$ to tune our process), the SVM cost is written as $C A +
B$, where we play with C to tune. (Set C to be large to simulate small
$\lambda$ , think of C as $C = \frac{1}{\lambda}$). Finally, SVM
process:
\[\min \limits _\theta C \sum \limits_{i=1}^{m} \left[
  y^{(i)}cost_1(\theta^Tx^{(i)}) + (1-y^{(i)}) cost_0(\theta^Tx^{(i)})
\right] + \frac{1}{2} \sum \limits_{j=1}^{n}\theta_j^2 \]
% 12-01; 13:45
Support Vector Machine (SVM) does not return the probability - it
outputs the value from hypothesis:
\[ h_\theta(x) = \left\{
  \begin{array}{cc}
    1 & \textrm{ if } \Theta^T \geq 0\\
    0 & \textrm{otherwise}
  \end{array}
\right. \]

\subsection{Large Margin Intuition}
\label{sec:12-2}

Sometimes SVM is called ``Large Margin Classifier'':
\begin{itemize}
\item for $y = 1$ we want $\theta^T x \geq 1$ (not just $\geq 0$)
\item for $y = 0$ we want $\theta^T x \leq -1$ (not just $< 0$)
\end{itemize}

Consider high value for C (like 100,000): in this case, the ``main''
part (the sum of $cost_1$ and $cost_0$) will be $\approx 0$, so the
cost function to minimise will be min $ C \times 0 + \frac{1}{2} \sum
\limits_{i=1}^{n} \Theta_j^2$. With condition that
\[ \begin{array}{cc}
  \Theta^T x^{(i)} \geq 1 & \textrm{ if } y^{(i)} = 1 \\
  \Theta^T x^{(i)} \leq -1 & \textrm{ if } y^{(i)} = 0
\end{array} \]
after optimisation, we'll get quite natural decision boundary - the
ones that have the biggest margin  (see slide 9).

\subsection{Mathematics Behind Large Margin Classification}
\label{sec:12-3}

\paragraph{Vector Operations}
Inner product of vectors u and v is $u^Tv$. If $u =
\left[ \begin{array}{c} u_1 \\ u_2 \end{array} \right] $, $v =
\left[ \begin{array}{c} v_1 \\ v_2 \end{array} \right] $. Length of u:
$||u|| = \sqrt{u_1^2 + u_2^2}; ||u|| \in \mathbf{R}$

p = length of projection of v onto u. And: $u^Tv = p \times ||u|| =
u_1 v_1 + u_2 v_2 = v^Tu$. Also $p \in \mathbf{R}$ - signed (can be
negative)!

\subsubsection{SVM Decision Boundary}
\label{sec:12-3-1}
Assume $\theta_0 = 0; n = 2$. Our cost function (to be minimised):
\[J(\theta) = \frac{1}{2} \sum \limits_{j=1}^{n}\theta_j^2 =
\frac{1}{2}(\theta_1^2 + \theta_2^2) = \frac{1}{2}(\sqrt{\theta_1^2 +
  \theta_2^2})^2 = \frac{1}{2}||\theta||^2\]
where \[ \begin{array}{ll}
  \theta^Tx^{(i)} \geq 1 & \textrm{ if } y^{(i)} = 1 \\
  \theta^Tx^{(i)} \leq -1 & \textrm{ if } y^{(i)} = 0
\end{array} \]

Consider $x^{(i)}$. Our product will be $\theta^Tx^{(i)} = p^{(i)}
||\theta|| = \theta_1 x_1^{(i)} + \theta_2 x_2^{(i)}$ - see page 13.

Re - writing optimisation objective: $ \min \limits_{\theta}
\frac{1}{2}\sum \limits_{j=1}^{n} \theta_j^2$,
where \[\begin{array}{ll}
  p^{(i)} * ||\theta|| \geq 1 & \textrm{ if } y^{(i)} = 1 \\
  p^{(i)} * ||\theta|| \leq -1 & \textrm{ if } y^{(i)} = 0
\end{array}\]
where $p^{(i)}$ is the projection of $x^{(i)}$ onto the vector
$\theta$. Simplification: $\theta_0 = 0$ (so the decision boundary
goes through the (0;0); very large C.

(see page 14 for graphical explanation). If $\theta$ vector (as a
normal to the boundary) will be sub-optimal (that is, not having
optimal angle), then:
\begin{itemize}
\item for y = 1: $p^{(i)} * ||\theta|| \geq 1$. The projection
  $p^{(i)}$ is relatively small, so $||\theta||$ should be large;
\item for y = 0: $p^{(i)} * ||\theta|| \leq -1$. The $p^{(i)}$ is
  negative, but relatively small in length, so $||\theta||$ should be
  large;
\end{itemize}
The problem is that this breaks our optimisation criteria (minimising
$||\theta||$). To hold $||\theta||$ small we need to turn the
$\theta$'s angle to optimal value. As such, the longer projections
are, the wider is the margin.

\subsection{Kernels}
\label{sec:12-4}
Kernels is a technique for developing non-linear classifier. Obvious
solution is:

Predict y = 1 if $\theta_0 + \theta_1x_1 + \theta_2x_2 +
\theta_3x_1x_2 + \theta_4x_1^2 + \theta_5x_2^2 + \dots \geq 0$, or
\[ h_\theta(x) = \left\{
  \begin{array}{ll}
    1 & \textrm{ if } \theta_0 + \theta_1x_1 + \dots \geq 0 \\
    0 & \textrm{ otherwise } 
  \end{array} \right. \]
In other format: $\theta_0 + \theta_1f_1 + \theta_2f_2 +
\theta_3f_3 + \theta_4f_4 + \theta_5f_5 + \dots$ where $f_1 = x_1, f_2
= x_2, f_3 = x_1x_2, f_4 = x_1^2, f_5 = x_2^2 \dots $ - need to find
the best choice for $f_i$

\paragraph{Idea}: Given x, compute new feature depending on proximity
to landmarks $l^{(1)}, l^{(2)}, l^{(3)}$ (see page 17): \[f_1 =
similarity(x, l^{(1)}) = \exp(- \frac{\overbrace{||x -
    l^{(1)}||^2}^{\textrm{length of } w = x - l^{(1)}}
}{2\sigma^2}) \]
\[f_2 = similarity(x, l^{(2)}) = \exp(- \frac{||x -
  l^{(2)}||^2}{2\sigma^2}) \]

This ``similarity'' function is called Kernel (or ``Gaussian Kernel'')
function, also written as $k(x, l^{(i)})$

\subsubsection{Kernels and Similarity}
\[f_1 = similarity(x, l^{(1)}) = \exp(- \frac{||x -
  l^{(1)}||^2}{2\sigma^2}) = \exp (- \frac{\sum \limits_{j=1}^{n} (x_j
  - l_j^{(1)})^2}{2\sigma^2}) \]

if $x\approx l^{(1)} : f_1 \approx \exp(-\frac{0^2}{2\sigma^2})
\approx 1 $;

if x is far from $l^{(1)}$ : $f_1 \approx \exp(-\frac{(\textrm{ large
    number })^2}{2\sigma^2}) \approx 0$

Each of these landmarks define a new feature (see page 19). The
$\sigma$ parameter defines how fast the feature value decreases with
distance.

Then, using idea to predict y = 1 if $\theta_0 + \theta_1x_1 +
\theta_2x_2 + \theta_3x_1x_2 + \theta_4x_1^2 + \theta_5x_2^2 + \dots
\geq 0$, we can write $\theta_0, \theta_1, \dots$ - see page 20 for
details.
\label{12-5}

In practice, the landmarks are put into the same location as the
points of the training set - so we measure how close the feature is to
the training set points. Then the feature vector is: \[ f = \left[
  \begin{array}{l} f_0 \\ f_1 \\ f_2 \\ \vdots \\ f_m \end{array}
\right] \], by convention $f_0 = 1$.

Then, for training example $(x^{(i)}, y^{(i)}$: $f_i^{(i)} =
sim(x^{(i)}, l^{(i)}) = 1$; $x^{(i)} \in \mathbf{R}^{n+1}$

\subsection{SVM with Kernels}
\label{sec:12-5-1}

Hypothesis: Given x, compute features $f \in \mathbf{R}^{m+1}$.

Predict ``y=1'' if $\theta^T f \geq 0$

Training:
\[ \min \limits_{\theta} C \sum \limits_{i=1}^{m} y^{(i)}
cost_1(\theta^T f^{(i)}) + (1 - y^{(i)})cost_0(\theta^T f^{(i)}) +
\frac{1}{2}\sum \limits_{j=1}^{n}\theta_j^2 \] but, instead of
$\theta^Tx^{(i)}$ we use $\theta^Tf^{(i)}$ (and regularisation
parameters sum is from 1 to m, not n) - and solve ``as normal''.

Another way to write the regularisation sum: $\sum \limits_{j}
\theta_j^2 = \theta^T \theta$ - ignoring $\theta_0$! So many SVM
implementations use some sort of simplification: instead of
calculating $||\theta||^2$ they try to minimise $\theta^T M \theta$ -
this allows to scale to much bigger training sets (up to m = 10,000).

Concrete implementation - ``use off-the-shelf'' soft 8-(.

\subsubsection{SVM Parameters}
\label{sec:12-5-2}

$C ( = \frac{1}{\lambda})$.
\begin{itemize}
\item Large C: Lower bias, high variance, over fitting (small
  $\lambda$).
\item Small C: Higher bias, low variance, under fitting (large
  $\lambda$).
\end{itemize}

$\sigma^2$ : recall $\exp (-\frac{||x-l^{(i)}||^2}{2\sigma^2})$
\begin{itemize}
\item Large $\sigma^2$: features $f_i$ vary more smoothly. Higher
  bias, lower variance.
\item Small $\sigma^2$: features $f_i$ vary less smoothly. Lower bias,
  higher variance.
\end{itemize}

\subsection{Using An SVM}
\label{sec:12-6}
Ideas:
\begin{itemize}
\item use SVM software packages (e.g. liblinear, libsvm) to solve for
  parameters $\theta$ There you need to specify:
  \begin{itemize}
  \item choice of parameter C.
  \item choice of kernel (similarity function), e.g.
    \begin{itemize}
    \item No kernel (``linear kernel'', predict y=1 if $\theta^T \geq
      0$ where $\theta$ is a ``standard'' linear classifier). Works
      fine when you have a large number of features (and relatively
      small no of train samples)
    \item Gaussian kernel: $f1 = \exp(-\frac{||x-l^{(i)}||^2}{2
        \sigma^2})$ where $l^{(i)} = x^{(i)}$. Need to choose
      $\sigma^2$. Works better with smaller n and relatively large
      training sets. Do perform feature scaling before using the
      Gaussian kernel!
    \item Other choices of kernel. Not all similarity functions make a
      vali kernels (need to satisfy technical condition called
      ``Mercer's Theorem'' to make sure SVM packages' optimisation
      runs correctly and do not diverge. These are: polynomial kernel
      ($k(x, l) = (x^T l)^2$, or $(x^T l)^3$, $(x^T l + 1)^3$ etc. -
      kind of $(x^T l + constant)^{degree}$. Usually diverges quite
      slowly, not used too often.
    \item more esoteric: String kernel, chi-square kernel, histogram
      intersection kernel, etc.
    \end{itemize}
  \end{itemize}
\end{itemize}

\subsubsection{Multi-class classification}
\label{sec:12-6-1}
When several classes (K): $y \in \{1, 2, 3, \dots \mathbf{K} \}$. Many
SVM packages do already have the built-in multi-class classification
functionality. Otherwise, use one-vs-all method: train K SVMs, one to
distinguish y = i from the rest, for $i = 1, 2, 3, \dots \mathbf{K}$,
get $\theta^{(1)}, \theta^{(2)}, \dots , \theta^{(K)}$. For concrete
x, pick class i with the largest $(\theta^{(i)})^T x$

\subsubsection{Logistic Regression vs SVMs}
n = number of features ($x \in \mathbf{R}^{n+1}$), m = number of
training examples.
\begin{itemize}
\item If n is large (relative to m), like n = 10,000; m = 10 \ldots
  1000: use logistic regression , or SVM without a kernel (``linear
  kernel'').
\item n is small, m is intermediate (n = 1 \ldots 1000, m = 10 \ldots
  10,000). Use SVM with Gaussian kernel
\item if n is small, m is large (n = 1 \ldots 1000, m > 50,000) :
  create/add more features. then use logistic regression or SVM
  without a kernel (just like a case $n \geq m$ - in both cases we do
  similar things)
\end{itemize}

Neural networks likely to work well for most of these settings but may
be slower to train.

Usually good SVM implementation are quite fast, plus they care about
the local optima problems (convex problems) which cause headache for
Neural Networks.

\section{Clustering}
\label{sec:13}

Training set is given without labels: $\left\{ x^{(1)}, x^{(2)},
  x^{(3)}, \dots , x^{(m)} \right\}$

\subsection{K-Means Algorithm}
\label{sec:13-2}
Steps:
\begin{itemize}
\item put two cluster centroids (in random places)
\item assign each of the data points to the relevant cluster -
  depending on distance to centroid
\item move each centroid to the centre of relevant cluster (the mean
  value for coordinates)
\item perform the cluster (re)assignment, move centroid {\it quantum
    sates}
\end{itemize}

\paragraph{K-means algorithm}

\subparagraph{Input:}
\begin{itemize}
\item $\mathbf{ K }$ (number of clusters)
\item Training set $\left\{ x^{(1)}, x^{(2)}, x^{(3)}, \dots , x^{(m)}
  \right\}$
\end{itemize}
$x^{(i)} \in \mathbf{R}^n$ (drop $x_0 = 1$ convention)

\subparagraph{Algorithm:} Randomly initialise $\mathbf{K}$ cluster
centroids $\mu_1, \mu_2, \dots, \mu_K \in \mathbf{R}^n$

\begin{tabbing}
  Repeat \{    \\
  \quad \quad \= for i = 1 to m \\
  \quad \quad \> \quad \=  $c^{(i)}$ := index (from 1 to K) of cluster
  centroid \\
  \quad \quad \> \quad \> closest to $x^{(i)}$ (usually using
  $||x^{(i)} - \mu_k||^2$ for minimisation - the k will be the same) \\
  \quad \quad \> for k = 1 to $\mathbf{K}$ \\
  \quad \quad \> \quad \> $\mu_k$ := average (mean) of points assigned
  to cluster k \\
  \}
\end{tabbing}
Sometimes, some clusters end up with no points assigned -- in this
case it is better to remove excessive cluster (so to end up with K-1
cluster). If you really need K clusters - re-initiate the whole
process with K random centroids (but better go on with K-1).

\subsection{Optimisation Objective}
\label{sec:13-3}
terms:
\begin{itemize}
\item $c^{(i)}$ = index of cluster (1, 2, \dots, K) to which example
  $x^{(i)}$ is currently assigned
\item $\mu_k$ = cluster centroid k $(\mu_k \in \mathbf{R}^n)$
\item $\mu_c(i)$ = cluster centroid of cluster to which example
  $x^{(i)}$ has been assigned
\end{itemize}

Optimisation objective: \[ J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots,
\mu_K) = \frac{1}{m} \sum \limits_{i=1}^m || x^{(i)} - \mu_c(i) ||^2
\]
\[ \min \limits_{ \begin{array}{l}
    c^{(1)}, \dots, c^{(m)}, \\
    \mu_1, \dots, \mu_K
  \end{array}} J(c^{(1)}, \dots, c^{(m)}, \mu_1, \dots, \mu_K) \]

This function J is sometimes called ``Distortion''

As such, our algorithm becomes:
\begin{tabbing}
  Repeat \{    \\
  Cluster assignment step: minimise J(...) with $c^{(1)}, \dots,
  c^{(m)}$ holding $\mu_1, \dots, \mu_K$  \\

  Move centroid step: minimise J(...) with  $\mu_1, \dots, \mu_K$
  holding  $c^{(1)}, \dots, c^{(m)}$ \\
  \}
\end{tabbing}

\subsection{Random Initialisation}
\label{sec:13-4}
The items are:
\begin{itemize}
\item Should have $\mathbf{K} < m$
\item Randomly pick K training examples
\item Set $\mu_1, \dots, \mu_K$ equal to these K examples
\end{itemize}

As K-means result depends on starting positions (in particular, it can
end up with local optima - i.e. joining two clusters into one etc). To
avoid this, we should try a number of different initialisation
variants (lets say, 100):

\begin{tabbing}
  For i = 1 to 100 \{    \\
  \quad Randomly initialise K-means \\
  \quad Run K-means. Get $c^{(1)}, \dots,c^{(m)}, \mu_1, \dots, \mu_K$  \\
  \quad Compute cost function (distortion)  $J(\mu_1, \dots, \mu_K, c^{(1)}, \dots, c^{(m)})$ \\
  \}
\end{tabbing}
Pick clustering that gave lowest cost $J(\mu_1, \dots, \mu_K, c^{(1)},
\dots, c^{(m)})$

That works fine for low K (say, from 2 to 10).

\subsection{Choosing the Number of Clusters}
\label{sec:13-5}

The best way is manually (from visualisation or so). Sometimes
so-called ``elbow method'' is used: compute distortion for different
values of K and select the one which provides the ``elbow'' - a point
where distortion goes down much slower then before.

Another way - if we're using K-means to get clusters to use for some
later/downstream purpose. In this case - evaluate K-means based on a
metric for how well it performs for that later purpose.

\section{Dimentionality Reduction}
\label{sec:14}

\subsection{Motivation I - Data Compression}
\label{sec:14-1}
Example:
\begin{itemize}
\item Reduce from 1D to 2D (through introduction of new dimension
  instead of existing x, y:
  \[
  \begin{array}[l]{rl}
    x^{(1)} \in \mathbf{R}^2 & \rightarrow z^{(1)} \in
    \mathbf{R} \\
    x^{(2)} \in \mathbf{R}^2 & \rightarrow z^{(2)} \in
    \mathbf{R} \\
    \vdots \\
    x^{(m)} \in \mathbf{R}^2 & \rightarrow z^{(m)} \in
    \mathbf{R} \\
  \end{array}
  \]
\item Reduce from 3D to 2D ... 1000D to 100D

  Done through projecting this data (x, y, z) on the plane ($z_1,
  z_2$)
\end{itemize}

\subsection{Motivation II - Visualisation}
\label{sec:14-2}

The idea to project the data set of $\mathbf{R}^{50}$ to
$\mathbf{R}^{2}$ with subsequent drawing it on the plane

\subsection{Principal Component Analysis (PCA) - formulation}
\label{sec:14-3}
It is good idea to perform data normalisation first.

To reduce from 2-dimension to 1-dimension:find a direction (a vector
$u^{(1)} \in \mathbf{R}^2$) onto which to project the data so as to
minimise the projection error.

To reduce from n-dimension to k-dimension:find k vectors $u^{(1)},
u^{(2)}, \dots, u^{(k)} \in \mathbf{R}^n$ - so-called ``linear
subspace of dimension k'' onto which to project the data so as to
minimise the projection error.

PCA is completely separated from the ``linear regression'': here we
find the line minimising the projection error while in latter we try
to predict y-value based on x-value and providing a ``prediction
line'' based on ``pure'' difference in y values. (see pp. 12, 13). All
dimensions are ``equal'': we do not have a separate values we know and
other values we want to predict.

\subsection{Principal Component Analysis (PCA) - algorithm}
\label{sec:14-4}

\paragraph{Data Prepossessing}

Training set: $x^{(1)}, x^{(2)}, \dots, x^{(m)}$

Prepossessing (feature scaling / mean normalisation):
\[ \mu_j = \frac{1}{m} \sum \limits_{i=1}^{m} x_j^{(i)} \]

Replace each $x_j^{(i)}$ with $x_j - \mu_j$.

If different features on different scales (e.g, $x_1$ = size of house,
$x_2$ = number of bedrooms), scale features to have comparable range
of values: $ x_j^{(i)} \gets \frac{x_j - \mu_j}{S_j}$, where $S_j$ -
standard deviation for dimension j.

Full math proof is ``beyond the scope of the course''. Result is:

\begin{algorithm}
  \caption{Principal Component Analysis (PCA) Algorithm - Reduce data
    from n-dimensions to k-dimensions }
  \begin{algorithmic}
    \STATE Compute ``covariance matrix'': \[ \Sigma = \frac{1}{m} \sum
    \limits_{i=1}^{n} (x^{(i)})(x^{(i)})^T \]
    \STATE Compute ``eigenvectors'' of matrix $\Sigma$: \\
    $\left[U, S, V\right] = svd(Sigma);$; Sigma is $n \times n$ matrix
    Then we'll get the U matrix: \[U = \left[
      \begin{array}{ccccc}
        | & | & | & & | \\
        u^{(1)} & u^{(2)} & u^{(3)} & \vdots & u^{(n)} \\
        | & | & | & & | \\
      \end{array}
    \right], U \in \mathbf{R}^{n \times n}\] \STATE So we take first k
    columns (vectors): $u^{(1)}, \dots, u^{(k)} - n \times k$ matrix
    (named $U_{reduce}$). So operation $x \in \mathbf{R}^n \rightarrow
    z \in \mathbf{R}^k$ becomes:

    \[ z = U_{reduce} x = \left[
      \begin{array}{ccccc}
        | & | & | & & | \\
        u^{(1)} & u^{(2)} & u^{(3)} & \vdots & u^{(k)} \\
        | & | & | & & | 
      \end{array}
    \right]^T x = \left[
      \begin{array}{ccc}
        --- & (u^{(1)})^T & --- \\
        \   & \vdots & \   \\
        --- & (u^{(k)})^T & --- 
      \end{array}
    \right] x
    \]
    where $U_{reduce}$ is of dimension $n \times k$, x is of $n \times
    1$, so $(U_{reduce})^T \times x$ becomes $k \times 1$
    \STATE in Octave, it will be: \\
    \quad $\left[U, S, V\right] = svd(Sigma);$ \\
    \quad $U_{reduce} = U(:, 1: k);$ \\
    \quad $z = U_{reduce}' * x;$
  \end{algorithmic}
\end{algorithm}

% сделать ссылкой внизу страницы!!
{\bf svd} stays for ``singular value decomposition'' - works like {\bf
  eig(Sigma)}, but a bit more stable.

\subsection{Reconstruction from Compressed Representation}
\label{sec:14-6}

The idea is to reconstruct data from reduced dimension data set: $z
\in \mathbf{R} \rightarrow x \in \mathbf{R}^2$, so if $z =
U^T_{reduce} x$, then \[ \underbrace{x_{approx}}_{\mathbf{R}^n} =
\underbrace{U_{reduce}}_{n \times k} \underbrace{z}_{k \times 1} \]

\subsection{Applying PCA}
\label{sec:14-7}

\subsubsection{Speedup supervised learning}
\label{sec:14-7-1}

Given $(x^{(1)}, y^{(1)}),(x^{(2)}, y^{(2)}),\dots, (x^{(m)},
y^{(m})$, and $(x^{(i)} \in \mathbf{R}^{10000}$ (i.e. for image
analysis of $100 \times 100$ pixels. This going to be quite slow, so:
\begin{itemize}
\item Extract input x's to unlabelled dataset $x^{(1)},x^{(2)},\dots,
  x^{(m)} \in \mathbf{R}^{10000}$
\item apply PCA so that dataset becomes $z^{(1)},z^{(2)},\dots,
  z^{(m)} \in \mathbf{R}^{1000}$ Got a new training set: $(z^{(1)},
  y^{(1)}),(z^{(2)}, y^{(2)}),\dots, (z^{(m)}, y^{(m})$; train
  algorithm using $h_\theta(z) = \frac{1}{1 + e^{-\theta^Tz}}$
\end{itemize}

When given a new sample x:
\begin{itemize}
\item calculate $x \rightarrow z$ using $U_{reduce}$
\item use z for making prediction based on $h_\theta(z)$
\end{itemize}
% find out how to separate "Notes":

\subparagraph{Note:} Mapping $x^{(i)} \rightarrow z^{(i)}$ should be
defined by running PCA only on the training set. This mapping can be
applied as well to the examples $x_{cv}^{(i)}$ and $x_{test}^{(i)}$ in
the cross validation and test sets.

\paragraph{Application of PCA}

\begin{itemize}
\item Compression
  \begin{itemize}
  \item Reduce memory/disk needed to store data
  \item Speed up learning algorithm
  \end{itemize}
  Here choose k by \% of variance retained. Good (typical) result - up
  to 99\%
\item Visualisation k = 2 or k = 3
\end{itemize}

\paragraph{Bad use of PCA: To prevent outfitting}

Like use $z^{(i)}$ instead of $x^{(i)}$ to reduce the number of
features to k < n. Thus, fewer features => less likely to overfit.

Bad idea (despite sometimes works). PCA throws away some information
without knowing the values of $y^{(i)}$. Use regularisation instead.

Quite often the whole ML system can go without any dimension reducing
at all. Always need to test on original/raw data before considering
PCA.

\section{Anomaly Detection}
\label{sec:15}

Example: aircraft engine features. Formally, we're given example
(training) set $\{ x^{(1)}, x^{(2)}, \dots, x^{(m)} \}$, we need to
estimate is $x_{test}$ anomalous?

We build a model $p(x)$ - probability model and ``small threshold''
$\epsilon$ such that:
\begin{itemize}
\item $p(x_{test} < \epsilon \rightarrow$ flag anomaly
\item $p(x_{test} \geq \epsilon \to$ OK
\end{itemize}

Another example - fraud detection:
\begin{itemize}
\item $x^{(i)}$ - features of user i' s activities
\item Model $p(x)$ from data (based on features like typing speed,
  pages visited etc).
\item Identify unusual users by checking which have $p(x) , \epsilon$
\end{itemize}
Manufacturing Monitoring computers in a data centre: $x^(i)$ =
features of machine i: $x_1$ = memory use; $x_2$ = number of disk
access/sec; $x_3$ = CPU load; $x_4$ = network traffic etc. Check
$p(x_{test} < \epsilon$

\subsubsection{Gaussian Distribution (Normal Distribution)}
\label{sec:15-2}
Say $x \in \mathbf{R}$. If x is distributed Gaussian with mean $\mu$,
variance $\sigma^2$, it is written as $x \sim \aleph(\mu, \sigma^2)$
(read ``x distributed as Gaussian from mu and sigma''); $\aleph$, or
``N scripted'' stands for ``normal''; $\sigma$ is called ``the
standard deviation''; $\sigma^2$ is a variance:
\[ p(x; \mu, \sigma^2) = \frac{1}{\sqrt{2 \pi} \sigma }
exp(-\frac{(x-\mu)^2 }{2\sigma^2}) \] -p(x) is parametrized by $\mu$
and $\sigma$-squared.

The total square of probability curve integrates to 1; it changes
shape depending on parameters.

\subsubsection{Parameter estimation}
Given dataset $\{x^{(1)}, x^{(2)}, \dots, x^{(m)}\}, x^{(i)} \in
\mathbf{R}$, find out $\mu, \sigma$:
\[\mu = \frac{1}{m} \sum \limits_{i=1}^m x^{(i)} ; \sigma^2 =
\frac{1}{m} \sum \limits_{i=1}^m (x^{(i)} - \mu)^2 \]

Sometimes $\frac{1}{m-1}$ variant is used (with slightly different
mathematical properties), but in practice m is usually quite large so
no much difference.

\subsubsection{Algorithm}
\label{sec:15-3}

Given the training set $\{x^{(1)}, \dots, x^{(m)}\}$, each example is
$x \in \mathbf{R}^n$. We expect that each feature is distributed
according to some Gaussian distribution:
\[ \begin{array}{l}
  x_1 \sim \aleph(\mu_1, \sigma_1^2) \\
  x_2 \sim \aleph(\mu_2, \sigma_2^2) \\
  x_3 \sim \aleph(\mu_3, \sigma_3^2) \\
\end{array} \]

So $p(x) = p(x_1;\mu_1, \sigma_1^2)*p(x_2;\mu_2, \sigma_2^2)* \dots
*p(x_n;\mu_n, \sigma_n^2) = \prod \limits_{j=1}^n p(x_j;\mu_j,
\sigma_j^2)$

- AKA ``Density estimation''. So, the algorithm:
\begin{enumerate}
\item Choose features $x_i$ that you think might be indicative of
  anomalous examples
\item Fit parameters $\mu_1, \dots \mu_n, \sigma_1^2, \dots,
  \sigma_n^2$:
  \[ \mu_j = \frac{1}{m} \sum \limits_{i=1}^m x_j^{(i)}, \sigma_j^2 =
  \frac{1}{m} \sum \limits_{i=1}^m (x_j^{(i)} - /mu_j)^2\]

  In vector form: if $\mu$ is a column vector, then $\mu = \frac{1}{m}
  \sum \limits_{i=1}^m x^{(i)}$; similar story for $\sigma$.
\item Given new example x, compute Gaussian probability $p(x)$:
  \[p(x) = \prod \limits_{j=1}^n p(x_j;\mu_j, \sigma_j^2) = \prod
  \limits_{j=1}^n \frac{1}{\sqrt{2 \pi} \sigma_j} \exp(-\frac{(x_j -
    \mu_j)^2}{2\sigma_j^2}) \] of the example being the ``normal''
  one. Diagnose anomaly if $p(x) < \epsilon$.
\end{enumerate}

Example was with $\epsilon = 0.02$

\subsubsection{Developing and Evaluating an Anomaly Detection system}
\label{sec:15-4}

\subparagraph{The importance of real-number evaluation}

Assume we have some data of anomalous and non- anomalous examples. (y
= 0 if normal, y = 1 is anomalous).
\begin{itemize}
\item Training set: $x^{(1)},x^{(2)}, \dots x^{(m)}$ (assume normal
  examples / not anomalous)
\item Cross validation set: $(x_{cv}^{(1)}, y_{cv}^{(1)}), \dots,
  (x_{cv}^{(m_{cv})}, y_{cv}^{(m_{cv})})$
\item Test set: $(x_{test}^{(1)}, y_{test}^{(1)}), \dots,
  (x_{test}^{(m_{test})}, y_{test}^{(m_{test})})$
\end{itemize}
Example: 10000 good (normal) engines; 20 flawed (anomalous) ones. Even
if some anomalous ``slipped'' into ``normal'' data, it is still OK.

Then (60-20-20 split):
\begin{itemize}
\item Training set: 6000 good engines (y=0)
\item CV: 2000 good engines (y=0), 10 anomalous (y=1)
\item Test: 2000 good engines (y=0), 10 anomalous (y=1)
\end{itemize}

\subparagraph{Algorithm evaluation}

Fit model $p(x)$ on training set, on a cross validation / test example
$x$, predict y based on $p(x)$ and $\epsilon$. Possible evaluation
metrics (NOT accuracy as data is quite skewed!):
\begin{itemize}
\item True positive, false positive, false negative, true negative
\item Precision / recall
\item $F_1$ - score
\end{itemize}

Can also use cross validation set to choose parameter $\epsilon$

\subsection{Anomaly Detection vs Supervised Learning}
\label{sec:15-5}

\begin{tabular}{l|c|l}
  Anomaly Detection & Supervised learning \\  
  \hline
  Very small number of positive examples (y=1): 0-20 is common. 
  Large number of negative (y=0) examples & Large number of positive and
  negative examples \\
  Many different ``types'' of anomalies. Hard for any algorithm to learn
  from positive examples what the anomalies look like; future anomallies
  may look nothing like any of the anomalous examples we've seen so far.
  & Enough positive examples for algorithm to get a sense of what
  positive examples are like, future positive examples likely to be
  similar to ones in training set. \\
\end{tabular}
 
Usual tasks:
\begin{itemize}
\item Anomaly detection
  \begin{itemize}
  \item Fraud detection (sometimes, if you have a lot of y=1 cases,
    this can become Machine Learning problem)
  \item Manufacturing (i.e. aircraft engines)
  \item Monitoring machines in a data centre
  \end{itemize}
\item Supervised learning
  \begin{itemize}
  \item Email spam classification
  \item Weather prediction (sunny/rainy/etc).
  \item Cancer classification
  \end{itemize}
\end{itemize}

\subsection{Choosing Features}
\label{sec:15-6}

\paragraph{Non-gaussian features}

The process depends on the Gaussian distribution of feature: it is
good idea to draw data histogram and check if this is a case. If
actual distribution is not actually Gaussian $\to$ play with
transformations (like apply $log(x)$ or $\log(x_i + c)$) or
$x^{\frac{1}{3}}$).

\paragraph{Error analysis for anomaly detection}

Want $p(x)$ large for normal examples x and small for anomalous
examples x. Most common problem is that $p(x)$ is comparable (say,
both large) for normal and anomalous examples.

In this case, look closely to the anomalous example and try to find
out the additional feature. In common, choose features that might take
on unusually large or unusually small values in the event of anomaly.
Sometimes including features like (for ``Monitoring computers in a
data centre'' example): $x_5 = \frac{\textrm{CPU
    load}}{\textrm{Network traffic}}$ or even $x_6 =
\frac{(\textrm{CPU load})^2}{\textrm{Network traffic}}$

\subsection{Multivariate Gaussian Distribution}
\label{sec:15-7}

The ``standard'' anomaly detection does not check the form of case
distribution - it just multiplies probabilities and so can miss the
anomaly (see p.25 of doc 16). To fix this, use Multivariate Gaussian
Distribution:

$x \in \mathbf{R}^n$; model $p(x)$ all in one go ($\mu \in
\mathbf{R}^n$ - through covariance matrix $\Sigma \in \mathbf{R}^{n
  \times n} $:

\[ \mu = \frac{1}{m} \sum \limits_{i=1}^{m} x^{(i)}; \Sigma =
\frac{1}{m}\sum \limits_{i=1}^{m}(x^{(i)}-\mu)(x^{(i)}- \mu)^T \]
\[p(x; \mu, \Sigma) = \frac{1}{(2 \pi)^{\frac{n}{2}}
  |\Sigma|^{\frac{1}{2}}} \exp(-\frac{1}{2}(x - \mu)^T
\Sigma^{-1}(x-\mu))\]

where $|\Sigma|$ is a determinant of $\Sigma$, calculate in Octave as
$det(Sigma)$

See pp. 27-32 of slides for illustrations on covariance matrix
($\Sigma$) and mean ($\mu$) influence on the probability distribution.

\subsection{Anomaly Detection using the Multivariance Gaussian
  Distribution}
\label{sec:15-8}

Usual algorithm:
\begin{enumerate}
\item Fit model $p(x)$ by setting $\mu$ and $\Sigma$
\item Given a new example x, compute \[ p(x) = \frac{1}{(2
    \pi)^{\frac{n}{2}} |\Sigma|^{\frac{1}{2}}} \exp(-\frac{1}{2}(x -
  \mu)^T \Sigma^{-1}(x-\mu)) \]
\item Flag an anomaly if $p(x) < \epsilon$
\end{enumerate}

Original model: $p(x) = p(x_1; \mu_1, \sigma_1^2) \times p(x_2; \mu_2,
\sigma_2^2) \times \dots \times p(x_n; \mu_n, \sigma_n^2)$

corresponds to multivariate Gaussian ($p(x; \mu, \Sigma)$ where
$\Sigma = \left[
  \begin{array}{cccc}
    \sigma_1^2 & 0 & \dots & 0 \\
    0  & \sigma_2^2 & \dots &0 \\
    \vdots & \vdots & \vdots & \vdots \\
    0 & \dots & & \sigma_n^2 
  \end{array}
\right] $

Uses:
\begin{itemize}
\item Original model
  \begin{itemize}
  \item Manually create features to capture anomalies where $x_1, x_2$
    take unusual combinations of values
  \item Computationally cheaper (scales better to large n: n=10,000,
    n=100,000)
  \item OK even if m (training set size) is small
  \end{itemize}
\item Multivariate Gaussian
  \begin{itemize}
  \item Automatically captures correlations between features
  \item Computationally more expensive ($\Sigma \in \mathbf{R}^{n
      \times n}, \Sigma^{-1}$ is calculated)
  \item Must have $m > n$ or else $\Sigma$ is non-revertible. In
    practice, makes sense when $m \geq 10n$
  \end{itemize}
\end{itemize}

\section{Recommender System}
\label{sec:16}

Recommender system (some implementations) can learn feature from the
raw data (without manually picking them up).
 
\subsection{Problem Formulation}
\label{sec:16-1}
Running example: Predicting movie ratings.

User rate movies using one\^ W zero to five stars. We have four users:
Alice, Bob, Carol, Dave; they rated some of five movies:

\begin{tabular}{c|cccc}
  \textbf{Movie} & \textbf{Alice(1)} & \textbf{Bob(2)} &
  \textbf{Carol(3)} & \textbf{ Dave(4)} \\
  \hline
  Love at last & 5 & 5 & 0 & 0 \\
  Romance forever & 5 & ? & ? & 0 \\
  Cute puppies of love & ? & 4 & 0 & ? \\
  Nonstop car chases & 0 & 0 & 3 & 4 \\
  Swords vs karate & 0 & 0 & 5 & ?
\end{tabular}

Parameters:
\begin{itemize}
\item $n_u$ = no. users (4 in our case)
\item $n_m$ = no. movies (5 in our example)
\item $r(i, j)$ = 1 if user j has rated movie i
\item $y(i, j)$ = rating given by user j to movie i (defined only if
  $r(i, j)$ = 1); values from 0 to 5
\end{itemize}

The task is to predict the missing ratings.

\subsection{Content Based Recommendations}
\label{sec:16-2}

First approach to Recommendation Systems. Introduce a set of features,
e.g $x_1$ (romance) and $x_2$ (action). Technically, add $x_0$ = 1
(``interceptor'' feature). Introduce number of features n = 2.

Then for each user j, learn a parameter $\theta^{(j)} \in
\mathbf{R}^3$. Predict user j as rating movie i with $(\theta^{(j)})^T
x^{(i)}$ stars.

\paragraph{Formulation}

\begin{itemize}
\item $r(i, j)$ = 1 if user j has rated movie i (0 otherwise)
\item $y^{i, j)}$ = rating by user j on movie i (if defined)
\item $\theta^{(i)}$ = parameter vector for user j
\item $x^{(i)}$ = feature vector for movie i
\item For user j, movie i, predicted rating:
  $(\theta^{(j)})^T(x^{i)})$.
\item $m^{(j)}$ = no of movies rated by user j
\end{itemize}
To learn $\theta^{(j)}: \min \limits_{\theta^{(j)}} \frac{1}{2
  m^{(j)}} \sum \limits_{i:r(i,j)=1} \left( (\theta^{(j)})^T (x^{(i)})
  - y^{(i, j)} \right)^2 + \frac{\lambda}{2 m^{(j)}} \sum \limits_{k =
  1}^n (\theta_k^{(j)})^2 $

Optimisation: remove $\frac{1}{m^{(j)}}$ from both part of the
formula. Then:
\begin{itemize}
\item To learn $\theta^{(j)}$ - parameter for user j): \[ \min
  \limits_{\theta^{(j)}} \frac{1}{2} \sum \limits_{i:r(i,j)=1} \left(
    (\theta^{(j)})^T (x^{(i)}) - y^{(i, j)} \right)^2 +
  \frac{\lambda}{2} \sum \limits_{k = 1}^n (\theta_k^{(j)})^2 \]
\item To learn $\theta^{(1)}, \theta^{(2)}, \dots \theta^{(n_u)}$: \[
  \min \limits_{\theta^{(1)}, \dots, \theta^{(n_u)}} \frac{1}{2} \sum
  \limits_{j=1}^{n_u} \sum \limits_{i:r(i,j)=1} \left(
    (\theta^{(j)})^T (x^{(i)}) - y^{(i, j)} \right)^2 +
  \frac{\lambda}{2} \sum \limits_{j=1}^{n_u} \sum \limits_{k = 1}^n
  (\theta_k^{(j)})^2 \] - becomes good old $J(\theta^{(1)}, \dots,
  \theta^{(n_u)}$
\end{itemize}

Then use gradient descent update (based on partial derivatives):

\[ \begin{array}{lll} \theta_k^{(j)} := \theta_k^{(j)} - \alpha
  \sum\limits_{i:r(i,j)=1}
  ((\theta^{(j)})^Tx^{(i)} - y^{(i, j)})x_k^{(i)} & \mathrm{ for } & k = 0\\
  \theta_k^{(j)} := \theta_k^{(j)} - \alpha \left(
    \sum\limits_{i:r(i,j)=1} ((\theta^{(j)})^Tx^{(i)} - y^{(i,
      j)})x_k^{(i)} + \lambda \theta_k^{(j)} \right) & \mathrm{ for }
  & k \neq  0\\
\end{array} \]

Requires manually selecting features $x^{(i)}$ which is not always
easy to do.

\subsection{Collaborative Filtering}
\label{sec:16-3}

This algorithm learns necessary features by itself - without manually
selecting them.

Imagine each user provided its preferences: $\theta^{(1)},
\theta^{(2)}, \dots, \theta^{(n_u)}$, to learn $x^{(i)}$:
\[ \min \limits_{x^{(i)}} \frac{1}{2} \sum \limits_{j:r(i, j) = 1}
((\theta^{(i)})^T x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum
\limits_{k=1}^n (x_k^{(i)})^2 \]

- actually, minimising the square error:
\begin{itemize}
\item $(\theta^{(i)})^T x^{(i)}$ - the predicted value (rating)
\item $y^{(i,j)}$ - actual value given by $j^{th}$ person to $i^{th}$
  movie
\item $\frac{\lambda}{2} \sum \limits_{k=1}^n (x_k^{(i)})^2$ -
  regularisation term
\end{itemize}

Expanding on the idea of learning the whole dataset (the data for all
m movies, $x^{(1)},x^{(2)}, \dots, x^{(n_m)}$):

\[ \min \limits_{x^{(1)},x^{(2)}, \dots, x^{(n_m)}} \frac{1}{2} \sum
\limits_{i=1}^{n_m} \sum \limits_{j:r(i, j) = 1} ((\theta^{(i)})^T
x^{(i)} - y^{(i,j)})^2 + \frac{\lambda}{2} \sum \limits_{i=1}^{n_m}
\sum \limits_{k=1}^n (x_k^{(i)})^2 \]

\paragraph{Collaborative filtering}
\begin{itemize}
\item Given $x^{(1)}, \dots, x^{(n_m)}$ (and movie ratings), can
  estimate $\theta^{(1)}, \dots, \theta^{(n_u)}$.
\item Given $\theta^{(1)}, \dots, \theta^{(n_u)}$ can estimate
  $x^{(1)}, \dots, x^{(n_m)}$
\end{itemize}

So chicken and egg problem. To solve: randomly guess initial $\theta$,
learn initial features x, then make better estimation for $\theta$,
get back: $\theta \to x \to \theta \to x \to \dots$

So we simultaneously learn user's preferences and movie
characteristics. The whole ``collaborative filtering'' term refers to
collaboration between users to learn better features.

\subsection{Collaborative Filtering Algorithm}
\label{sec:16-4}

The idea is to minimise $x^{(1)}, \dots, x^{(n_m)}$ and $\theta^{(1)},
\dots, \theta^{(n_u)}$ simultaneously: \[ J(x^{(1)}, \dots, x^{(n_m)},
\theta^{(1)},\dots, \theta^{(n_u)} = \frac{1}{2} \sum \limits_{(i,
  j):r(i, j) = 1} ((\theta^{(j)})^Tx^{(i)} - y^{(i, j)})^2 +
\frac{\lambda}{2} \sum \limits_{i=1}^{n_m} \sum
\limits_{k=1}^n(x_k^{(i)})^2 + \frac{\lambda}{2} \sum
\limits_{j=1}^{n_u} \sum \limits_{k=1}^n (\theta_k^{(j)})^2 \]

determining \[\min \limits_{
  \begin{array}{c}
    x^{(1)}, \dots, x^{(n_m)} \\    
    \theta^{(1)},\dots, \theta^{(n_u)}
  \end{array}} J(x^{(1)}, \dots, x^{(n_m)}, \theta^{(1)},\dots,
\theta^{(n_u)})\]

where $x \in \mathbf{R}^n$ - not $x \in \mathbf{R}^{n+1}$, and $\theta
\in \mathbf{R}^n$

\paragraph{Collaborative filtering algorithm}

\begin{enumerate}
\item Initiate $x^{(1)}, \dots, x^{(n_m)}, \theta^{(1)},\dots,
  \theta^{(n_u)}$ to small random values.
\item Minimise $J(x^{(1)}, \dots, x^{(n_m)}, \theta^{(1)},\dots,
  \theta^{(n_u)})$ using gradient descent (or an advanced optimisation
  algorithm). E.g. for every $j = 1, \dots, n_u; i = 1, \dots, n_m$:
  \[x_k^{(i)} := x_k^{(i)} - \alpha \underbrace{\left(\sum
      \limits_{j:r(i,j) = 1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})
      \theta_k^{(j)} + \lambda x_k^{(i)} \right)}_{ }
  \]
  \[\theta_k^{(j)} := \theta_k^{(j)} - \alpha \underbrace{\left(\sum
      \limits_{j:r(i,j) = 1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})
      x_k^{(i)} + \lambda \theta_k^{(j)}
    \right)}_{\frac{\delta}{\delta \theta_k^{(j)}} J(\dots)} \]
\item For a user with parameters $\theta$ and a movie with (learned)
  features x, predict a start rating of $\theta^T x$.
\end{enumerate}

\subsection{Vectorisation: Low rank matrix factorisation}
\label{sec:16-5}

Create matrix of ratings with $n_m = 5 \times n_u=4$:
\[ \mathbf{Y} = \left[
  \begin{array}{cccc}
    5 & 5 & 0 & 0 \\
    5 & ? & ? & 0 \\
    ? & 4 & 0 & ? \\
    0 & 0 & 5 & 4 \\
    0 & 0 & 5 & 0
  \end{array} \right]\]

Assuming that predicted rating is $(\theta^{(j)})^T(x^{(i)})$: \[
\left[
  \begin{array}{cccc}
    (\theta^{(1)})^T(x^{(1)}) &  (\theta^{(2)})^T(x^{(1)}) & \dots &
    (\theta^{(n_u)})^T(x^{(1)}) \\
    (\theta^{(1)})^T(x^{(2)}) &  (\theta^{(2)})^T(x^{(2)}) & \dots &
    (\theta^{(n_u)})^T(x^{(2)}) \\
    \vdots & \vdots & \vdots & \vdots \\
    (\theta^{(1)})^T(x^{(n_m)}) &  (\theta^{(2)})^T(x^{(n_m)}) & \dots &
    (\theta^{(n_u)})^T(x^{(n_m)}) \\
  \end{array}
\right]
\]

Define matrices \[\mathbf{X} = \left[
  \begin{array}{c}
    - (x^{(1)})^T - \\
    - (x^{(2)})^T - \\
    \vdots \\
    - (x^{(n_m)})^T - \\
  \end{array}
\right], \mathbf{\Theta} = \left[
  \begin{array}{c}
    - (\theta^{(1)})^T - \\
    - (\theta^{(2)})^T - \\
    \vdots \\
    - (\theta^{(n_m)})^T - \\
  \end{array}
\right]\]

Predicted ratings then: $\mathbf{X * \Theta^T}$; the whole process is
called {\bf Low rank matrix factorisation} (because of mathematical
property ``low rank matrix'' of the whole $X * |theta^T$ product).

\paragraph{Finding related movies}

For each product i we learn a feature vector $x^{(i)} \in
\mathbf{R}^n$. To find movies j related to movie i: find movies whose
distance to i ($||x^{(i)} - x^{(j)}||$) is small. This would mean that
movies j and i are ``similar''.

\subsection{Implementation Detail: Mean Normalisation}
\label{sec:16-6}

Motivation: imagine we have a user (Eve) which did not rate any movie
yet. In this case only term affecting her cost function value is the
regularisation term \[\frac{\lambda}{2} \sum \limits_{i=1}^{n_m} \sum
\limits_{k=1}^n(x_k^{(i)})^2 + \frac{\lambda}{2} \sum
\limits_{j=1}^{n_u} \sum \limits_{k=1}^n (\theta_k^{(j)})^2\]

Obviously this will be minimal when (for number of features n = 2):
$\theta^{(5)} = \left[ \begin{array}{c} 0 \\ 0 \end{array} \right]$

Then for any x the $(\theta^{(5)})^T x^{(i)}$ which is not very
useful. In this case it'd better to recommend a movie with ``overall''
high rating (do distinguish from movies which got ``0''s from previous
users).

The mean normalisation is going to fix this. We compute vector $\mu$
which grabs the mean rating from all movies:
\[ \mathbf{Y} = \left[
  \begin{array}{ccccc}
    5 & 5 & 0 & 0 & ?\\
    5 & ? & ? & 0 & ? \\
    ? & 4 & 0 & ? & ? \\
    0 & 0 & 5 & 4 & ? \\
    0 & 0 & 5 & 0 & ?
  \end{array} \right]; 
\mu = \left[\begin{array}{c}  2.5 \\ 2.5 \\ 2 \\ 2.25 \\
    1.25\end{array}\right]  \to
\mathbf{Y} = \left[ 
  \begin{array}{ccccc}
    2.5 & 2.5 & -2.5 & -2.5 & ? \\
    2.5 & ? & ? & -2.5 & ? \\
    ? & 2 & -2 & ? & ? \\
    -2.25 & -2.75 & 2.75 & 1.75 & ? \\
    -1.25 & -1.25 & 3.75 & -1.25  & ?
  \end{array} \right]
\]

Now use this derived matrix for learning $\theta^{(j)}, x^{(i)}$: for
user j on movie i predict $(\theta^{(j)})^T (x^{(i)}) + \mu_i$.

For user 5 (Eve): $\theta^{(5)} = \left[ \begin{array}{c}  0 \\
    0 \end{array} \right] \to prediction = \mu_i $ which makes some
sense: we expect ``average'' rating from the user we have no rating at
all yet.

Similar trick with unrated movies is possible but might be not that
important.

\section{Large Scale Machine Learning}
\label{sec:17}

Learning with large datasets (near $1 * 10^8$ samples) adds a set of
computational problems - like summation of m entries on the single
step of gradient descent.

The good idea is to make a ``sanity check'' of the algorithm itself on
some subset of a data. Sometimes a small part of data gives a good
idea of algorithm properties (like high bias etc).

\subsection{Stochastic Gradient Descent}
\label{sec:17-2}

\paragraph{Linear regression with gradient descent}

The ``usual'' approach for a number of problems is to use a Gradient
Descent to optimise some cost function J. This approach could be quite
computationally expensive as training set size reaches really high
values.

Suppose we're learning Linear Regression with hypothesis $h_\theta(x)
= \sum \limits_{j=0}^n \theta_{j} x_{j}$ and cost function
$J_{train}(\theta) = \frac{1}{2m}\sum
\limits_{j=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2$

Algorithm looks like:
\begin{tabbing}
  Repeat \{    \\
  \quad \quad \= $\theta_j := \theta_j - \alpha \frac{1}{m} \sum
  \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} $ \\
  \quad \quad \> (for every $j = 0, \dots, n$) \\
  \}
\end{tabbing}

The whole idea of this algorithm is that we initialise a process with
some random vector $\theta$ and move (``descend'') to the optimum
using derivatives. This can be quite expensive: the summation term in
algorithm will need to summarise m elements for every n dimensions
{\it on each step}. Such algorithm is also called a ``batch gradient
descent''.

To optimise this process, we use Stochastic gradient descent:
\[
\begin{array}{c|c}
  \textnormal{\bf Batch gradient descent} & \textnormal{\bf Stochastic gradient descent}
  \\
  J_{train}(\theta) = \frac{1}{2m}\sum
  \limits_{j=1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 & cost(\theta, 
  (x^{(i)}, y^{(i)})) = \frac{1}{2} (h_\theta(x^{(i)}) - y^{(i)})^2 \\ 
  \quad & J_{train}= \frac{1}{m} \sum \limits_{i=1}^m cost(\theta,
  (x^{(i)}, y^{(i)})) \\
  \begin{array}{l}
    \mathrm{ Algorithm: Repeat } \{ \\
    \quad  \theta_j := \theta_j - \alpha \frac{1}{m} \sum
    \limits_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \\
    \quad  (\textnormal{for every } j = 1, \dots, n) \\
    \}
  \end{array} 
  & 
  \begin{array}{l}
    \mathrm{ Algorithm:}  \\
    1. \textnormal{Randomly shaffle dataset}  \\
    2. \textnormal{Repeat } \{ \\
    \quad \mathrm{for } i = 1, \dots, m \{ \\
    \quad \quad \theta_j := \theta_j - \underbrace{(h_\theta(x^{(i)}) -
      y^{(i)})x_j^{(i)}}_{ \frac{\delta}{\delta\theta_j} cost(\theta,
      (x^{(i)}, y^{(i)}))} \\ 
    \quad \quad (\textnormal{for every } j = 0, \dots, n) \\
    \quad \}\\
    \}
  \end{array} 
\end{array}
\]

calculate cost for each training example as its' square error in
respect to $\theta$. Then, on each outside iteration step, loop over
samples and on each step ``slightly'' modify $\theta$ towards better
fitting of current $i^{th}$ sample. This speeds up things a bit:
rather then summarising the whole errors, use each sample for small
modifications.

In ``general'', every step does not always move solution to the
optimum, but overall process converges to the solution point. Amount
of outside loops can be quite small - sometimes even 1. It is
considered enough to make up to 10 iterations.

This idea is applicable to other algorithms (logistic regression,
neural network etc).

\subsection{Mini-Batch Gradient Descent}
\label{sec:17-3}

Sometimes a bit faster then the Stochastic Gradient Descent.
\begin{itemize}
\item Batch gradient descent: Use all m examples in each iteration
\item Stochastic gradient descent: Use 1 example in each iteration
\item Mini-batch gradient descent: Use $b$ examples in each iteration
\end{itemize}

There $b$ is called mini-batch size, usually somewhere in $2 \dots
100$.

Example: let $b = 10$, examples $(x^{(i)}, y^{(i)}), \dots, x^{(i+9)},
y^{(i+9)}))$. On each iteration: \[ \theta_j = \theta_j - \alpha
\frac{1}{10} \sum \limits_{k=i}^{i+9}(h_\theta(x^{(k)} - y^{(k)})
x_j^{(k)}\] Next iteration: $ i := i + 10$, so for $b=10, m = 1000$ i
will have values $1, 11, 21, 31, \dots, 991$

This can be faster then Stochastic algorithm due to vectorization (and
paralleling work).

\subsection{Stochastic Gradient Descent Convergence}
\label{sec:17-4}

\subsubsection{Checking for convergence}
\begin{itemize}
\item Batch gradient descent
  \begin{itemize}
  \item Plot $J_{train}(\theta)$ as a function of the number of
    iterations of gradient descent
  \item $J_{train}(\theta) = \frac{1}{2m} \sum \limits_{i=1}^m
    (h_\theta(x^{(i)}) - y^{(i)})^2$
  \end{itemize}
\item Stochastic gradient descent:
  \begin{itemize}
  \item $cost(\theta, (x^{(i)}, y^{(i)})) =
    \frac{1}{2}(h_\theta(x^{(i)}) - y^{(i)})^2$
  \item During learning, compute $cost(\theta, (x^{(i)}, y^{(i)}))$
    before updating $\theta$ using $(x^{(i)},y^{(i)})$.
  \item Every 1000 iterations (say), plot $cost(\theta, (x^{(i)},
    y^{(i)}))$ averaged over the last 1000 examples processed by
    algorithm.
  \end{itemize}
\end{itemize}

Picture - see page 14 of doc 17:
\begin{itemize}
\item UL - two variants with bigger and smaller $\alpha$ (learning
  rate). Sometimes smaller rate leads to better result (if algorithm
  as a whole is OK)
\item UR - increasing d from 1000 to 5000. Smoother curve but lost
  some precision.
\item LL (cost does not decrease) - one can try to increase d - this
  will make algorithm to converge slowly
\item LR - algorithm diverges. Use smaller $\alpha$.
\end{itemize}

Learning rate $\alpha$ is typically constant. Can slowly decrease
$\alpha$ over time if we want $\theta$ to converge, e.g. $\alpha =
\frac{const1}{iterationNumber + const2}$. This can not only speed up
the convergence but also give slightly better hypothesis (but this
requires more complex tuning and makes whole algorithm more complex).

\subsection{Online Learning}
\label{sec:17-5}

Works on the continuous stream of data.

\paragraph{Example}

Shipping service website where user comes, specifies origin and
destination, you offer to ship their package for some asking price,
and user sometimes choose to use your shipping service ($y=1$),
sometimes not ($y=1$).

Features x capture properties of user, of origin/destination and
asking price. We want to learn $p(y=1|x, \theta)$ to optimise price.

Start with logistic regression: Repeat forever {
  \begin{itemize}
  \item Get (x, y) corresponding to user
  \item Update $\theta$ using $(x, y)$ (not $(x^{(i)}, y^{(i)})$ - we
    discard this example immediately after using):
    \[ \theta_j := \theta_j - \alpha (h_\theta(c) - y) x_j; j = 0,
    \dots, n \]
  \end{itemize}
}

It adapts to changing user preferences - i.e. when new type of users
join, or because of changes in economy users tend to pay less etc.

\paragraph{Example 2 }

Product search (learning to search).

User searches for ``android phone 1080p camera'' Have 100 phones in
store. Will return 10 results,
\begin{itemize}
\item For each phone, given specific user query, we build feature
  vector x: how many words in user query match name of phone, how many
  words in query match description of phone etc.
\item y = 1 if user clicks on link, y = 0 otherwise
\item Learn $p(y = 1|x;\theta)$
\end{itemize}
So return (show to user) 10 phones they're most likely to click on.

So every time we'll get 10 pairs $(x, y)$ - so we'll get 10 training
examples on each user iteration. This data can be discarded after use
too; we also can show ``10 best'' every time for particular user.

Other name for this problem - ``Click - Through Rate (CTR)
prediction''. Other examples include: choosing the special offers to
show user; customised selection of news articles; product
recommendation etc.

\subsection{Map Reduce and Data Parallelism}
\label{sec:17-6}

Some ML problems are too big to run on the single machine so data is
processed in parallel.

\paragraph{Map-reduce}

Let's assume we run a Batch gradient descent with m = 400: $\theta_j
:= \theta_j - \alpha \frac{1}{400} \sum_{i=1}^{400} (h_\theta(x^{(i)})
- y^{(i)}) x_j^{(i)}$. We can parallel this as:
\begin{itemize}
\item Machine 1: Use $(x^{(1)}, y^{(1)}), \dots, (x^{(100)},
  y^{(100)})$ to compute \[temp_j^{(1)} = \sum_{i=1}^{100}
  (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\]
\item Machine 2: Use $(x^{(101)}, y^{(101)}), \dots, (x^{(200)},
  y^{(200)})$ to compute \[temp_j^{(2)} = \sum_{i=101}^{200}
  (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}\]
\item $\vdots$
\end{itemize}
Now combine the results: $\theta_j = \theta_j - \alpha
\frac{1}{400}(temp_j^{(1)} + temp_j^{(2)} + temp_j^{(3)} +
temp_j^{(4)})$ - for $j = 0,\dots, n$

This approach works for many algorithms - if they allow separation
(i.e. in summations). Another example - advanced optimisation with
logistic regression:

\[ J_{train}(\theta) = -\frac{1}{m} \sum \limits_{i=1}^m y^{(i)} \log
h_\theta(x^{(i)}) - (1 - y^{(i)}) \log (1-h_\theta(x^{(i)})) \]
\[\frac{\delta}{\delta \theta_j} J_{train}(\theta) = \frac{1}{m}
\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) * x_j^{(i)} \]

can be distributed by ranges of m.

The same story is about distribution between processing cores on the
same machine. The network latency will be no issue.

\section{Application Example: Photo OCR}
\label{sec:18}

OCR - Optical Character Recognition

\subsection{Problem Description and Pipeline}
\label{sec:18-1}

Need to find out the text regions and decode them looking at the
picture. Steps:
\begin{enumerate}
\item Text detection
\item Character segmentation
\item Character classification
\end{enumerate}

\subsection{Sliding Windows}
\label{sec:18-2}

Is a type of classifier used for {\bf text detection}.

Illustrated on the example of pedestrian detection. The idea is to use
a small standardised window which moves over the photo and uses a sort
of classifier (i.e. trained on some sort of sample data) to detect the
cases when the window lays over the pedestrian. The move is made with
step of some size (called ``stride''): ideally equal to 1 pixel (but
this can be computational expensive). 

Such moves are done with the windows of different sizes: from smallest
to reasonable large ones.

Returning to characters: train classifier on various fonts and/or
letter combination. Then, run sliding window over the photo, get the
probabilities of ``this fragment is a text'' events. Next, apply
so-called ``expansion'': take each of the ``white'' pixel and paint
all pixels around it in range, say, 5 pixels.

Next, select white areas which look like rectangles and build
rectangles (bound boxes) around them (see page 14 of Notes).

\paragraph{Character Segmentation} 

is performed using 1D-sliding window (slide a window 1 direction
once). Use logistic regression / neural network train a classifier as
shown at page 15. 

Last step - {\bf character classification} performed ``as usual'',
decoding bitmap pictures into characters.

\subsection{Getting Lots od Data and Artificial Data}
\label{sec:18-3}

The idea of artificial Data synthesis comprises of two approach:
\begin{itemize}
\item creating new training data from scratch.

For OCR we still need to collect some real-life data. To generate some
artificial part of it, take the font library and place the characters
on some random background (see example of synthetic data on pp 19-20).

\item getting a ``small'' real-life training set and amplify it.

We take a real-life images and add some artificial distortions (like
on p 21). This distortion should be representation of the type of
noise?distortions in the test set; plus it usually does {\bf not} help
to add purely random / meaningless noise to the data.
\begin{itemize}
\item if $x_i = $ intensity (brightness) if pixel i
\item then in amplified data: $x_j \leftarrow x_j + \textnormal{random noise}$
\end{itemize}

Example (for speech recognition): use the original audio and add some
noises. 
\end{itemize}

\paragraph{Discussion on getting more data}

\begin{enumerate}
\item Make sure you have a low bias classifier before expending the
  effort (plot learning curves). E.g. keep increasing the number of
  features/number if hidden units in neural network until you have a
  low bias classifies.
\item ``How much work would it be to get 10x as much data as we
  currently have?'' - sometimes not so much efforts with:
  \begin{itemize}
  \item Artificial data synthesis
  \item Collect / label it yourself
  \item ``Crowd source'' (like Amazon Mechanical Turk)
  \end{itemize}
\end{enumerate}

\subsection{Ceiling Analysis: What Part of Pipeline to Work on Next}
\label{sec:18-4}

Ceiling analysis is used to prioritise efforts. We do it by {\bf
  estimating errors due to each component}. On example of OCR: what
part of the pipeline should you spend the most time trying to improve? 

For this purpose, it is advisable to have a single-row metric (like
accuracy, in \%). Then play some tricks: 

take some test data, manually label it with correct answers (simulate
perfect, or 100\% accuracy of this particular module), estimate the
overall system accuracy, like:

\begin{tabular}{c|c}
  \textbf{Component} & \textbf{Accuracy} \\
  \hline
  Overall System & 72\% \\
  Text detection & 89\% \\
  Character segmentation & 90\% \\
  Character recognition & 100 \%
\end{tabular}
- i.e. when Text detection provides 100\% accuracy, overall systems
improves from 72 to 89 \%, when Character segmentation work perfectly
well - from 89 to 90\% etc.

So it becomes obvious that improvements on Character segmentation step
are not very promising (as this will give only 1\% improvement).

Another example - Face recognition system. Steps:
\begin{itemize}
\item Pre-process (removing background)
\item Face detection:
  \begin{itemize}
  \item Eyes segmentation
  \item Nose segmentation
  \item Mouth segmentation
  \end{itemize}
\item Logistic regression
\end{itemize}

Here the ceiling analysis will look like:
\begin{tabular}{c|c}
  \textbf{Component} & \textbf{Accuracy} \\
  \hline
  Overall System & 85\% \\
  Pre-process (remove background)& 85.1\% \\
  Face detection & 91\% \\
  Eyes segmentation & 95\% \\
  Nose segmentation & 96\% \\
  Mouth segmentation & 97\% \\
  Logistic regression & 100\%
\end{tabular}

The component which looks most promising is Face detection (5.9\%
increase). 

\section{Summarise}
\label{sec:19}

\begin{itemize}
\item Supervised learning
  \begin{itemize}
  \item Linear regression, logistic regression, neural networks, SVM
  \end{itemize}
\item Unsupervised learning 
  \begin{itemize}
  \item K-means, PCA (principal component analysis: reducing data from
    n-dimensions to k-dimensions , Anomaly detection
  \end{itemize}
\item Special applications/special topics
  \begin{itemize}
  \item Recommender systems, large scale machine learning.
  \end{itemize}
\item Advice on building a machine learning system
  \begin{itemize}
  \item Bias/variance, regularisation, deciding what to work on next,
    evaluation of learning algorithms, learning curves, error
    analysis, ceiling analysis.
  \end{itemize}
\end{itemize}


\end{document}
