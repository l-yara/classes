%\documentclass{ncc}
%\documentclass{article}
\documentclass{scrartcl}


 % Typical maths resource packages
\usepackage{amsmath,amssymb,amsfonts,amscd}
 % Packages to allow inclusion of graphics
\usepackage{graphics}                
% For creating coloured text and background
\usepackage{color}                    
% For creating hyperlinks in cross  references 
\usepackage{hyperref}                 

\usepackage{algorithm, algorithmic}
\usepackage{textcomp}

\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian,english]{babel}
\usepackage{listings}
\lstloadlanguages {[LaTeX]TeX, Octave}
\lstset {language=[LaTeX]TeX,
  extendedchars=true ,escapechar=|}

\newcommand{\cplx}{\mathbb{C}} % Complex numbers space
\newcommand{\bigR}{\mathbb{R}} % Real numbers space
\newcommand{\hilbert}{\mathbb{H}} % Hilbert space
\newcommand{\ket}[1]{\left| #1 \right>} % for Dirac kets
\newcommand{\bra}[1]{\left< #1 \right|} % for Dirac bras
\newcommand{\braket}[2]{\left< #1 | #2 \right>} % for Dirac brakets
\newcommand{\half}{\frac12} % one half
\newcommand{\hqrt}{\frac1{\sqrt2}} %
\newcommand{\means}{\Rightarrow} % "means" or "leads to"
\newcommand{\same}{\Leftrightarrow} % "means" or "leads to"
\newcommand{\slide}[2]{slide~#1/#2} % reference to slides
\newcommand{\video}[2]{ } % reference to video - empty 
%\newcommand{\video}[2][ ]{video #1~#2} % reference to video

\begin{document}

\section{Axioms}
\label{sec:2, week 1}

\subsection {Superposition Principle}
\label{sec:2-1}

\begin{itemize}
\item{\bf Superposition axiom} Suppose we have a k-level quantum system
  \begin{itemize}
  \item k distinguishable or classical states of the system
  \item Possible classical states $\ket{0},\ket{1}, \dots , \ket{k-1}$
  \end{itemize}
\item {\bf Superposition principle:} if a system can be in one of the k states,
  it can also be in any linear superposition of k states: \[\alpha_0 \ket{0} +
  \alpha_1 \ket{1} + \dots + \alpha_{k-1} \ket{k-1} \] where $\alpha_i \in
  \cplx$ - complex ``probability amplitude'' for each of these states: \[\sum
  \limits_{i=0}^{k-1} |\alpha|^2=1\]

  Examples for k = 3:

  $\frac 1{\sqrt{3}} \ket0 + \frac1{\sqrt{3}} \ket1 + \frac1{\sqrt{3}}\ket2$ or
  $\frac1{2}\ket0 - \frac1 {\sqrt{2}}\ket1 + (\frac1{\sqrt{2}} +
  \frac{i}{\sqrt{2}}) \ket2$
\end{itemize}

\subsection{Measurement axiom}
\begin{itemize}
\item Suppose our system is in state $$\ket\psi = \alpha_0\ket0 + \alpha_1\ket1
  + \ldots + \alpha_{k-1} \ket{k-1} $$
\item Measure: outcome is one of the k classical states
  \begin{itemize}
  \item Observe j with probability $|\alpha_j|^2$.
  \item New state = $\ket j$.
  \end{itemize}
\end{itemize}

\paragraph{Qubit}

Two-level systems (k=2) are called {\bf qubits}. These can be presented as
system where electron can be either in ``ground'' state $\ket0$ or in excited
state $\ket1$. The ``common'' state is a superposition $\alpha_0 \ket0 +
\alpha_1 \ket1$

\subsubsection{Geometrical Interpretation}
\label{sec:2-2}

The notation of state (linear superposition) can be presented also:
\[\alpha_0 \ket0 + \alpha_1 \ket1 + \dots +
\alpha_{k-1} \ket{k-1} = \begin{pmatrix} \alpha_0 \\ \alpha_1 \\ \vdots \\
  \alpha_{k-1} \end{pmatrix}\] As so we can present the state as a vector in
some space of k-measures where axes are vectors representing $i^{th}$ states:
$\begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}$ represents the system in ground
state ($\ket0$), $\begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}$ represents $1^{st}$
excited state ($\ket1$) etc.

So, the quantum state of a k-level system is a unit vector (which means it's
size is equal to 1) in k-dimensional complex Gilbert space: $\vec u \in \cplx^k$

For qubits: $\ket\psi = \alpha_0 \ket0 + \alpha \ket1$ can be represented in two
- dimensional space as a vector of length 1 and angle $\theta$. Projections of
this vector are the probabilities of the system to be in the relevant state
after measurement:
\[ Pr[0] = \alpha_0^2 = \cos^2 \theta; Pr[1] = \alpha_1^2 = sin^2 \theta\]

\subsubsection{Measurement in an arbitrary basis}
\label{sec:2-3}

For given vector $\psi$ representing a state of a qubit we can use arbitrary
basis (let's call it u, it should consist of axis $\ket u$ and
$\ket{u^{\perp}}$) and do our measurements in this new basis u:
\begin{itemize}
\item $\ket u = \beta_0\ket0 + \beta_1\ket1$
\item $\ket {u^{\perp}} = -\beta_1\ket0 + \beta_0\ket1$
\end{itemize}

In this new basis, the bow angle will be $\theta'$ and probabilities for
outcomes of measurement will be:
\begin{itemize}
\item $u$ with probability $\cos^2 \theta' \leftrightarrow (\ket u, |\psi
  \rangle)^2 $
\item $u^{\perp}$ with probability $\sin^2 \theta' \leftrightarrow
  (|u^{\perp}\rangle, |\psi \rangle)^2$
\end{itemize}

\paragraph{Example } Hadamard basis (so - called ``sign basis''). A basis turned
$-\frac\pi4$:
\begin{itemize}
\item $\ket+ = \frac1{\sqrt{2}} \ket0 + \frac1{\sqrt{2}} \ket1$
\item $\ket- = \frac1{\sqrt{2}} \ket0 - \frac1{\sqrt{2}} \ket1$
\end{itemize}

Curiosity: can we distinguish $\ket+$ from $\ket-$ through a measurement in the
standard basis? The problem is that for state $\ket+$ probabilities for $\ket0$
and $\ket1$ are both $\frac1{2}$ - and so for $\ket-$. The idea is to used ``yet
another'' basis - namely, switch to sign basis in this case.

So measure (for example) $\ket\psi = \frac1{2} \ket0 + \frac{\sqrt{3}}{2}\ket1$
in $\ket+ / \ket-$ basis can be found by good old linear algebra:
\begin{enumerate}
\item Compute inner product:
  \[ Pr[+] = |(\ket\psi, \ket+)|^2 = \left|\left(\frac1{2}\ket0 +
      \frac{\sqrt{3}}{2}\ket1, \frac1{\sqrt{2}}\ket0 + \frac1{\sqrt{2}}\ket1
    \right)\right|^2 =\]
  \[ = \left| \frac1{2 \sqrt{2}} + \frac{\sqrt{3}}{2\sqrt{2}} \right|^2 =
  \frac{(1 + \sqrt{3})} {8} = \frac{4 + 2 \sqrt{3}}8 \]

  $ Pr[-] = |(\ket\psi, \ket-)|^2$, or simpler $\dots = 1 - Pr[+] = \frac{4 - 2
    \sqrt{3}}{8}$
\item Change of basis: rewrite state in new basis $\ket\psi = \beta_0 \ket+ +
  \beta_1 \ket-$ so that $Pr[+] = |\beta_0|^2, Pr[-] = |\beta_1|^2$. Then,
  accepting that
  \begin{itemize}
  \item $\ket+ = \frac1{\sqrt{2}} \ket0 + \frac1{\sqrt{2}} \ket1$
  \item $\ket- = \frac1{\sqrt{2}} \ket0 - \frac1{\sqrt{2}} \ket1$
  \end{itemize}
  put these into formula, after simplifications:
  \[ Pr[+] = | \frac1{2 \sqrt{2}} + \frac{\sqrt{3}}{2\sqrt{2}} |^2 = \frac{4 + 2
    \sqrt{3}}{8} \]
  \[ Pr[-] = | \frac1{2 \sqrt{2}} - \frac{\sqrt{3}}{2\sqrt{2}} |^2 = \frac{4 - 2
    \sqrt{3}}{8} \]
\end{enumerate}

\subsection{Uncertainty principle}
\label{sec:2-4}

\paragraph{Heisenberg}forever: One can never know with perfect accuracy both of
those two important factors which determine the movement of one of the smallest
particles - its position and its velocity.

Similar story for qubits (hydrogen atom) :
\begin{itemize}
\item it has energy (Bit) : $\ket0$ or $\ket1$
\item it has sign : $\ket+$ or $\ket-$
\end{itemize}

It is impossible to know both bit and sign of qubit, because if we have value
for, say, bit, this will provide maximum uncertainty for sign (0.5 - 0.5
probability) and vice verse: if we know sign (and thus fix electron in either
state) we get uncertainly for the any other basis.

Thus, these two basis are incompatible with each other (as well as any two
different basis).

To quantify this we define a {\bf spread} of a quantum state.
\[\ket\psi = \alpha_0 \ket0 + \alpha_1 \ket1 = \beta_0 \ket+ + \beta_1
\ket- \]
\begin{itemize}
\item The spread in standard basis $:= S(|\psi|) = |\alpha_0| + |\alpha_1|$
\item The spread in sign basis $:= \hat{S}(|\psi|) = |\beta_0| + |\beta_1|$
\end{itemize}
For fixed states:
\begin{itemize}
\item In standard basis:
  \begin{itemize}
  \item $S(\ket0) = 1 + 0 = 1$
  \item $S(\ket+) = \frac1{\sqrt{2}} + \frac1{\sqrt{2}} = \sqrt{2}$
  \end{itemize}
\item In sign basis:
  \begin{itemize}
  \item $\hat{S}(\ket0) = \frac1{\sqrt{2}} + \frac1{\sqrt{2}} = \sqrt{2}$
  \item $\hat{S}(\ket+) = 1 + 0 = 1$
  \end{itemize}
\end{itemize}

Uncertainty principle for bit and sign: $S(|\psi\rangle) \hat{S}(|\psi\rangle)
\geq \sqrt{2}$ for any $|\psi\rangle$

\section{Two Qubits \& Entanglement}
\label{sec:week 2}
Two qubit system is classically represent with two bits of information: $00, 01,
10, 11$, so quantum state is a superposition over all four classical
possibilities: $$|\psi \rangle = \alpha_{00}|00\rangle + \alpha_{01}|01\rangle +
\alpha_{10}|10\rangle + \alpha_{11}|11\rangle $$, and $$|\alpha_{00}|^2 +
|\alpha_{01}|^2 + |\alpha_{10}|^2 + |\alpha_{11}|^2 = 1$$
 
Observer j with probability $|\alpha_j|^2$. New state = $|j\rangle$

\subsubsection{Partial Measurement}
\label{sec:3-2}

What is the result of measurement {\bf just one} qubit? I.e. the probability to
see the first qubit as 0 is $$P[0] = |\alpha_{00}|^2 + |\alpha_{01}|^2$$

But what about the new state? Just discard the states which are not possible any
more, and re-normalise probabilities: $$New State = \frac{\alpha_{00}|00\rangle
  + \alpha_{01}|01\rangle }{\sqrt{|\alpha_{00}|^2 + \alpha_{01}|^2} } $$

\subsection{Entanglement}
\label{sec:3-3}

Imagine a composite system: qubit 1 with state $\alpha_{0}\ket0 +
\alpha_{1}\ket1$ and qubit 2 with state $\beta_{0}\ket0 + \beta_{1}\ket1$. The
composite state will be \[ (\alpha_{0}\ket0 + \alpha_{1}\ket1) (\beta_{0}\ket0
+\beta_{1}\ket1) = \alpha_0 \beta_0 |00\rangle + \alpha_0 \beta_1 |01\rangle +
\alpha_1\beta_0 |10\rangle + \alpha_1 \beta_1 |11\rangle \]

Now back: Given the state of the composite system, determine the state of each
qubit. In theory, we have 4 variables ($\alpha_0, \alpha_1, \beta_0, \beta_1$),
4 products - so we can calculate. The problem is - not all composite states are
possible to ``de-compose''; such ``en-decomposing'' states are called {\bf
  entangled}.

For such states, each qubit does not have its own separate state - it is not in
some definite state. All we can say is that the whose system is in some
superposition quantum state.

Canonical example - {\bf Bell State}
$$|\psi\rangle = \frac1{\sqrt{2}} |00\rangle + \frac1{\sqrt{2}}|11\rangle $$
- the formula implies that $\alpha_0, \beta_0, \alpha_1, \beta_1$ are non-zero
which contradicts the fact $\alpha_0 \beta_1 = 0; \alpha_1 \beta_0 = 0$.

\paragraph{Measuring the Bell State}

Measure first qubit:
\begin{itemize}
\item see 0 with probability $\frac1{2}$. New state = $|00\rangle$
\item see 1 with probability $\frac1{2}$. New state = $|11\rangle$
\end{itemize}
The trick is: let's separate qubits by a big distance. The measure first of them
- let's say, we get 0. Then measure the second one (we are expecting to get 0
with probability 1). As such, we have coincidence in particle states -
regardless of the distance between particles (something like if two particles
share a coin flip before they are separated).

\subsection{EPR Paradox}
\label{sec:3-4}

Einstein, Podolsky, Rosen (1935). Was introduced to show problems with QM
theory. The idea is: we can rewrite the state of a Bell State using the sign
basis:
$$|\psi\rangle = \frac1{\sqrt{2}} |00\rangle + \frac1{\sqrt{2}}|11\rangle =
\frac1{\sqrt{2}} |++\rangle + \frac1{\sqrt{2}}|--\rangle $$

Then, assuming the trick from previous paragraph:
\begin{itemize}
\item Faster then light communication?
\item If when two particles were together they flipped two coins to coordinate
  how they would answer on bit and sign measurement, then they violate
  uncertainty principle (measure first qubit in bit basis and second qubit in
  sign basis; know both bit and sign for first qubit contradicts uncertainty
  principle).
\end{itemize}
Variants for solution:
\paragraph{Local Hidden Variable Theory:} The two particles carry with them all
the information necessary to locally decide the outcome of any future
measurements.

- contradicted by Bell's experiment

\paragraph{Quantum Mechanics:} As soon as the first qubit is measured, say in
the bit basis, the entanglement between the two qubits is destroyed. The new
state is either $|00\rangle$ or $|11\rangle$, each of which is untangled.
Measuring the second qubit in the sign basis no longer reveals any information
about the first qubit. (Does not constitute faster than light communication,
since no information is communicated).


\section{Bell's Experiment}
\label{sec:4}
John Bell devised an experiment with one of two outcomes:
\begin{enumerate}
\item Nature is inconsistent with quantum mechanics but might be better
  explained by some local hidden variable theory
\item Nature is consistent with quantum mechanics but inconsistent with any
  local hidden variable theory
\end{enumerate}
The Bell experiment relies on properties of entangled qubits. Performed numerous
times, outcome is second (consistent with quantum mechanic).
 
\subsection{Experiment Setup}
\label{sec:4-1}
Two boxes situated far apart (light should not have enough time to travel
between during experiment => time of experiment should be short): first one gets
input x (0 or 1), second box gets input y (also 0 or 1). Both boxes are set up
to output a bit: a for first box, b for second.

Experiment: if both boxes are give 1 input, we expect both boxes to output the
different values ($a \neq b$). Otherwise, want output of the boxes to be the
same The idea is that
\begin{itemize}
\item if boxes are described by local hidden variables, the experiment should be
  successful with probability $\leq \frac{3}{4} = 0.75$

  Proof: Fail on any of the 4 inputs implies success probability $\leq
  \frac{3}{4}$ Suppose output on x=y=0 is a=b=0. By locality, output on x=0, y=1
  and x=1, y=0 must also be a=b=0.

  Finally by locality, output on x=y=1 must also be a=b=0 => contradiction.

\item But, if boxes share a Bell state, then experiment can succeed with
  probability as high as 0.85. See next section for proof.
\end{itemize}

\subsection{Properties of Bell state}
\label{sec:4-2}

\subsubsection{Rotational in-variance of Bell State}
\label{sec:4-2-1}

The idea is to measure state of both qubits in some basis $(u, u^{\perp})$.
Initially, the probabilities are the same: $P[u] = \frac1{2}, P[u^{\perp}] =
\frac1{2}$. But, if measured state of qubit 1 is u, then probability of qubit 2
to be in state u is P[u] = 1.

Now add yet one more basis: $(v, v^{\perp})$ - rotated from $(u, u^{\perp})$ by
angle $\theta$. If qubit 1 is in state $u$, the probability for qubit 2 to be in
state $v$ is $\cos^2 \theta$.

Proof: The state of a qubit can be expressed as: $$|\psi\rangle =
\frac1{\sqrt{2}} |00\rangle + \frac1{\sqrt{2}}|11\rangle = \frac1{\sqrt{2}}
\ket{uu} rangle + \frac1{\sqrt{2}}|u^{\perp}u^{\perp}\rangle$$ - through $\ket u = a
\ket0 + b \ket1;|u^{\perp}\rangle = -b \ket0 + a \ket1; a, b \in R; a^2 + b^2 =
1$ and re-calculating

For complex a and b, use different form of Bell's state: $$ |\psi\rangle =
\frac1{\sqrt{2}} | 01 \rangle - \frac1{\sqrt{2}} | 10 \rangle $$, then $$\ket u
= \alpha \ket0 + \beta \ket1; |u^{\perp}\rangle = -\beta^* \ket0 + \alpha^*
\ket1;$$ $ \alpha, \beta \in C; |\alpha|^2 + |\beta|^2 = 1$ - complex
conjugates.

\subsubsection{Bell's Experiment}
\label{sec:4-3}

In box 1, we measure the state of a qubit in a basis depending on the input
value (select basis U if input is 1 and some other if 0). In box 2 - select yet
another basis V if input is 1 and other is input is 0).

If y = 1 (second box is given 1), then P[matching output] = $cos^2 \theta$ where
$\theta$ is the angle between basis U and V (see previous subsection).

The trick is to select relevant basis's in a manner to make such outcome
maximally probable. One of the variants is: Box 1: basis $U_0$ (to be used if
x=0) is the ``normal'' basis; basis $U_1$ will be turned $\pi / 4$

Box 2: Basis $V_0$ (for y = 0): turned by $\pi /8$; basis $V_1$ (for y =1)
turned by $ - \pi / 8$

So, the probabilities will be:
\begin{tabular}{cc|c}
  \textbf{ x=} & \textbf{ y = } & \textbf{P(match)} \\
  \hline
  0 & 0  & $cos^2 \pi/8$ \\
  0 & 1  & $cos^2 \pi/8$ \\
  1 & 0  & $cos^2 \pi/8$ \\
  1 & 1  & $cos^2 3\pi/8 = sin^2 \pi/8$ \\
\end{tabular}

In last case, P(not match) = $cos^2 \pi / 8$; so for the whole experiment
$$P[success] = cos^2 \pi / 8 \approx 0.85$$

Returning to original formulations:
\begin{itemize}
\item Success probability $\leq 3/4 \Rightarrow$ Nature is inconsistent with
  quantum mechanics but might be better explained by some local hidden variable
  theory
\item Success probability $> 3/4 \Rightarrow$ Nature is consistent with quantum
  mechanics and inconsistent with any local hidden variable theory
\end{itemize}
The Bell experiment has been performed numerous time; the result have always
been consistent with quantum mechanics. Einstein wasted near 20 years of hos
life.

\subsubsection{Certifying Randomness}
\label{sec:5-1}

{\bf Task:} Construct a physical random generator whose output can be certified
to be random. The Bell experiment is used to certify the ``true randomness'' of
the outputs.

The idea is to check if Bell's experiment is successful with probability at
least 0.85. Details - see papers mentioned at (5-1 10:10)

\subsection{Evolution of a Qubit}
\label{sec:5-2}

Allowable states of k-level system: unit vector in a k-dimensional complex
vector space (called a Hilbert space):

\[\ket\psi = \alpha_0\ket0 + \dots + \alpha_{k-1}\ket{k-1} =
\begin{pmatrix} \alpha_0 \\ \vdots \\ \alpha_{k-1} \end{pmatrix} \in \cplx^k\]


\subsection{Axioms of Quantum Mechanics}
\label{sec:5-2}
... re-formulation
\begin{enumerate}
\item {\bf Superposition principle: } allowable states of k-level system: unit
  vector in a k-dimensional complex vector space (called a Hilbert space)

\item {\bf Measurement}

  \begin{itemize}
  \item A measurement is specified by choosing an orthonormal basis.
  \item The probability of each outcome is the square of the length of the
    projection onto the corresponding basis vector.
  \item The state collapses to the observed basis vector
  \item If $\ket{u_0}, \ket{u_1}, \dots, \ket{u_{k-1}}$ was the chosen basis and
    $\ket\psi = \alpha \ket{u_0} + \alpha \ket{u_1} + \dots + \alpha_{k-1} |
    u_{k-1}\rangle$, then qubit is in the state $u_i$ with probability
    $|\alpha_i|^2 = |(|\psi \rangle,|u_i\rangle)|^2$. New state is
    $|u_i\rangle$.
  \end{itemize}


\item {\bf Unitary evolution}

  Quantum systems evolve by rotating of the Hilbert space (so-called rigid body
  rotation, meaning that the angles between vectors are preserved). As rotation
  of the space is a linear transformation, it can be represented by a matrix.

  Example: turning basis by angle $\theta$. Coordinate vectors: $\ket0 \to \cos
  \theta \ket0 + \sin \theta \ket1;\ket0 \to - \sin \theta \ket0 + \cos \theta
  \ket1$, so

\[\bigR_\theta = \left( \begin{array}{cc}\cos\theta &-\sin\theta \\ \sin
    \theta & \cos\theta\end{array}  \right)\]

Later on, \[ \bigR_{- \theta} = \left( \begin{array}{cc} \cos \theta & \sin
    \theta \\ -\sin \theta & \cos \theta \end{array} \right) = \bigR_\theta^T \]
and \[ \bigR_\theta \bigR_{-\theta} = \mathbf{I}; \bigR_\theta \bigR_{\theta}^T
= \bigR_\theta^T \bigR_{\theta} = \mathbf{I} \]
\end{enumerate}

\subsection{Unitary Transformation}
\label{sec:5-3}

Unitary transformation is a rotation of a qubit state vector in a complex space.
The rotation matrix is: $U = \begin{pmatrix} a & c \\ b & d \end{pmatrix}; a, b,
c, d \in \cplx$.

If our state is represented be real (not complex) parameters, then
transformation can be expressed as rotation by angle $\theta$: $R_\theta =
\begin{pmatrix} \cos \theta & -\sin\theta \\ \sin\theta &
  \cos\theta \end{pmatrix}$

Examples:
\begin{itemize}
\item $\ket\psi \to U\ket\psi$
\item for $\ket0$ vector: $ \dbinom{a}{b} = \ket0 \to \dbinom ab = a\ket0 + b
  \ket1$
\item for $\ket1$ vector: $ \dbinom{c}{d} =\ket1 \to \dbinom{c}{d} = c\ket0 + d
  \ket1$
\end{itemize}

By definition, a {\bf conjugate transpose} matrix $U^\dag$ is obtained from $U$
by taking the transpose and then taking the {\it complex conjugate} (that is,
negating their imaginary parts bu not their real parts) of each entry. In
addition, if for given matrix $A$ it's inverse matrix $A^{-1} = A^\dag$, that is
$A \times A^\dag = I$, then matrix $A$ is called {\bf unitary matrix}.

The main idea is that angles are preserved, and orthogonal vectors remains
orthogonal: $|a|^2 + |b|^2 = 1 = |c|^2 + |d|^2$, and $a^*c + b^*d = 0$

Another property - {\bf Linearity}, i.e. if
\begin{gather*}
  U\ket0 = \ket{\phi_0} = a\ket0 + b \ket1 \\
  U\ket1 = \ket{\phi_1} = c\ket0 + d \ket1 \end{gather*} then
\[\begin{split}
  U(\alpha \ket0 + \beta \ket1) &= \alpha\ket{\phi_0} + \beta\ket{\phi_1} \\ &=
  \alpha (a\ket0 + b \ket1) + \beta (c\ket0 + d\ket1) \\ &= (\alpha a + \beta b)
  \ket0 + (\alpha b + \beta d) \ket1 \end{split}\]

\subsubsection{Single Qubit Gates}
\label{sec:5-4}

\paragraph{Quantum gates} transforms qubits passed to them. Examples:
\begin{itemize}
\item ``Bit flip'': $\alpha_0 \ket0 + \alpha_1 \ket1 \to \alpha_1 \ket0 +
  \alpha_0 \ket1 $ Transformation matrix:
  \begin{gather*}
    \mathbf{X} = \left( \begin{array}{cc} 0 & 1 \\ 1 & 0 \end{array} \right);
    \mathbf{X} \ket0 = \ket1; \mathbf{X} \ket1 = \ket0; \\
    \mbox{ and } \mathbf{X}^T = \left( \begin{array}{cc} 0 & 1 \\ 1 &
        0 \end{array} \right) = \mathbf{X}; \mathbf{X}^2 = \mathbf(I)
  \end{gather*}

  Can be considered as a reflection (rotation) of vector over axis tilted at
  45\textdegree.

\item ``Phase flip'': $\alpha_0 \ket0 + \alpha_1 \ket1 \to \alpha_0 \ket0 -
  \alpha_1 \ket1 $ Transformation matrix:
  \begin{gather*}
    \mathbf{Z} = \left( \begin{array}{cc} 1 & 0 \\ 0 & -1 \end{array} \right);
    \mathbf{Z} \ket0 = \ket0; \mathbf{Z} \ket1 = - \ket1; \\
    \mbox{ again } \mathbf{Z}^T = \mathbf{Z}; \mathbf{Z}^2 = \mathbf(I)
  \end{gather*}

  Reflection over $\ket0$ axis. BTW, \begin{gather*} \mathbf{Z} \ket+ = \ket-;
    \\ \mathbf{Z} \ket- = \ket+
  \end{gather*} - Phase flip swaps the axis of the sign basis.

\item ``Hadamard transform''

  \begin{gather*}
    \mathbf{H} = \left( \begin{array}{cc} \frac1{\sqrt{2}} & \frac1{\sqrt{2}} \\
        \frac1{\sqrt{2}} & - \frac1{\sqrt{2}}
      \end{array} \right);\\
    \mathbf{H} \ket0 = \ket+ = \frac1{\sqrt{2}} \ket0 +
    \frac1{\sqrt{2}} \ket1 \\
    \mathbf{H} \ket1 = \ket- = \frac1{\sqrt{2}} \ket0 -
    \frac1{\sqrt{2}} \ket1 \\
    \mbox{ again }  \mathbf{H}^T = \mathbf{H}; \mathbf{H}^2 = \mathbf(I) \\
    \mathbf{H} \ket+ = \ket0; \mathbf{H} \ket- = \ket1\\
  \end{gather*}

  Reflection over $\pi / 8 $ axis.
\end{itemize}

Now we have a bit of tricks: if we do not have a Z gate, we can emulate one with
the chain $\mathbf{H} \to \mathbf{X} \to \mathbf{H}$, in other words,
$\mathbf{Z} = \mathbf{H} \mathbf{X} \mathbf{H}$. Big picture:

$$\begin{array}{ccccc}
  &\ket0 & \stackrel{X}{---} & \ket1 & \\
  & | & & | &  \\
  H & | & & | & H \\
  & | & & | & \\
  &\ket+ &  \stackrel{Z}{---} & \ket- & \\
\end{array}$$

\subsubsection{No cloning theorem}
\label{sec:5-5}

It is impossible to create a qubit with exactly the same state as some arbitrary
state.

\subsubsection{Two qubit gate}
\label{sec:5-5-1}

Transformation U transforms the two=qubit state $|\psi \rangle$ into another
two-qubit state $|\phi\rangle$. Such transformation is set by 4 x 4 matrix (U)
that $\mathbf{U} |\psi_1 \rangle = |\phi_1 \rangle; \mathbf{U} | \psi_2 \rangle
= | \phi_2 \rangle$.

Then, if $|\psi \rangle = |\psi_1 \rangle + | \psi_2 \rangle$, then
$\mathbf{U}|\psi \rangle = |\phi_1 \rangle + |\phi_2 \rangle$

Now try to build a gate (circuit ?) to copy a quantum bit:
\subsubsection{Quantum circuit for copying a quantum bit}
\label{sec:5-5-2}

Input: unknown state $|\psi\rangle = a \ket0 + b \ket1$. Put this and ``clean
qubit'' $\ket0$, expected output should be: \begin{equation}(a \ket0 + b
  \ket1)(a \ket0 + b \ket1) = a^2 |00\rangle + ab |01 \rangle + ab |10 \rangle +
  b^2|11\rangle \label{copy-eq} \end{equation}

Cases:
\begin{itemize}
\item $|\psi \rangle = \ket0$, expected output $|00\rangle$
\item $|\psi \rangle = \ket1$, expected output $|11\rangle$
\item Then, by linearity, if input is $|\psi \rangle = a \ket0 + b \ket1$ then
  output should be $a |00\rangle + b |11\rangle$
\end{itemize}
The last must be equal to (\ref{copy-eq}), so it must be ab=0 which is possible
only if a=0 or b=0.

So, {\bf the fact that you can clone $\ket0 \mbox{ or } \ket1$ prevents you from
  copying the ``normal'' state } or

\paragraph{No cloning theorem} It is impossible to clone an unknown quantum
state.

Or, even stronger, it is not possible to construct a two qubit of the same
states from the one qubit in this state and one arbitrary one.

\section{Quantum circuits and teleportation}
\label{sec:6}

\subsection{CNOT gate + circuits}
\label{sec:6-1,2}

One-qubit gate U can be specified either by matrix U or by
\begin{gather*} \ket0 \to \dots \\ \ket1 \to \dots \end{gather*} - because it is
a linear transformation.

For two-qubit gates: $$\alpha_{00} |00\rangle + \alpha_{01} |01\rangle +
\alpha_{[10} |10\rangle + \alpha_{11} |11\rangle \to \alpha_{00}' |00\rangle +
\alpha_{01}' |01\rangle + \alpha_{10}' |10\rangle + \alpha_{11}' |11\rangle$$

So that U is the 4-dimensional unitary ($UU^\dag = U^\dag U = I$) matrix.

\subsubsection{CNOT gate}
\label{sec:6-1-1}

The first bit (a AKA control) goes unchanged: $a \to a$; the second one (b AKA
Target bit) is being flipped if a is 1 and NOT flipped otherwise: $b \to b
\oplus a$.

So: $\begin{array}{ccc}
  |00\rangle & \to & |00\rangle \\
  |01\rangle & \to & |01\rangle \\
  |10\rangle & \to & |11\rangle \\
  |11\rangle & \to & |10\rangle \\
\end{array}$, and superposition is being mapped as: $\begin{array}{c}
  \alpha_{00} |00\rangle + \alpha_{01} |01\rangle + \alpha_{[10} |10\rangle +
  \alpha_{11} |11\rangle \\
  \downarrow \mbox{ CNOT } \\
  \alpha_{00} |00\rangle + \alpha_{01} |01\rangle + \alpha_{11} |10\rangle +
  \alpha_{10} |11\rangle 
\end{array}$

The matrix looks even more interesting: $\mathbf{CNOT} = \left(
  \begin{array}{cccc}
    1 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 1 & 0
  \end{array}
\right)$,

and $\mbox{CNOT CNOT}^\dag = I$

\subsubsection{Quantum circuit}
\label{sec:6-1-2}

Let's play with Hadamard gate $\binom{H\ket0=\ket+=\frac1{\sqrt2}\ket0 +
  \frac1{\sqrt2}\ket1}{H\ket1=\ket-=\frac1{\sqrt2}\ket0 - \frac1{\sqrt2}\ket1}$
on qubit 1 followed by CNOT on both. Transformations:
\begin{equation*} \begin{split} \ket{00} \to \mbox{Step 1: } & (\frac1{\sqrt{2}}
    \ket0 + \frac1{\sqrt{2}} \ket1 ) ( \ket0 ) =
    \frac1{\sqrt{2}} \ket{00} + \frac1{\sqrt{2}} \ket{10} \\
    \to \mbox{Step 2: } & \mbox{flip $2^{nd}$ qubit when the $1^{st}$ is 1: getting Bell state: }\\
    \to &\frac1{\sqrt{2}}(\ket{00} + \ket{11}) \\
  \end{split}\end{equation*}

By playing the input states, we can get all four Bell states:
\begin{align*}
  \ket{00} \to \ket{\phi^+} = \frac1{\sqrt{2}} (\ket{00} + \ket{11})\\
  \ket{10} \to \ket{\phi^-} = \frac1{\sqrt{2}} (\ket{00} - \ket{11})\\
  \ket{01} \to \ket{\psi^+} = \frac1{\sqrt{2}} (\ket{01} + \ket{10})\\
  \ket{11} \to \ket{\psi^-} = \frac1{\sqrt{2}} (\ket{01} - \ket{10})
\end{align*}
- so-called Bell basis states

As Hadamard and CNOT are their own inversions, the same input can go ``from
right to left'': feed the circuit with, say, $\ket{\phi^+}$ and get $\ket{00}$
at the left. So it is possible to restore the initial state - measure qubits in
bit basis after passing it through ``CNOT -> Hadamard'' circuit.

\subsection{Teleportation}
\label{sec:6-3}

It is impossible to clone quantum information but it is possible to teleport a
quantum state to another location.

Settings: Alice has a qubit is state $\ket\psi = \alpha\ket0 + \beta\ket1$,which
she wants to transfer. In addition, Alice and Bob create a pair in Bell state:
$\ket\psi = \frac1{\sqrt{2}} \ket{00} + \frac1{\sqrt{2}}\ket{11}$

Then Alice measures both her qubit $\psi$ and her qubit from the Bell pair with
outcome, of two bits: $b_1, b_2$ - one of (00, 01, 10, 11). This iformation is
passed to Bob.

Bob performs some (depending on $b_1, b_2$) unitary transformation on his part
of Bell state - in such a way that outcome is $\alpha\ket0 + \beta\ket1$ - so
the qubit destroyed by Anna during her measurement ``suddenly'' re-appears on
the Bob's end. So we have a ``teleportation'' of quantum state.

% point 6-3 08:26

\subsubsection{Mechanics: Assume CNOT}
\label{sec:6-3-1}

Imagine we can create an entanglement pair out from Alice's $\alpha\ket0 +
\beta\ket1$ and Bob's ``pure'' $\ket0$, and put it through CNOT gate. The
outcome will be $\alpha\ket{00} + \beta\ket{11}$.

Then Alice measures her qubit {\it in sign basis}. The outcome for the pair will
be:
\[\begin{split}
  \alpha \ket{00} + \beta \ket{11}) &= \alpha\left(\frac1{\sqrt{2}}\ket+ +
    \frac1{\sqrt{2}}\ket-\right)(\ket0) +
  \beta\left(\frac1{\sqrt{2}}\ket+ - \frac1{\sqrt{2}}\ket-\right)(\ket0)  \\
  &= \frac1{\sqrt{2}}\ket+(\alpha\ket0 + \beta\ket1) +
  \frac1{\sqrt{2}}\ket-(\alpha\ket0 - \beta\ket1)
\end{split}\] So:
\begin{itemize}
\item if Alice measured $+$, Bob gets $\alpha\ket0 + \beta\ket1$ - job done
\item if Alice measured $-$, Bob gets $\alpha\ket0 - \beta\ket1$ - need to apply
  the ``phase flip'' Z gate.
\end{itemize}
- now we have a challenge to create the entangled state $\alpha\ket{00} +
\beta\ket{11}$ without quantum communication between Alice and Bob.

\subsubsection{Mechanics: Implementation}
\label{sec:6-4}

So task is to create a $\alpha\ket{00} + \beta\ket{11}$ state. Suppose we have
two qubits sharing a Bell state. Can we use it to effectively apply CNOT
remotely?
\begin{itemize}
\item {\bf Step 1 (initial):} Alice has her source (or ``unknown'') qubit
  $\alpha\ket0 + \beta\ket1$; Alice and Bob share the Bell state
  $\left(\frac1{\sqrt{2}}\ket{00} + \frac1{\sqrt{2}}\ket{11}\right)$. Common
  state: \begin{gather*}(\alpha\ket0 +
    \beta\ket1)\left(\frac1{\sqrt{2}}\ket{00} + \frac1{\sqrt{2}}\ket{11}\right) = \\
    = \frac\alpha{\sqrt{2}}\ket{000} + \frac\alpha{\sqrt{2}}\ket{011}
    +\frac\beta{\sqrt{2}}\ket{100} + \frac\beta{\sqrt{2}}\ket{111}
  \end{gather*}

\item {\bf Step 2: } Alice applies CNOT gate to her qubits (from qubit 1 to
  qubit 2): in states when qubit 1 is 0, nothing happens, when qubit 1 is 1, the
  qubit 2 flips: $$\frac\alpha{\sqrt{2}}\ket{000} +
  \frac\alpha{\sqrt{2}}\ket{011} +\frac\beta{\sqrt{2}}\ket{110} +
  \frac\beta{\sqrt{2}}\ket{101}$$

\item {\bf Step 3:} Alice measures qubit 2. Possible state outcomes:
  \begin{itemize}
  \item $0 \to \alpha\ket{00} + \beta\ket{11}$ (selected 1st and 4th parts of
    Step 2). {\bf Done:} Alice and Bob share two qubits with the same state
    $\alpha\ket0 + \beta\ket1$.
  \item $1 \to \alpha\ket{01} + \beta\ket{10}$ (selected 2nd and 3rd parts of
    Step 2). Then Bob performs a Bit flip ($X$) on his qubit, and we get
    $\alpha\ket{00} + \beta\ket{11}$ again.
  \end{itemize}

\item Alice performs measurement on unknown qubit (qubit 1) in Hadamard basis
  (i.e. applies $H$, then measures), reports outcome to Bob. If outcome is $+$,
  Bob does nothing, if it is $-$ - Bob applies ``phase flip'' - $Z$ to his
  qubit.
\end{itemize}
For the diagram of the whole protocol - see slide 6-4, page 6. Result: both
Alice's qubits are measured (``destroyed''), but her unknown state qubit is
``materialised'' on the Bob side.

\subsection{Computation by Teleportation}
\label{sec:6-5}

Order of application: Apply $ZX$ means apply first $X$, then $Z$!

I general case, it would not work to apply some arbitrary gate at the Bob's side
before teleportation - because at most cases $U \times X \neq X \times U$ (see
slides 6-5, notice U-gate at the Bob's side). For details - see
http://arxiv.org/pdf/quant-ph/9908010v1.pdf)

\section{Bra-ket notation, Eigenvectors, Tensor products}
\label{chap:7}

Euler's formula: $e^{ix} = \cos x + i \sin x$

 

Ket notation:$$\ket\psi = \begin{pmatrix} \alpha_0 \\\alpha_1 \\ \vdots \\
  \alpha_{k-1} \end{pmatrix} = \alpha_0\ket0 + \alpha_1\ket1 + \dots +
\alpha_{k-1}\ket{k-1}$$

Corresponding to this vector (called ``ket'') there exists ``bra'' vector:
$$(\alpha_0^* \enskip \alpha_1^* \enskip \dots \enskip \alpha_{k-1}^*) =
\bra\psi $$, where $\alpha_i^*$ - complex conjugates for $\alpha_i$.

Suppose we have another vector state $\ket\phi = \beta_0\ket0 + \beta_1\ket1 +
\dots + \beta_{k-1}\ket{k-1}$. The inner product of $\ket\psi$ and $\ket\phi$
will be: $$(\alpha_0^* \enskip \alpha_2^* \enskip \dots \enskip \alpha_{k-1}^*)
\times \begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_{k-1} \end{pmatrix}
= \sum_j\alpha_J^* \beta_J^* = \braket\psi\phi $$

The length of a vector: $|\ket\psi| = \sqrt{ \braket\psi\psi} = \sqrt{\sum
  \alpha_j^* \alpha_j} = \sqrt{\vphantom{\sum\alpha_j^*} \sum|\alpha_j|^2}$

Another story: inner product is equal to the $\cos\theta$ between unit vectors
in complex space: $\cos \theta = \frac{|\braket\psi\phi|}{|\ket\psi||\ket\phi|};
0\leq \theta \leq \pi/2$
% video 7-2

\paragraph{Projection Matrix} P onto $\ket\phi$ - i.e. the matrix which for
given vector $\ket\psi$ provides a projection of this vector to
$\ket\phi= \begin{pmatrix}\beta_0 \\ \vdots \\ \beta_{k-1} \end{pmatrix}$
(vector!)

Such projection matrix will be $P=\begin{pmatrix} \beta_0 \\ \vdots \\
  \beta_{k-1}\end{pmatrix} (\beta_0^* \dots \beta_{k-1}^*)$ or, in bra-ket
notation, $$P=\ket\phi \bra\phi$$

Then, when applied to vector $\ket\psi = (\ket\phi\bra\phi)\ket\psi$. Further
down: $\ket\psi = (\ket\phi\bra\phi)\ket\psi = \ket\phi \times
\underbrace{\bra\phi\ket\psi}_{\text{inner product}} = \braket\phi\psi \ket\phi$
- i.e. the vector $\ket\phi$ scaled by the inner product.

Another story: $P^2 = P$ (projection of already projected vector equals to this
projection). In out notation: $P^2 = \ket\phi \bra\phi \ket\phi \bra\phi$,
assuming that middle part is inner product of the unit vector, and so equals to
1: $\ldots = \ket\phi\bra\phi = P$

\paragraph{U unitary} - by definition $$U U^\dag = U^\dag U = I$$

and rows are orthonormal (as are the columns): if $U_{.j}$ = $j^{th}$ column of
U then the vectors represented by these columns/rows are orthogonal in space of
measurement k = ``U preserves angles'' (or, rather, inner products): if
$U\ket\phi = \ket{\phi'}, U\ket\psi = \ket{\psi'}$ then
$\braket\phi\psi=\braket{\phi'}{\psi'}$

Proof: if $U\ket\phi = \ket{\phi'}, U\ket\psi = \ket{\psi'}$ then $\bra{\phi'}
=\bra\phi U^\dag$ then $\braket{\phi'}{\psi'} = \braket{\bra\phi U^\dag}{U
  \ket\psi}$. With $U U^\dag = I$ - done.

\subsection{Hermitian Matrices, eigenvectors}
\label{sec:7-3}

\paragraph{Def} Matrix A is {\bf Hermitian} $\Leftrightarrow A=A^\dag$. It has
to be square, entries on the diagonal has to be real, and entries not on the
diagonal has to be conjugates relatively to diagonal.

If A is Hermitian and Real, it is symmetric.

\paragraph{Def} Vector $\ket\phi$ is {\bf eigenvector} of matrix A if $A\ket\phi
= \lambda\ket\phi (\lambda \in \cplx)$

\paragraph{Spectral Theorem: } if A is Hermitian $\Rightarrow$ A has an
orthonormal set of eigenvectors ($\ket{\phi_0}, \dots, \ket{\phi_{k-1}}$) with
real evalues ($\lambda_0, \dots, \lambda_{k-1}$).

I.e. matrix A maps unit sphere to ellipsoid of k principle axis.
% video 7-3,time 05:40

For example, for {\bf X operator}: $X = \begin{pmatrix} 0 & 1 \\ 1&
  0 \end{pmatrix}$; obviously Hermitian. $\ket+,\ket-$ are eigenvectors for it:
\begin{itemize}
\item $X\ket+ = \begin{pmatrix} 0 & 1 \\ 1 &
    0 \end{pmatrix}\binom{\frac1{\sqrt2}}{\frac1{\sqrt2}} =
  \binom{\frac1{\sqrt2}}{\frac1{\sqrt2}}; \; \lambda_+ = 1 $
\item $X\ket- = \begin{pmatrix} 0 & 1 \\ 1 &
    0 \end{pmatrix}\binom{\frac1{\sqrt2}}{-\frac1{\sqrt2}} =
  \binom{-\frac1{\sqrt2}}{\frac1{\sqrt2}}; \; \lambda_- = -1 $
\end{itemize}

Another example: {\bf Hadamard transform}: $H = \begin{pmatrix} \frac1{\sqrt2} &
  \frac1{\sqrt2} \\ \frac1{\sqrt2}& -\frac1{\sqrt2} \end{pmatrix}$;

For arbitrary vector $\ket\phi$ that of $H \ket\phi = \lambda\ket\phi$. Than we
have: $$(H-\lambda I)\ket\phi = 0 \text{ i.e. determinant } \det(H-\lambda I) =
0$$
Then $$\det \begin{pmatrix} \frac1{\sqrt2} - \lambda & \frac1{\sqrt2} \\
  \frac1{\sqrt2} & -\frac1{\sqrt2} - \lambda \end{pmatrix} =
\left(\frac1{\sqrt2} - \lambda \right)\left( -\frac1{\sqrt2} - \lambda \right) -
\frac1{\sqrt2} \frac1{\sqrt2} = -\frac12 + \lambda^2 -\frac12 = 0$$

So $\lambda = \pm 1 $; we can select any of these value and find out the
relevant eigenvector.
% video 7-3, 11:30

So Hermitian matrix A has orthonormal set of eigenvectors $\ket{\phi_0} \dots
\ket{\phi_{k-1}}$ with real evalues $\lambda_0 \dots \lambda_{k-1}$.

If we'll write our matrix A in the $\ket{\phi_0} \dots \ket{\phi_{k-1}}$ basis,
it will be just the diagonal matrix $\Lambda = \begin{pmatrix} \lambda_0 &&&0 \\
  & \lambda_1 && \\ & & \ddots & \\ 0 &&& \lambda_{k-1} \end{pmatrix}$. Then $A
= U^\dag \Lambda U$ where $U= \begin{pmatrix} \vdots & \vdots &&\vdots \\
  \ket{\phi_0} & \ket{\phi_1} & \dots & \ket{\phi_{k-1}} \\ \vdots & \vdots
  &&\vdots \end{pmatrix}$. Then (remember, $\ket\phi \bra\phi = P$ - projection
of the vector on itself!): $A = \lambda_0 \ket{\phi_0}\bra{\phi_0} + \lambda_1
\ket{\phi_1}\bra{\phi_1} + \dots + \lambda_{k-1}
\ket{\phi_{k-1}}\bra{\phi_{k-1}} = \sum \limits_{j=0}^{k-1}\lambda_j P_j$

\subsection{Tensor Products}
\label{sec:7-4}

\paragraph{Tensor Product} is an operation of ``forming'' a Hilbert space of
possible states for two particles ($\ket{00}, \ket{01}, \ket{10}, \ket{11}$, 4 -
dimension space) out of two spaces for the states of two separate particles. So
if we have Hilbert spaces $\mathbf{H}_1 \in \cplx^2$ and $\mathbf{H}_2 \in
\cplx^2$ then ``common'' space is $\mathbf{H} = \mathbf{H}_1 \otimes
\mathbf{H}_2$; so the state vectors become: $\ket0 \otimes \ket0 = \ket{00} =
\ket0\ket0; \ket0 \otimes \ket1 = \ket{01} = \ket0\ket1$ etc.

So the state vectors in the relevant spaces are ``product-ed'' too (math
expressions are boring obvious).

Consider two vectors: $\ket{\phi_1} \otimes \ket{\phi_2}$ and $\ket{\psi_1}
\otimes \ket{\psi_2}$. Inner product of these will
be: $$(\braket{\phi_1}{\psi_1}) (\braket{\phi_2}{\psi_2})$$

To describe a state in the first system, you need k complex parameters; and l
complex parameters to describe in second.

Let $H_1 = \mathbf{C^k}$ and $H_2 = \mathbf{C^l}$. The tensor product will be a
space $H = H_1 \otimes H_2 \in \cplx^{k*l}$. To describe a state in the first
system, you need k complex parameters; and l complex parameters to describe in
second. The whole state will be a vector consisting of:
$$\begin{pmatrix}
  \ket0 \otimes \ket0 & \ket0 \otimes \ket1& \dots& \ket0 \otimes \ket{l-1} \\
  \dots &\dots & \dots & \dots \\
  \ket{k-1}\otimes \ket0 & \ket{k-1} \otimes \ket1 & \dots & \ket{k-1} \otimes
  \ket{l-1}
\end{pmatrix}$$

\subsection{Tensor Product of Operators (gates)}
\label{sec:7-5}

Imagine we apply $U= \begin{pmatrix} a & c \\ b & d \end{pmatrix}$ and
$V= \begin{pmatrix} e & g \\ f & h \end{pmatrix}$ to two vectors from different
spaces: $$U \otimes V = \begin{pmatrix} aV & cV \\ bV & dV \end{pmatrix} =
\begin{pmatrix} \left[ aV = \begin{pmatrix} ae & ag \\ af & ah \end{pmatrix}
  \right] & cV \\ bV & dV \end{pmatrix}$$

Example: U = H (Hadamard); V = X (bit flip). Then
$$H \otimes V =\begin{pmatrix} \frac1{\sqrt2}X & \frac1{\sqrt2}X \\
  \frac1{\sqrt2}X & -\frac1{\sqrt2}X \end{pmatrix} =
\begin{pmatrix}
  0 & \frac1{\sqrt2} & \; & 0 & \frac1{\sqrt2} \\
  \frac1{\sqrt2} & 0 & \; & \frac1{\sqrt2} & 0 \\
  \; \\
  0 & \frac1{\sqrt2} & \; & 0 & -\frac1{\sqrt2} \\
  \frac1{\sqrt2} & 0 & \; & -\frac1{\sqrt2} & 0 \\
\end{pmatrix}$$

\section{Observables and Schrodinger's equation}
\label{sec:8}

\subsection{Observables}
\label{sec:7-1}

An {\bf observable } is a quantity like energy, position, momentum - something
which can be measured. We feed the quantum state to some measurement device and
get some real number

For some k-dimension system the observable A will be a $k \times k$ Hermitian
matrix: $A = A^\dag$. This means that, according to the Spectral Theorem, $A$
has k orthogonal vectors $\ket{\phi_0}, \ket{\phi_1} \dots \ket{\phi_{k-1}}$
with real eigenvalues $\lambda_0, \lambda_1 \dots, \lambda_{k-1}$ that
$$A \ket{\phi_i} = \lambda_i \ket{\phi_i}$$

\paragraph{Measurement}

Let $\ket\psi = \sum \alpha_i\ket{\phi_i}$. Measurement outcome is $\lambda_i$
with probability $|\alpha_i|^2$; new state $\ket{\psi_{new}} = \ket{\phi_i}$.

I.e. specifying matrix $A$ we really specify orthonormal basis with
corresponding eigenvalues. So, if our measurement results in $i$, the outcome
will be equal to $\lambda_i$

{\bf Example} let $\ket\psi = \alpha \ket0 + \beta\ket1$; Observable $X
= \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}$. Eigenvectors of $X$ are
\begin{gather*} \ket{\phi_0} = \ket+; \; \lambda_0 = 1 \\ \ket{\phi_1} = \ket-;
  \; \lambda_1 = -1 \end{gather*}

When we measure, we get the value in the matrix's basis: $$\ket\psi =
\frac{\alpha + \beta}{\sqrt2} \ket+ + \frac{\alpha-\beta}{\sqrt2} \ket-$$

Outcome:
\begin{itemize}
\item +1 with probability $\left| \frac{\alpha + \beta}{\sqrt2}\right| ^2$; new
  state $\ket+$
\item -1 with probability $\left| \frac{\alpha - \beta}{\sqrt2}\right| ^2$; new
  state $\ket-$
\end{itemize}

So expected value is $= 1 \left| \frac{\alpha + \beta}{\sqrt2}\right| ^2 +
(-1)\left| \frac{\alpha - \beta}{\sqrt2}\right| ^2$

For the situation with {\bf repeated eigenvectors} (i.e. we have $\ket{\phi_1}$
and $\ket{\phi_2}$ with both $\lambda_1 = \lambda_2 = 1$). In this case it is
impossible to find out which of the state was measured: any other vector
$\ket\psi = \alpha\ket{\phi_1} + \beta\ket{\phi_2}$ will also be eigenvector
with $\lambda = 1$. Then the measure outcome will be a vector - projection of
$\ket\psi$ onto the plain formed by $\ket{\phi_1}, \ket{phi_2}$.
% video 8-2

\paragraph{Example}

Imagine we have good old hydrogen atom with k possible state of the electron -
as such, it's quantum state is a unit vector in $\psi_0. \psi_1, \dots,
\psi_{k-1}$. If we need to measure energy, our observable then will be a matrix
A with same states but eigenvalues equal to energies of corresponding states:
$$\begin{pmatrix} E_0 & 0 & \dots & 0 \\ 0 & E_1 & \dots & 0 \\ \\ \dots \\ 0 & 0 & \dots & E_{k-1} \end{pmatrix}$$

Another example: imagine we're going to have eigenvectors $\ket+, \ket-$ and
want them to have eigenvalues 2, -3. First step: find a projection for our new
basis:
\begin{itemize}
\item Vector $\ket+$: $\ket+\bra+ = \ket+ (\ket+)^\dag =
  \binom{\frac1{\sqrt2}}{\frac1{\sqrt2}} (\frac1{\sqrt2} \quad \frac1{\sqrt2})
  = \begin{pmatrix} \frac12 & \frac12 \\ \\ \frac12 & \frac12 \end{pmatrix}$
\item Vector $\ket-$: $\ket-\bra- = \ket- (\ket-)^\dag =
  \binom{\frac1{\sqrt2}}{-\frac1{\sqrt2}} (\frac1{\sqrt2} \quad -\frac1{\sqrt2})
  = \begin{pmatrix} \frac12 & -\frac12 \\ \\ -\frac12 & \frac12 \end{pmatrix}$
\end{itemize}
Summary: $A = 2 \ket+\bra+ + (-3) \ket-\bra- =
\begin{pmatrix} \frac52 & -\frac12 \\ -\frac12 & \frac52 \end{pmatrix} $ Let's
apply this to the $\ket+$ state: $(2 \ket+\bra+ + (-3) \ket-\bra-)\ket+ = 2\ket+
+ (-3) 0 = 2 \ket+$

\paragraph{In general: } given $\ket{\phi_i}, \lambda_i$, corresponding
observable is
$$A = \sum \lambda_i \ket{\phi_i}\bra{\phi_i}$$
- therefore equivalent to our previous notion of measurement: ``pick the
orthonormal basis''

\subsection{Expectation Value and Variance}
\label{sec:8-3}

\begin{itemize}
\item An observable M for a k-level quantum system is a $k\times k$ Hermitian
  matrix. Random value $X$ denotes outcome of measurement of state $\ket \psi =
  \sum \alpha_i \ket{phi_i}$.
\item Distribution of X: $Pr[X = \lambda_j] = |\alpha_j|^2$
\end{itemize}
So we have usual parameters:
\begin{gather*}
  \mu = E[X] \\ \sigma^2 = Var[X] = E[(X-\mu)^2]
\end{gather*}
Interpretation: if we'd rotate the shape around $\mu$ - axis, moment of inertia
will be equal to the one created by ...

So, if we have observable $M$ on state $\ket\psi$, then:
$$\mu=E[X]=\bra\psi M\ket\psi$$

\paragraph{Prof:} by definition $\mu = E[x] = \sum |\alpha_j|^2 \lambda_j$; and
$M$ is a diagonal matrix, so \begin{gather*}\bra\psi M\ket\psi = (\alpha_0^* \;
  \alpha_1^* \dots \alpha_{k-1}^*) \begin{pmatrix} \lambda_0 &&& \\ & \lambda_1
    && \\ && \ddots &\\ &&& \lambda_{k-a}\end{pmatrix} \begin{pmatrix} \alpha_0
    \\ \alpha_1
    \\ \vdots \\ \alpha_{k-1} \end{pmatrix} = \\
  = \sum \alpha_j^*\lambda_j\alpha_J = \sum \alpha_j^*\alpha_j\lambda_j = \sum
  |\alpha_j|^2 \lambda_j
\end{gather*}

Variance: \begin{gather*} \sigma^2 = Var[X] = E[X^2] - (E[X])^2 = E[X^2] - \mu^2\\
  = \bra\psi M^2 \ket\psi - (\bra\psi M \ket\psi)^2
\end{gather*}

\paragraph{Prof:} by definition $E[x^2] = \sum |\alpha_j|^2 \lambda_j^2$, and
\begin{gather*}\bra\psi M^2\ket\psi = (\alpha_0^* \; \alpha_1^*
  \dots \alpha_{k-1}^*) \begin{pmatrix} \lambda_0 &&& \\ & \lambda_1 && \\ &&
    \ddots &\\ &&& \lambda_{k-a}\end{pmatrix}^2 \begin{pmatrix} \alpha_0 \\
    \alpha_1
    \\ \vdots \\ \alpha_{k-1} \end{pmatrix} = \\
  = \sum \alpha_j^*\lambda_j^2\alpha_J = \sum \alpha_j^*\alpha_j\lambda_j^2 =
  \sum |\alpha_j|^2 \lambda_j^2
\end{gather*}

(when you square observable, it is eigenvalues which are being squared, the
eigenvectors remain the same).

\subsection{Schredinger's Equation}
\label{sec:8-4}

Axiom of unitary rotation:
\begin{itemize}
\item {\bf Unitary evolution axiom:} a quantum system evolves by unitary
  rotation of the Hilbert space: $$U U^\dag = U^\dag U = I$$
\item But.. by which unitary rotation?
\item Energy observable $H$, called the Hamiltonian of the
  system \begin{itemize}
  \item Its eigenvectors $\ket{\phi_i}$'s are the states with definite energy
  \item The eigenvalues $\lambda_i$'s are the energy of the corresponding state
  \end{itemize}
\item Example $H = \begin{pmatrix} -\frac12 & \frac52 \\ \frac52 &
    -\frac12 \end{pmatrix}$
  \begin{itemize}
  \item $\ket+$ with energy = 2
  \item $\ket-$ with energy = -3
  \end{itemize}
\item The {\bf Schrodinger's equation }. If $\ket{\psi{t}}$ - the state of the
  system at time $t$, then:
$$i\hbar \frac\partial{\partial t}\ket{\psi(t)} = H\ket{\psi(t)}$$
- differential equation:
\begin{itemize}
\item $i = \sqrt{-1}$
\item $\hbar$ - Plank's constant
\end{itemize}
\end{itemize}

\paragraph{Solving Schrodinger's equation}

Let initial state $\ket{\psi(0)} = \ket{\phi_j}$ where $\ket{\phi_j}$ is some
eigenvector of $H$ with a corresponding eigenvalue $\ket{\lambda_j}$

Then $\ket{\psi(t)} = e^{-\frac{i\lambda_j t}{\hbar}} \ket{\phi_j}$ - at time
$t$ the state is the same eigenvector $\ket{\phi_j}$ with some phase
$e^{-\frac{i\lambda_j t}{\hbar}}$ associated with it (see slide 8-4 page 5).
I.e. the phase ($\theta$ angle) rotates at a rate proportional to energy.

Next, as $H \ket{\phi_j} = \lambda_j \ket{\phi_j} \Rightarrow $ at any time the
state vector points at the same direction, just with different amplitude:
$$\ket{\psi(t)} = a(t)\ket{\phi_j}$$
So, solving equation:
\begin{equation*}
  \begin{split}
    i\hbar\frac{\partial a(t)}{\partial t}\ket{\phi(t)} &= H(a(t) \ket{\phi_j})
    = \\
    &=a(t) \lambda_j \ket{\phi_j}\\
    \frac{\partial a(t)}{a(t)} &= \frac{\lambda_j}{i\hbar} \partial t\\
    \Rightarrow \quad a(t) &= e^{-\frac{i\lambda_j t}{\hbar}}
  \end{split}
\end{equation*}

Again, ``if the state starts with some direction, it always points at this
direction - with some phase precession over time''.

Of course, in general, the starting position is some superposition of states -
good old $\ket{\psi(0)} = \sum \alpha_j \ket{\phi_j}$. By linearity, this means
that $\ket{\psi(t)} = \sum \alpha_j e^{-\frac{i\lambda_j t}{\hbar}}
\ket{\phi_j}$

i.e. each state stays invariant in time - ``just precesses''.

In the eigenbasis, we can write
$$\ket{\psi(t)} =
\begin{pmatrix} e^{-\frac{i\lambda_1 t}{\hbar}} & 0 \\ \\ 0 &
  e^{-\frac{i\lambda_k t}{\hbar}} \end{pmatrix} \ket{\psi(0)}$$ - the state at
the moment $t$. The matrix (let's call it $U(t)$) is obvious unitary: $U(t)
U^\dag(t) = U^\dag(t) U(t) = I$. Shorthand notation:
$$U(t) = e^{-\frac{i H t}{\hbar}}$$
(in format: $B = e^A$ for matrices A and B means that B has the same
eigenvectors as A but eigenvalues of B are the exponents of eigenvalues for A)

\paragraph{Example}

Suppose Hamiltonian is $H = X$; start of the zero state: $\ket\psi(0) = \ket0$.
What is $\ket\psi(t)$?

Solution: The X's eigenvectors are: $\ket+$ with eigenvalue 1, $\ket-$ with
eigenvalue -1. So:
\begin{equation*} \begin{split}
    \ket{\psi(0)} &= \frac1{\sqrt2}\ket+ + \frac1{\sqrt2}\ket- \\
    \ket{\psi(t)} &= \frac1{\sqrt2}e^{-\frac{i\lambda_1 t}{\hbar}} +
    \frac1{\sqrt2}e^{-\frac{i\lambda_2 t}{\hbar}} \\
    &= \frac1{\sqrt2}e^{-\frac{i \; t}{\hbar}} + \frac1{\sqrt2}e^{\frac{i \;
        t}{\hbar}}
  \end{split} \end{equation*}

% video 8-5
The whole story about Schrodinger's equation is that it ties the evolution of
the system to the system's energy (as observable H is the ``energy''
measurement).

From the unitary evolution axiom we derive that evolution operator (U) should
look like $U=e^{-iMt}$ for some Hermitian operator $M$. Let's try to understand
why M should be H (Hamiltonian).

If A is arbitrary (``any'') observable this means that ($\equiv$) it conserves
physical quantity over time, then A must commute with M:
$$ A M = M A$$ (in general, matrix multiplication does not commute).

Let's we have a state $\ket\psi$, and after some infinitely small time t we have
a state $\ket{\psi'} = U \ket \psi = e^{-iMt}\ket\psi $.

``A is a conserved quantity'' means that $\bra\psi A \ket\psi = \bra{\psi'} A
\ket{\psi'} = \bra\psi U^\dag A U \ket\psi$.

As this holds for all $\psi$, then:
$$A = U^\dag A U = e^{iMt} A e^{-iMt}\approx (1+iMt) A (1-iMt)$$

(t is small so this ``$\approx$'' should work). Again:
$$ \dots \approx A + it[MA-AM] \Rightarrow it [MA-AM] = 0$$
(ignoring $O(t^2)$ terms coming from multiplication). So $MA = AM$, and A must
commute with M.

So, for now on: \begin{itemize}
\item $U = e^{-iMt}$ where M is Hermitian
\item if A is conserved $\Rightarrow AM = MA$
\end{itemize}
Th idea is that energy is always conserved (in contrast with, say, momentum
which can change if potential energy changed). There must be some ``intrinsic
reason'' why A and M commute.
% video 8-5, 10:21
The most natural reason would be that $H = \hbar M$ - i.e. A (in our case A is
H) is just linearly connected to M. At least, we expect $H = f(M)$, but, due to
some ``out of our scope physical reasons'' this function $f$ must be linear. For
the case: imagine we have a system consisting of two destroying pieces 1 and 2,
having associated operators $M_1, M_2$ and Hamiltonian $H_1, H_2$. The energy of
composed system is $H_1 + H_2$; on the other hand, the operator of the composed
system will also be $M = M_1 + M_2$, so $H = f(M_1 + M_2)$ and $f(M_1 + M_2) =
f(M_1) + f(M_2)$, so $f(M)$ must be a linear function.

\section{Continuous quantum states}
\label{sec:ContinuousQuantumStates}
% video 9-1
The formalism applicable to discrete states works for continuous states too.

\subsubsection*{ Representation of the continuous state:}

Imagine a particle which can be in one of discrete points of the 1D space, but
from $-\infty$ to $\infty$. Its state is $\ket\psi = \sum\limits_{j=-k}^{k}
\alpha_j \ket j$, where $|\ket\psi|^2 = \sum|\alpha_j|^2 = 1$. Shorter form
(slightly abusive but still OK): $\alpha_j = \psi(j)$.

Now consider distance between points to be $\delta$ (not 1) - see slide 9-1,
page 3. Then with $\delta \to 0; k \to \infty$, and for every $x$ we can build
so-called wave-function: $\Psi(x): \bigR \to \cplx$.Thus we get the infinite
dimensional vector - function which returns the complex amplitude for every real
value. Then:
$$\int \limits_{-\infty}^\infty \Psi^*(x) \Psi(x) \delta x =\int
\limits_{-\infty}^\infty |\Psi(x)|^2 \delta x = 1$$

So particle on the line is described by wave function. It is normalised,

At time $t = 0$: position is described as Gaussian
$$\psi(x, t=0) = \frac1{\sqrt[4]{2\pi}} e ^{-x^2}$$ 

\subsubsection*{Observables}

\begin{itemize}
\item How does $\psi(x, t)$ evolves with time?
\item What is the velocity of the particle? If we measure the state, we'll
  destroy the state - so more convenient is to measure momentum ($mv$), not
  velocity (as measuring momentum does not include measuring position in two
  different times).
\end{itemize}

\subsubsection*{Schrodinger's equation for free particle in 1D}
% video 9-2 (site only)
Motion equation is a {\bf local} equation - which means ``every point is space
minds it's own business'', but wave function also is dependant on the values in
the immediate neighbourhood: change speed in $\psi(x,t)$ value is proportional
to the difference between current value and average value of wave function - see
slide 9-2, page 3:
 $$\frac{\partial\psi(x, t)}{\partial t}  \propto y $$

 When $\partial \to 0$, then $y$ is similar to derivative: $y \sim
 \frac{\partial \psi(x)}{\partial x}$. But really it is proportional to the
 second derivative: $y \sim \frac{\partial \psi^2(x)}{\partial^2x}$:

$$ y \sim \frac{\psi(x-\partial x) + \psi(x+\partial x) }2 -
\psi(x) = \frac{ (\psi(x+\partial x)-\psi(x)) - (\psi(x)-\psi(x-\partial x))}2$$

- this is proportional to the second derivative. So Schrodinger's equation says:

$$i \frac{\partial \psi(x,t)}{\partial t} = \text{<<some constant>>}\frac{\partial^2 \psi(x, t)}{\partial x^2}$$

- notice $i$ on the left part. This means that complex amplitude of $\psi$ will
be the same but the whole vector will rotate keeping normalisation.

The full form of equation:
$$ i\hbar\frac{\partial \psi}{\partial t} = - \frac{\hbar ^2}{2m}
\frac{\partial^2 \psi}{\partial x^2}$$

\subsubsection*{Uncertainty principle - position \& momentum}

Measuring position means finding probability as function of $X$ with parameters
$E[x]$ (mean value) and distribution $\Delta x = E[x^2] - E^2[x]\; (=\sigma^2)$
and/or momentum: probability of seeing some value of momentum as function of
momentum value: $P[p]$ with spread $\Delta p = E[p^2] - E^2[p]$ So uncertainty
principle says:
$$ \Delta x \Delta p \geq \hbar / 2$$

As $\psi(x, t)$ is a continuous function, the probability of finding a particle
in some point x is 0, so we get a probability of finding particle in some
interval $[x .. x + \delta x]$:
$$P = \int \limits_x^{x+\delta x} |\psi(x, t)|^2 \partial x $$
- see slide 9-3, page 3 for combination of position and momentum.

Now for velocity: as $\psi(x, t = 0) = e^{ikx}$ - can be presented as complex
plane perpendicular to axis; so complex amplitude vector draws a ``spring'' with
a step of $2 \pi/k$: $\psi(x) = \psi(x + 2\pi/k$ (see slide 9-3 page 4).

Then $\psi(x, t) = e^{i(kx + \omega t)}$, and we can apply the ``rough'' form of
the Schrodinger's equation $\frac{i\partial\psi}{\partial t} = \frac{\partial^2
  \psi}{\partial x^2}$: $ii \omega e^{i(kx + \omega t)} = (ik)^2 e^{i(kx +
  wt)}$. Removing exponents: $\omega = k^2$, and
$$\psi(x, t) = e^{ik(x + kt)}$$
% video 9-4

So velocity is proportional to $k$:
\begin{itemize}
\item period = $2\pi/k$
\item time = $2\pi/k^2$
\item velocity = $\frac{2\pi/k}{2\pi/k^2} = k$
\end{itemize}


Actually, we're given $\psi(x, t)$ as wave function, not $e^{ikx}$. We can
imagine distribution of $\phi(v, t)$ depending on velocity $v$ as some separate
superposition. This should be an inner product of the ``original'' function:
$$\phi(v, t) = < e^{ikx}, \psi(x, t)> = \int \limits_\infty^\infty e^{-ikx}
\psi(x, t) \delta x$$

Then $\phi$ is a Fourier transform of $\psi$ (position). See slide 9-3, page 6
for illustration; and from the properties of the Fourier transform: $\Delta x
\Delta p \geq \hbar/2$ (actually it is $\geq$ some constant, of course).

\section{Position and momentum observables }
\label{sec:Position_and_momentum_observables}
% video 10-1

% ?????
Let's return to the idea of k discrete points on the line: $[0 .. k-1]$. The
state is $\ket\psi = \sum \limits_{j=0}^{k-1} \alpha_j \ket j$. Position
observable $M = M^\dag$ (Hermitian); eigenvector is a state of definite
position: $M = \left[ \begin{array}{cccc} 0 & 0 & & 0 \\ 0 & 1 & & \\ & & \ddots
    & \\ 0 & & &k- 1 \end{array}\right]$; $M \ket j = j \ket j$ (eigenvalue is
also $j$).

If we want particle to be anywhere on the line, we go for wave function $\psi(x)
: \bigR \to \cplx $ (infinite dimension function).
% ?????
Inner product: if we have two wave functions $\psi(x)$ and $\phi(x)$, we can
define {\bf inner product}: $$<\phi(x)| \psi(x)> = \int \limits_{-\infty}^\infty
\overline{\phi(x)} \psi(x) \delta x $$

where $\overline{ \phi(x)}$ - conjugate: if $\alpha = a + ib$ then $\alpha^* =
\overline\alpha = a-ib$

So observable M is a Hermitian: $M = M^\dag \Leftrightarrow \bra i M \ket j =
\overline{\bra j M \ket j}$ - (AKA self - adjoint matrix). (BTW, this holds for
any two states $\psi$ and $\phi$: $\bra\phi M \ket\psi = \bra \psi M \ket \phi$

So the observable is a linear mapping of wave functions: self-adjoint M that
maps wave functions to wave functions.

Example: {\bf position observable} $\hat{x}$. When applied to some wave function
$\psi$: $\hat x \psi(x) = \phi(x)$ where $\phi(x) = x \; \psi(x)$.

Remember in a finite-position case we had a position observable
$\left[ \begin{array}{cccc} 0 & & & 0 \\ & 1 & & \\ & & \ddots & \\ 0 & & &k-
    1 \end{array}\right]$. When applied to some state $\psi(x) = \begin{pmatrix}
  \alpha_0 \\ \alpha_1 \\ \vdots \\ \alpha_{k-1} \end{pmatrix}$ gave $
\begin{pmatrix} 0 \; \alpha_0 \\ 1 \; \alpha_1 \\ \vdots \\ (k-1) \;
  \alpha_{k-1} \end{pmatrix} $ - the position's eigenvector is a position itself
($\ket j$): whatever is in the position $j$ gets multiplied by position number
(eigenvalue) itself. And this is exactly what happens with position observable
$\hat x$.


{\bf Momentum operator}: $\hat p = -i\hbar \frac \partial {\partial x}$. When
applied to the state:
$$ \hat p \; \psi(x) = - i \hbar \frac{\partial \psi(x)}{\partial x}$$

Finite-dimension analogue: the derivative $\frac \partial {\partial x}$ on our
discrete 1D space in point $x$ should be something like ``take values in $x-1$
and $x+1$ points, calculate median, divide by 2 (distance between $x-1$ and
$x+1$)''. This corresponds to $\left[ \begin{array}{cccccc}
    0& 1&  &  & & 0 \\
    -1& 0& 1&  & & \\
    &-1& 0& 1& & \\
    &  & \ddots & \ddots & \ddots \\
    & & &-1 & 0 &1 \\
    0 & & & & -1 &0 \\
  \end{array}\right]$. Applied to $\psi(x) = \begin{pmatrix} \alpha_0 \\ \alpha_1 \\ \vdots \\
  \alpha_{j-1} \\ \alpha_j \\ \alpha_{j+1} \\ \vdots \\
  \alpha_{k-1} \end{pmatrix}$ will be
% video 10-1 13:10
$\begin{pmatrix} \vdots \\ \vdots \\ \alpha_{j+1} - \alpha_{j-1} \\ \vdots \\
  \vdots \end{pmatrix}$

The problem is, this matrix is not Hermitian, so we need to multiple the matrix
by $i$:
$$\left[ \begin{array}{cccccc}
    0& i&  &  & & 0 \\
    -i& 0& i&  & & \\
    &-i& 0& i& & \\
    &  & \ddots & \ddots & \ddots \\
    & & &-i & 0 &i \\
    0 & & & & -i &0 \\
  \end{array}\right]$$

So, for {\bf momentum operator} $\hat p = -i\hbar \frac \partial {\partial x}$.
Remember the Schrodinger's equation: $i \hbar \frac{\partial \ket\psi}{\partial
  t} = H \ket\psi$ - so we ``just'' need to understand what is the $H$
(Hamiltonian). Formally, this is the energy operator: Potential Energy + Kinetic
Energy. By definition, the free particle does not have a potential energy.
Classically, Kinetic Energy is $\frac{m\;v^2}2 =anujsaxena3 \frac{p^2}{2 \; m}$.
Heuristically (``guessing'') just replace momentum with momentum operator (this
trick is named a ``correspondence principle''):

\begin{gather*}
  i \; \hbar \; \frac{\partial \psi}{\partial t} = \frac{\hat p^2}{2 m} \ket
  \psi = -i \hbar \frac\partial{\partial x} (-i \hbar \frac\partial{\partial x})
  \frac1{2m} \ket\psi = \\
  = \frac{-\hbar}{2m}\frac{\partial^2 \ket\psi}{\partial x^2}
\end{gather*}
-i.e. the second derivative of a state
% video 10-2

Remember: different operator have different eigenvectors $\Leftrightarrow$ they
do not commute: $Z \; X \neq X Z$, or $ZX - XZ \neq 0$

\subsubsection{Position-momentum Uncertainty:}
$$[\hat x, \hat p] = \hat x \hat p - \hat p \hat x = i\hbar$$

Or, through spreads: $\Delta\hat x \Delta\hat p \geq \frac{|[\hat x, \hat p]|}2
\geq \frac \hbar 2$ - actually, a sub-case of a more general theorem saying that
$ \Delta A \Delta B \geq \frac{|[A, B]|}2$

So let's look at this: \begin{gather*}(\hat x \hat p - \hat p \hat x) \psi(x) =
  \hat x \hat p \psi(x) - \hat p \hat x \psi(x) \\
  = x \frac{-i \hbar \partial }{\partial x} \psi(x) - \left( \frac{-i
      \hbar \partial
    }{\partial x} x \psi(x)\right) = \\
  = -i \hbar \left(x \frac{\partial \psi}{\partial x} - \underbrace{
      \frac \partial {\partial x} x \psi(x)}_{\psi(x) + x\frac\partial{\partial
        x} \psi(x) } \right) = i \hbar \psi(x)
\end{gather*}

\subsection{Particle in a box}
\label{sec:Particle-in-a-box}
% video 10-3

Basically, it is a toy model for a hydrogen atom: the electron is ``trapped'' by
proton: Coulomb attraction confines the electron to within some distance $l$ -
so we model it in 1D as radial distance. See slides 10-3, page 3: outside of
$[0..l]$ range we consider potential to be infinite (in any case, electron can
not cross this border).

We'll solve the Schrodinger's equation for the ``particle in the box'':
$$i \hbar \frac{\partial \psi}{\partial t} = H \psi = \frac{\hat{p}^2}{2m}
\ket\psi + V(x) \ket \psi = \underbrace{-\frac{\hbar^2}{2m}
  \frac{\partial^2}{\partial x^2}}_{Hamiltonian} \ket\psi $$

where $V(x)$ - potential energy. Boundary condition: $\psi(0) = \psi(l) = 0$

So let's try to solve this for an eigenstate of H: $H \ket \phi = \lambda \ket
\phi$. Once we find such eigenstate, then we'll know that if $\psi(0) = \ket\phi
\Rightarrow \psi(t) = e ^ {-i \lambda t/\hbar} \ket\phi$. Then we can figure out
all the eigenvectors of $H$ with corresponding eigenvalues; after decomposing
initial state $\psi$ by these eigenvalues we'll understand how that state will
evolve.

Let's guess these eigenstates: we have operator $H = -\frac{\hbar^2}{2m}
\frac{\partial^2}{\partial x^2}$ and state $\phi(x) = e^{ikx}$.

Applying: $H \phi(x)= -\frac{\hbar^2}{2m}\frac{\partial^2}{\partial x^2} e^{ikx}
= \frac{\hbar^2}{2m} k^2 e^{ikx}$. This is eigenstate with eigenvalue $E =
\frac{\hbar^2 k^2}{2m}$; the same eigenvalue is for $\phi^\perp(x) = e^{-ikx}$.

So we have an eigen-space, and solution will be $\phi_E(x) = Ae^{ikx} +
Be^{-ikx} (\text{by Euler's formula:}) = C\sin kx + D\cos kx$; eigenvalue
(energy) is $E_k = \frac{\hbar^2 k^2}{2m}$.

We also need to impose the boundary conditions: $\phi(0) = 0; \phi(l) = 0$.
Then:
\begin{gather*}
  \phi(0) = C \times 0 + D \times 1 = 0 \Rightarrow C=0 \\
  \phi(l) = C \times sin kl = 0 \Rightarrow kl = n\pi \Rightarrow k_n =
  \frac{n\pi}l
\end{gather*}

So energy is $E_n = \frac{\hbar^2 K_n^2}{2m} = \frac{\hbar^2 n^2 \pi^2}{2ml^2}$,
and eigen-function is $\phi_n(x) = c \sin \frac{n\pi}l x$

Now we need to ensure that $\int \limits_0^l C^2 \sin^2\frac{n\pi}l x \delta x =
1 \Rightarrow C^2 = 2/l \Rightarrow C=\sqrt{2/l} $

So {\bf solution for Schrodinger's equation}:
$$\text{Energy: } E=\frac{\hbar^2 n^2 \pi^2}{2ml^2}; \quad \text{Wave function: } \psi_n(x) = \sqrt \frac2l \sin \frac{n
  \pi x}l$$

-''quantized'' by integer $n$ - see plotting on slide 10-3, page 5. Every next
$n$ value means increasing energy by $n^2$; eigen-states are looking like
standing waves.

Any particular $\psi(x)$ could be decomposed as $\psi(x) - \alpha_n \psi_n(x)$
at time 0. So at time $t$: $$\psi(x, t) = \alpha_n e^{-iE_nt/\hbar} \psi_n(x)$$

% video 10-4

\subsection{Qubits}
\label{sec:Qubits}

\begin{itemize}
\item To implement a qubit we ensure that electron has small enough energy $E <
  E_3$ - to ensure that our electron is either in $n=1$ or in $n=2$ state. Then
  we can codify
  \begin{itemize}
  \item $n=1$ as $\ket0$
  \item $n=2$ as $\ket1$
  \end{itemize}
\item Suppose $\ket{\psi(t=0)} = \alpha\ket0 + \beta\ket1 =
  \alpha\sqrt{\frac2{l}} \sin \frac{\pi x}l + \beta \sqrt{\frac2l}\sin
  \frac{2\pi x}l $
\item Then:
$$\ket{\psi(t)} = \alpha \sqrt{\frac2l} e^{-\frac{iE_1t}\hbar} \sin \frac{\pi
  x}l + \beta \sqrt{\frac2l} e^{-\frac{iE_2t}\hbar} \sin \frac{2\pi x}l =
\dots$$ - each of two components precesses with its own frequency (rates are
proportional to $E_1$ and $E_2$ respectively). Factoring out the $E_1$:
$$\dots = \sqrt{\frac2l} e^{-\frac{iE_1t}\hbar} \left( \alpha \sin \frac{\pi x}l
  + \beta e^{-\frac{i(E_2-E_1)t}\hbar} \sin \frac{2 \pi x}l \right)$$

- So we have relative rotation with frequency proportional to $\Delta E =
E_2-E_1$; for hydrogen atom it will be $\Delta E \approx 10 \; eV $, and the
frequency is $v \approx 2.5 \times 10^{15} Hz$E

This frequency is close to the frequency of light - this is why one can control
qubit optically through interaction with precise light pulses directed at qubit.
\end{itemize}

% video 11-1
\section{Quantum circuits}
\label{sec:Circuits}

\subsection{n qubit systems}
\label{sec:nQubitSystems}
A quantum state of the system of $n$ qubits is a superposition of their quantum
states - so it is a $\cplx^{2^n}$ space. This power comes from the idea of the
Tensor Products: the system composed from two subsystems of $k$ and $m$
parameters is characterised by $k \times m$ parameters due to Quantum
entanglement. Axioms for n-qubit systems:
\begin{itemize}
\item Axiom 1: superposition principle The system state is $$\Psi = \sum_x
  \alpha_x \ket x \text{, normalised: } \sum_x |\alpha_x|^2 = 1$$ where each x
  is all n-bit string

\item Axiom 2: unitary evolution.

  We can select any two qubit and make some operation on them:
$$\left[ \begin{array}{cccc}
    1& 0& 0 & 0 \\
    0& 1& 0 & 0 \\
    0& 0& 0 & i \\
    0& 0& -i & 0 \\
  \end{array}\right] \otimes I_{n-2} \text{; the whole system state changes: } \Psi = \sum_x \alpha'_x \ket x  $$
the whole Hilbert space rotates on applying this Hamiltonian (slides 11-1, pages
6, 7).
\end{itemize}

\subparagraph{Example} Let's apply Hadamard to the first qubit of the system.
Initial state is: $$\psi_{before} = \alpha_{0x'} \ket{0x'} + \alpha_{1x'}
\ket{1x'}$$ where $x'$ is the state string of all qubits except 1.

After application:
$$\psi_{after} = \frac{\alpha_{0x'} + \alpha_{1x'}}{\sqrt2} \ket{0x'} +
\frac{\alpha_{0x'} - \alpha_{1x'}}{\sqrt2} \ket{1x'}$$ - updating all $2^n$
amplitudes $\alpha_x$ {\bf in the single operation}.

Looks quite promising to be used in computations. One of the problems is that
the whole operation is ``private'': the whole system is destroyed at a moment of
the {\bf measurement} (getting output).

% video 11-2
\subsection{Universal family of gates}
\label{sec:FamilyOfGates}

Quantum circuit is a sequence of gates with some amount of input qubits - see
slides 11-2, page 2. The ``classical output'' can be a state string.

\subsubsection{Universal quantum gate set}

In classical circuits, a certain set of gates enables universal computation:
AND, OR, XOR etc and NAND ($\overline{x \wedge y}$) is universal.

Quantum analogue: the claim is that having CNOT, H, X, Z, $\pi/8$ rotations one
can implement any circuit:
\begin{itemize}
\item clearly, we can not implement an arbitrary U with infinite precision
\item instead, given $\epsilon$, we implement $U_{\epsilon}$ which is
  $\epsilon$-close to $U$: $||U-U_{\epsilon}|| \leq \epsilon$. Expected amount
  of the elements needed for $d \times d$ system ($d$ inputs and $d$ outputs)
  looks like $O(d^2 \log^3 \frac1{\epsilon})$
\end{itemize}

\subsection{Reversible Computation}

Quantum computers are always reversible: if exists $U$ that, when applied to
$\ket x$ produces $U \ket x$, then exists $U^\dag$ that, when applied to $U \ket
x$, results in $\ket x$. Or $$U U^\dag = U^\dag U = I$$

One can just take the initial circuit and apply the reverses $g^\dag$ of the
elements $g$ in the reverse order (slides 11-3, page 2).

Classical circuits for computing a Boolean function provide $$f: \{0, 1\}^n
\rightarrow \{0, 1\}$$. Such function is not reversible, as some information
about input is lost (e.g. it is not possible to restore input of the AND
function by output and, say, the value of one of the inputs).
 
A quantum version of $C_f$ have to be reversible, so we add qubits $b$ in state
0 in addition to $x$ set and implement out function that (see slide 11-3, page
4):
\begin{itemize}
\item x states are passed ``as is''
\item b is XOR-ed with the function output, so initially we get the function
  result in b qubits: $b \oplus f(x)$; then, after applying the same $U_f$, we
  get reversed values.
\end{itemize}

Elements for classical circuit (see slide 11-3 page 4):
\begin{itemize}
\item NOT: X-gate
\item XOR: CNOT gate
\item AND: C-SWAP gate with c=0
\end{itemize}

So classical reversible computation can be built as $R_c$ with $x=\sum \limits_x
\alpha_x \ket x , \ket0$ input and $C(x), junk(x) = \sum \limits_x \ket{C(x)} |
junk(x) $ output. The problem is that we really want output of $C(x)$, and $x$
(plus, probably, some zero's), but not $junk(x)$.

% video 11-4
We need to remove junk because it prevents quantum interference (slides 11-3,
page 6). We can not just throw the junk away as output parts are entangled.

In order to preserve the result, we male a copy of $C(x)$ using the C-NOT and a
bunch of 0's (slides 11-3, page 9). Then we reverse whole story: $x, C(x),
junk(x)$ back to the x and 0's.

So it is possible, having an input of classical circuit $C: c \to C(x)$, to
build a quantum circuit such that $U_c: c, 0' \to x, C(x), 0's$, or in other
notation (through superposition):
$$ \sum_x \alpha_x \ket x \ket{0 \dots 0} \to \sum_x \alpha_x \ket x \ket{C(x)}
\ket{0\dots0}$$ (remember $\ket{xy} = \ket x \ket y = \ket x \otimes \ket y$).

% video 12-1

\section{Early Quantum Algorithms}
\label{sec:EarlyQuantumAlgs}

\subsection{Fourier Sampling}
\label{sec:FourierSampling}

The Hadamard transformation is a basic building block of quantum circuits.
\begin{itemize}
\item Single qubit: $\left( \begin{array}{cc} \frac1{\sqrt{2}} & \frac1{\sqrt{2}} \\
      \frac1{\sqrt{2}} & - \frac1{\sqrt{2}}
    \end{array} \right)$
  \begin{itemize}
  \item $\ket0 \to \ket+ = \frac1{\sqrt2} \ket0 + \frac1{\sqrt2} \ket1$
  \item $\ket1 \to \ket- = \frac1{\sqrt2} \ket0 - \frac1{\sqrt2} \ket1$
  \end{itemize}
\item Double qubit: $\left( \begin{array}{cc} \frac1{\sqrt{2}} & \frac1{\sqrt{2}} \\
      \frac1{\sqrt{2}} & - \frac1{\sqrt{2}}
    \end{array} \right) \otimes \left( \begin{array}{cc} \frac1{\sqrt{2}} & \frac1{\sqrt{2}} \\
      \frac1{\sqrt{2}} & - \frac1{\sqrt{2}}
    \end{array} \right) = \left( \begin{array}{rrrr} 
      \half & \half & \half & \half \\
      \half & -\half & \half & -\half \\
      \half & \half & -\half & -\half \\
      \half & -\half & -\half & \half \\
    \end{array} \right) $
  \begin{equation*} \begin{split} \ket{00} \to \ket{++} &= \left(\frac1{\sqrt2}
        \ket0 + \frac1{\sqrt2} \ket1\right) \otimes
      \left(\frac1{\sqrt2} \ket0 + \frac1{\sqrt2} \ket1\right) = \\
      &= \frac12\ket{00} + \frac12\ket{01} + \frac12\ket{10} + \frac12\ket{11}\\
      \vdots \\
      \ket{11} \to \ket{--} &= \left(\frac1{\sqrt2} \ket0 - \frac1{\sqrt2}
        \ket1\right) \otimes
      \left(\frac1{\sqrt2} \ket0 - \frac1{\sqrt2} \ket1\right) = \\
      &= \frac12\ket{00} - \frac12\ket{01} - \frac12\ket{10} + \frac12\ket{11}\\
    \end{split}
  \end{equation*}
\item Triple qubits: similar:
  \begin{equation*} \begin{split} \ket{000} &\to \ket+++ = \frac1{2\sqrt2}
      \ket{000} + \dots +
      \frac1{2\sqrt2} \ket{111} \\
      \ket{001} &\to \ket+++ = \frac1{2\sqrt2} \ket{000} + \dots +
      \frac1{2\sqrt2} \ket{111} \\
      \vdots \\
      \ket{111} &\to \ket--- = \dots
    \end{split}
  \end{equation*}
\end{itemize}

So, for arbitrary $n$ qubits: $H^{\otimes n}$: for all 0's - it's simple: $
\ket{0 \dots 0} \to \frac1{2^{n/2}} \sum \limits_{x \in \{0, 1\}^n} \ket x $

For arbitrary $\ket u = \ket {u_1 u_2 \dots u_n}$:
$$H^{\otimes n} \ket u = \sum_x \frac{(-1)^{u \cdot x}}{2^{n/2}} \ket x \text{ where }u
\cdot x = u_1 x_1 + \dots + u_n x_n $$

Example: for n = 3, u = 1 1 1, x = 1 0 1: $u \cdot x = 1 \cdot 1 + 1 \cdot 0 + 1
\cdot 1 = 2; \frac{(-1)^{u \cdot x}}{2^{n/2}} = \frac1{2\sqrt2}$

\subsubsection{Fourier Sampling}
\label{sec:FourierSampling}

Fourier Sampling is applying $H^{\otimes n}$ to the input state $\ket \psi =
\sum \limits_x \alpha_x \ket x$, resulting in output state $\ket {\hat \psi} =
\sum \limits_x \beta_x \ket x$, and subsequent measurement. Measurement outcome
is $y$ with probability $|\beta_y|^2$.

It is called Fourier transform because Hadamard transform is really a Fourier
transform of the group (?). But, the important part is the idea that you can set
up (create) some superposition $\ket \psi$, perform $H^{\otimes n}$, then
measure. This will give better sampling then direct measurement of $\ket\psi$.
% video 12-2

\subparagraph{Parity problem}

Imagine we're given some function $f: \{0, 1\}^n \to \{0, 1\}$ as a black box.
We know that $f(x) = u \cdot x$ for some ``hidden'' $u \in \{0, 1\}^n$. Our task
is to figure out $u$ in {\bf as few queries as possible}.

Classically, we'd feed it with input values like [1 0 \dots 0], [0 1 \dots 0],
[0, 0, \dots 1] and check output - total n measurements.

For quantum circuit, we'd be given $U_f$ with input $x = \sum \limits_x
\alpha_x, b$ and output $\sum \limits_x \alpha_x, b \oplus f(x)$. To reconstruct
a function, we can use so-called {\bf Bernstein - Vazirani Algorithm}:
\begin{itemize}
\item Set up a superposition $\frac1{2^{n/2}} \sum \limits_x (-1)^{f(x)} \ket x
  = \frac1{2^{n/2}} \sum \limits_x (-1)^{u \cdot x} \ket x$ (output of the
  Hadamard transform with input of $u$ we're looking for).
\item Fourier sample to obtain $u$.
\end{itemize}

\subparagraph{Setting up superposition}(slides 12-1, page 8):

feed input with $\ket{0^n} \to \frac1{2^{n/2}} \sum \limits_x \ket x$ and $\ket
b = \ket- = \hqrt \ket0 - \hqrt \ket1 $. Then, depending of answer:
\begin{itemize}
\item if $f(x) = 0$ then $\ket{b \oplus f(x)} = \ket- = \hqrt \ket0 - \hqrt
  \ket1$
\item if $f(x) = 1$ then $\ket{b \oplus f(x)} = - \ket- = \hqrt \ket1 - \hqrt
  \ket0$ - toggle phase
\end{itemize}
So now our function is:
$$ \frac1{2^{n/2}} \sum \limits_x \ket x \ket- \stackrel{U_f}{\longrightarrow}
\underbrace{\frac1{2^{n/2}} \sum \limits_x (-1)^{f(x)} \ket x}_{needed} \ket-$$

The final schema looks like slides 12-1, page 9.

\subsubsection{Recursive Fourier Sampling}
\label{sec:RecursiveSampling}

Further development of this algorithm is a {\bf Recursive Fourier Sampling}:
\begin{itemize}
\item Recursive version of the parity problem
\item Classical algorithms satisfy the recursion
$$T(n) > nT(n/2) + n$$
Solution: $T(n) = \Omega(n^{\log n})$ (super-polynomial)
\item Quantum algorithm satisfies recursion
$$T(n) = 2T(n/2) + O(n \log n)$$
Solution: $T(n) = O(n \log n)$ - polynomial
\end{itemize}

\subsection{Simon's Algorithm}
\label{sec:SimonAlg}

We're given s 2-1 function $f: \{0, 1\}^n \to \{0, 1\}^n$ such that there is a
secret string $s \in \{0, 1\}^n$ that $f(x) = f(x \oplus s)$.

Challenge: find $s$.

Example: $n = 3, s = 101, f(x=011) = f(x=110)$

Classically, we'd have a circuit implementing $f(x)$, and feed it with inputs
until we'd get a collision. According to birthday paradox, expected time is
after feeding $2^{(n/2)}$ - exponential time (number of attempts).

\paragraph{Algorithm}

assuming $r$ as random n bit string
\begin{itemize}
\item Set up random superposition $\hqrt \ket r + \hqrt \ket{r \oplus s}$ - see
  slide 12-3, page 5. When we measure $f(r)$, we get the superposition we've
  wanted.
\item Fourier sample to get a random $y: y \cdot s = 0 (mod2)$ Output state
  before measurement is $\sum \limits_y \beta_y \ket y$, where
$$\beta_y = \hqrt (\frac{(-1)^{r \cdot y}}{2^{n/2}} + \frac{(-1)^{(r \oplus s)
    \cdot y}}{2^{n/2}} = \frac{(-1)^{r\cdot y}}{2^{\frac{n+1}2}} \left[ 1 +
  (-1)^{s\cdot y}\right] $$
\begin{itemize}
\item Case 1: $s \cdot y \equiv 1 \to \beta_y = 0$
\item Case 2: $s \cdot y \equiv 0 \to \beta_y = \frac{(-1)^{r \cdot
      y}}{2^{\frac{n-1}2}}$
\end{itemize}
So half of y's will have a property for Case 2 ($y: y \cdot s \equiv 0$), and
they all will have equal probability of measurement $|\beta_y|^2 =
\frac1{2^{n-1}}$
\item Repeat steps n-1 times to generate n-1 linear equations in s
\item Solve for s (there will be two solutions: all 0's and the one we're
  looking for). OK, we have a system of $n-1$ equations $y^{(1)}, y^{(2)},
  \dots, y^{(n-1)}$, so what is the chance of these not to be dependant on
  previous (non-trivial)?
  \begin{itemize}
  \item for the first one: chance of being trivial is $\frac1{2^n}$ - at least
    one of members should not be 0
  \item for the second one: if the first equation is not trivial, chance for
    second to be trivial is ``it is all-0s or equal to first'':
    $\frac1{2^{n-1}}$
  \end{itemize}
  so summary chance for this system to be trivial is:
$$\frac1{2^n} + \frac1{2^{n-1}} + \dots + \frac14 \leq \half $$
- so equations are independent with probability $\geq \half$. If system is
dependant, we'll need to run it again.
\end{itemize}
the final schema - see slides 12-3, page 8

\subsubsection{Double Slit Experiment}
\label{sec:DblSlitExp2}
Simon's algorithm can be viewed as some form of a double-slit experiment.
Imagine initial state $u = u_1 \dots u_n$ passed through two Hadamard transforms
as on slide 12-4, page 3.

After first transformation: $\sum \limits_{x \in \{0, 1\}^n} \frac{(-1)^{u \cdot
    x}}{2^{n/2}} \ket x$; after second one: $\dots \to \sum \limits_y \beta_y
\ket y$, where
$$ \beta_y = \sum_x \frac{(-1)^{u\cdot x}}{2^{n/2}} \frac{(-1)^{x\cdot y}}{2^{n/2}}$$
\begin{itemize}
\item Case 1: y = u
$$ \beta_y = \sum_x \frac1{2^n} = 1$$
\item Case 2: $y \neq u$
$$ \beta_y = 0$$
\end{itemize}

\subsection{Quantum Fourier Transform}
\label{sec:QuantumFourierTransform}

\subsubsection{Building Blocks}

\paragraph{Period finding}

We're given function that maps numbers [0 \dots N] to elements of some set $S$:
$f : \{0, 1, \dots, N-1\} \to S$, and:
\begin{itemize}
\item $f$ is periodic with (unknown) period $r: \forall x \; f(x) = f(x + r
  (mod\; N))$
\item apart from periodic, there is no other equality: $x \neq y \means f(x)
  \neq f(y)$
\end{itemize}

The challenge is to find $r$. We can consider this function as a black box with
input $x$ and output $f(x)$. (for graph - see slide 13-0, page 2).

Primitive approach is $O(r)$. Quantum algorithm is $O(\log N)$.

Consider our black box as quantum circuit with $$\text {input } \sum \alpha_x
\ket x \ket0 \text{ and output } \sum \limits_x \alpha_x \ket x \ket{f(x)}$$

\paragraph{Quantum Period Finding Algorithm}

see slide 13-0, page 3 for schema. The idea is:

input becomes $\ket0$ transformed by Hadamard transform (now called Quantum
Fourier Transform - $QFT_N$) and fed to $U_f$. Resulting $\ket{f(x)}$ is
measured (or just thrown away); resulting $\ket x$ is transformed through
$QFT_N$ again, result is stored in $l$.

The claim is $\frac{l}{N} \approx \frac{k}{r}$ - if $N \gg r$ we can efficiently
reconstruct $r$.

Special case: assume $r / N$ ($N$ is a multiple of $r$).

\subsubsection{Definitions and Properties}
\label{sec:DefinitionsAndProps}

Quantum Fourier Transform (QFT) is a general case of Hadamard Transform -
actually, Hadamard Transform is a special case of QFT for a restricted group of
integers mod 2 (?).

The {\bf unity} is a set of points (vectors) $\omega \in \cplx: \omega^N = 1$.
Then step $\omega = e^{\frac{2\pi i}N}$, and $\omega^j = e^{\frac{2\pi j i}N}$
Example for $N=5$ - see slide 13-1 page 2.

The Fourier Transform is a matrix $N \times N$:
$$F_N = \frac1{\sqrt N} \left(  
\begin{array}{cccccc}
  1 &  1&  1& 1& \dots & 1 \\
  1 &  \omega & \omega^2 & \omega^3 & \dots & \omega^{N-1} \\
  1 &  \omega^2 & \omega^4 & \omega^6 & \dots & \omega^{2(N-1)} \\
  \vdots & \vdots &  \vdots & \vdots & \ddots & \vdots \\
  1 &  \omega^{N-1} & \omega^{2(N-1)} & \omega^{3(N-1)} & \dots & \omega^{(N-1)^2} \\
\end{array}\right)$$
so that $\omega = e^{\frac{2\pi i}N}$, and $(F_N){jk} = \omega^{jk}$

This is a unitary transform: any two distinct row or column are orthogonal.

Prof: Consider $$ \frac1N(1 + \omega + \omega^j + \omega^{2j} + \dots +
\omega^{(N-1)j}) = \left\{
  \begin{array}{cl}
    1 & if \; j = 0 \\ 
    0 & if \; j \neq 0
  \end{array}
\right. $$

The inner product of a column by itself will get a sum of $N$ terms for $j=0$ -
so product is 1. The inner product of any two different columns will be the case
for $j \neq 0$ and result in 0 $\to F_N$ is unitary.

Example: $N = 4 \to \omega = i; \; F_n = \frac12 \left(
  \begin{array}{cccc}
    1 & 1 & 1 & 1 \\
    1 & -i & -1 & -i \\
    1 & -1 & 1 & -1 \\
    1 & -i & -1 & i 
  \end{array} \right)$

Applying to some superposition: suppose initial superposition is
$\begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_4 \\ \alpha_4 \end{pmatrix}
= \begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix}$, then $F_N
\times \begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \alpha_4 \\
  \alpha_4 \end{pmatrix} = \frac12 \left(
  \begin{array}{cccc}
    1 & 1 & 1 & 1 \\ 1 & -i & -1 & -i \\ 1 & -1 & 1 & -1 \\ 1 & -i & -1 & i 
  \end{array} \right) \times \begin{pmatrix} 0 \\ 1 \\ 0 \\ 0 \end{pmatrix}
= \begin{pmatrix} 1 \\ i \\ -1 \\ -i \end{pmatrix} = \half \ket0 + \frac{i}2
\ket1 - \half \ket2 - \frac{i}2 \ket3$ - same amplitude but different phases.
% video 13-3

When Fourier Transform is applied to some input, we get $N$ - dimensional
vector:
$$F_N \times A = \frac1{\sqrt N} \left(  
\begin{array}{cccccc}
  1 &  1&  1& 1& \dots & 1 \\
  1 &  \omega & \omega^2 & \omega^3 & \dots & \omega^{N-1} \\
  1 &  \omega^2 & \omega^4 & \omega^6 & \dots & \omega^{2(N-1)} \\
  \vdots & \vdots &  \vdots & \vdots & \ddots & \vdots \\
  1 &  \omega^{N-1} & \omega^{2(N-1)} & \omega^{3(N-1)} & \dots & \omega^{(N-1)^2} \\
\end{array}\right) \times
\begin{pmatrix} \alpha_0 \\ \alpha_1 \\ \vdots \\ \alpha_{N-1} \end{pmatrix} =
\begin{pmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_{N-1} \end{pmatrix} $$

Classical approach require $O(N^2)$ steps. {\bf Fast Fourier Transform (FFT)}
runs in $O(N \log N)$ steps. In the quantum case:
\begin{itemize}
\item input vector is represented as the state of $n = \log N$ qubits: $\sum
  \limits_j \alpha_j \ket j$
\item FT ``just'' transforms this: $\sum \limits_j \alpha_j \ket j \to \sum
  \limits_k \beta_k \ket k$
\end{itemize}
our question is: does the quantum circuit which makes this transformation work
with polynomial time? QFT works in $O(n^2) = O(\log^2 N)$ steps - exponential
growth. The problem is that the amplitudes are inaccessible until we measure. On
measurement we see only $k$ with probability $|\beta_k|^2$. So we need a trick
to see the whole picture.

% video 13-4
\subsubsection{Period Finding}
\label{sec:PeriodFinding}

There are two interesting properties of QFT:
\begin{itemize}
\item Convolution-multiplication property of FT

  if $F_n(\sum \limits_j \alpha_j \ket j) = \sum \limits_k \beta_k \ket k$. Then
  we can apply the same transformation to the cyclic shift of initial position
  (imagine shift by 1 for example):
  $$ F_n((\sum \limits_j \alpha_j \ket j)_{shifted}) = $$ 
$$ = \frac1{\sqrt N} \left(\begin{array}{cccccc}
    1 &  1&  1& 1& \dots & 1 \\
    1 &  \omega & \omega^2 & \omega^3 & \dots & \omega^{N-1} \\
    1 &  \omega^2 & \omega^4 & \omega^6 & \dots & \omega^{2(N-1)} \\
    \vdots & \vdots &  \vdots & \vdots & \ddots & \vdots \\
    1 &  \omega^{N-1} & \omega^{2(N-1)} & \omega^{3(N-1)} & \dots & \omega^{(N-1)^2} \\
  \end{array}\right) \times 
\begin{pmatrix} \alpha_{N-1} \\ \alpha_0 \\ \alpha_1 \\ \vdots \\
  \alpha_{N-2} \end{pmatrix} = $$
$$= \begin{pmatrix} 1 \times \beta_0 \\ \omega \times \beta_1 \\ \omega^2 \times
  \beta_2 \\ \vdots \\ \omega^{N-1} \times \beta_{N-1} \end{pmatrix} $$ -
basically, amplitudes just change their phases (amplitudes are the same, actual
phase change {\bf depends on actual shift})

At this point, if we're going to do a measurement on the ``shifted'' output, the
probability will be the same as on the ``main'' case: $|\beta_k|^2$

So, if you're doing Quantum Fourier Sampling (QFS), the {\bf cyclic shift does
  not change the output distribution} - so-called {\bf
  Convolution-multiplication} property of FT.
\item if $F_M\left(\sum \limits_{j=0}^{M-1} \alpha_j \ket j \right) = \sum
  \limits_{k=0}^{M-1} \beta_k \ket k$. Claim: $\alpha_{j+r} = \alpha_j \means
  \beta_{k+\frac{M}r} = \beta_k$ (see slide 13-3, page 4).
  % video 13-4 10:03
  First prove a special case (non-zero amplitude only in multiplices of $r$ (see
  slip 13-3, page 5): $$\sqrt{\frac r N}(\ket0 + \ket r + \ket {2r} + \dots +
  \ket {N - r}) \xrightarrow{F_N} \frac1{\sqrt r} \left(\ket0 + \ket{\frac N r}
    + \ket{\frac{2N}{r}} + \dots + \ket {\frac{(r-1)N}{r}} \right)$$

  For example: $\ket0 \xrightarrow{F_N} \frac1{\sqrt{N}}(\ket0 + \ket1 + \dots +
  \ket{N-1})$ (only the first column is used).

  As we see, if left side function has a period of $r$, then the right side
  (after FT) has the period of $\frac{N}r$ - so the smaller $r$, the bigger
  period of Fourier-transformed function is (for special case!).

  % video 13-5
  Prof: as $(F_N){jk} = \omega^{jk}$, let's try to find the amplitudes: for
  particular entry: $$\beta_k = \sum \limits_{j=0}^{\frac Nr - 1}\sqrt{\frac rN}
  \frac1{\sqrt N} \omega^{jrkN/r} = \frac{\sqrt r}N \sum \limits_{j=0}^{\frac Nr
    - 1} \omega^{jkN}$$

  But $\omega^N = 1$, so whole $\omega^{jkN} = (1)^{jk} = 1$, and
$$\beta_k = \frac{\sqrt r} N \times \frac Nr = \frac1{\sqrt r} \text{ ; done}$$

Our resulting $B = \frac1{\sqrt{r}}\sum \limits_{k=0}^{r-1} \ket{k\frac Nr}$ is
already a unit vector, so amplitudes in all other points {\bf must } be 0.
\end{itemize}

\paragraph{Period Finding}

Quantum circuit-based solution - \slide {13-3} 8 (notice change in notation, now
total amount of elements in range of definition is $M$, not $N$).

First QFT, being applied to $\ket0$, will result in $\frac1{\sqrt
  M} \begin{pmatrix} 1 \\ 1\\ \vdots \\ 1 \end{pmatrix} = \frac1{\sqrt M} \sum
\limits_{x=0}^{M-1} \ket x$

After applying $U_f$: outcome will be $\sum \limits_{x=0}^{M-1}\ket x f(x)$ plus
$f(x)$ in second register which is to be measured for some value $f(k)$.
Measuring means that we exclude from the first register all states which are
inconsistent with the $f(k)$ being measured. So the state of first register
before measurement will be
$$\sqrt{\frac rM}( \ket k + \ket{k+r}+ \ket{k+2r} + \dots +
\ket{k+ M - r}) = \sqrt{\frac rM}\sum\limits_{j=0}^{\frac Mr -1} \ket{jr + k} $$

Now apply $QFT_M$. We remember that the measurement will not depend on cyclic
shift on the state (as $QFT_M$ is not changed with it) - so measurement will
remove $k$ from $\dots \ket{jr + k} \dots$ part - so we'll measure a
superposition of $\frac1{\sqrt r} \sum\limits_{l=0}^{r-1}\ket{l \frac Mr}$.

So with every measurement we'll get a random multiple of $\frac Mr$. We'll
repeat the whole story a few times and get a Greatest Common Denominator (GCD)
of the results.

% video 14-1
\subsection{Shor's Algorithm}
\label{sec:ShorsAlgorithm}

The Period Finding (PF) is used in Shor's Algorithm of factoring integers which
means a finding of factors for give $N$ that $N = p_1^{\;l_1} p_2^{\;l_2} \dots
p_k^{\;l_k}$. Classical approach takes exponential time; solving this problem is
highly anticipated in cryptography.

Conventions: the source number is $N$ of length $n$ bits; $N \approx 2^n$. The
best classical algorithm takes $O(exp(\sqrt n))$. Quantum algorithm takes
$O(n^3)$.

For our examples, assume $N = 21$. Now look: $\sqrt 1 = \pm 1; 1^2 \equiv 1
(\mod 21)$, but also $-1^2 = 20^2 \equiv 1 (\mod 21)$. There is another square
root of 21: $8^2 = 64 \equiv 1 (\mod 21)$. The claim is that in this case:
\begin{itemize}
\item $gsd(8 + 1, 21)$ will give one of the non-trivial roots (3)
\item $gsd(8 - 1, 21)$ will give another non-trivial roots (7)
\end{itemize}
% video 14-1, 11:44

Other non-trivial square root is -8: use $21-8 = 13$:
$$-8^2 = 13^2 \equiv 1 (\mod 21) \means gcd(13-1, 21) = 7; gcd(12, 21) = 3$$

\paragraph{Lemma:} If $x$ is a non-trivial square root of $1 (\mod N)$ then
$gcd(x+1, N)$ and $gcd(x-1, N)$ is a nontrivial factor of $N$.

\paragraph{Prof:} $x^2 \equiv \pm 1(\mod N) \same x^2 - 1 \equiv 0 (\mod N)
\same N \mid (x^2-1) \same N \mid (x+1)(x-1)$

But $x$ {\bf does not} divide $x\pm1: x \neq1(\mod N)$ - such is possible only
when $(x+1)$ divides one of the prime divisors (label P in future) while $(x-1)$
divides another one (label Q).

% video 14-2
As out N is really large (say 1000 digits). Our approach will be:

select small number (let's 2) and consider powers of 2:
\begin{itemize}
\item $2^0 = 1(\mod 21)$
\item $2^1 = 2(\mod 21)$
\item $2^2 = 4(\mod 21)$
\item $2^3 = 8(\mod 21)$
\item $2^4 = 16(\mod 21)$
\item $2^5 = 11(\mod 21)$
\item $2^6 = 1(\mod 21)$
\end{itemize}

Knowing that $2^6 \equiv 1(\mod21) \means (2^3)^2 \equiv 2^6 \equiv 1(\mod21)$,
and $2^3 = 8$ is our non-trivial square.

\paragraph{Lemma:} Let $N$ be an odd composite with at least two distinct prime
factors, and let $x$ be uniformly random between 0 and $N-1$. If $gcd(x, N) =
1$, then with probability at least $1/2$ the order $r$ of $x (\mod N)$ is even,
and $x^{r/2}$ is a nontrivial square root of $1 (\mod N)$.

Order $r$ means that $1 \equiv x^r (\mod N)$. If $r$ is even and $y = x^{r/2}
\neq \pm 1 (\mod N)$ then $y^2 = x^r \equiv 1 (\mod N)$ - can be used to find a
non-trivial root of N and use it for factoring.

Proof is out of scope.

This is nice, but we still have a task to find an order $r$ (can be quite
large).

\begin{tabular}{c|c}
  \textbf{ a } & \textbf{ $f(a) = x^a (\mod N)$ } \\
  \hline
  0 & 1  \\
  1 & 2  \\
  2 & 4  \\
  3 & 8  \\
  4 & 16  \\
  5 & 11  \\
  6 & 1  \\
  7 & 2  \\
  8 & 4  \\
  9 & 8  \\
  10 & 16  \\
  11 & 11  \\
  12 & 1  \\
  13 & 2  \\
  14 & 4  \\
\end{tabular}

- we see a form of periodic function $\means$ we can find a period. To find it,
set up a superposition: $\frac1{\sqrt{M}} \sum \limits_{a=0}^{M-1}f(a)$, use
relevant algorithm.

After getting period (in our example 6), we're ready to compute non-trivial
square root: $2^{6/2} = 8$, take $gcd(8+1, 21); gcd(8-1, 21)$.

Final circuit - slides 14-1 page 10:
\begin{itemize}
\item Start with two registers of $\ket0$
\item apply $QFT_M$ to the first register, obtain (?)
\item feed both registers to quantum circuit; calculate $f(a) = x^a (\mod N)$
\item measure (or throw away) second register
\item apply $QFT_M$ to the first register and measure
\end{itemize}

After measuring, we get the period $r$ value. Calculate $x^{r/2}$ - non-trivial
square root; use it to calculate factors of M. This works with probability
$\frac12$, so check and re-run if it did not work.

\paragraph{Algorithm in a Nutshell}

To find the factors of $N = PQ :P,Q \in {primes}$
\begin{itemize}
\item Create a function $f(a) = h^a (mod N)$, for some integer h where 1<h<N and
  h \& N are relatively prime.
\item $f(a)$ is periodic with period $r$. Use period finding to find $r$.
\item If $r$ is odd, repeat with different $h$ until $r$ is even.
\item $P = gcd(h^{r/2}+1,N),\; Q = gcd(h^{r/2-1}, N)$
\end{itemize}

% video 14-3
\subsubsection{Quantum Fourier Transformation Circuit}
\label{sec:QFTCircuit}

FT has its specific structure - as seen on slides 14-3 page 2. Classical schema
works in $O(n^2)$ time, but efficient FT circuit works much faster. This is
possible due to using the structure of FT operation:

\begin{itemize}
\item First - sort $\ket \alpha$, separating odd and even rows in two different
  halves of a vector. Similarly re-sort the columns of FT matrix. (slide 14-3
  page 3).
\item Now notice some pattern in rows: $\omega^{(j = \frac n2)2k} = \omega{2jk +
    nk}$ - as $\omega^{kn} = 1$. (slide 14-3, page 3, right part of a slide) -
  every row $j$ in upper part is equal to the $j + n/2$ row in the lower part.
\item As $\omega = e^{2 \pi i/n}$, then $\omega^2$ can be represented as
  $\omega^2 = e^{2 \pi i /(n/2)}$. Then if the whole FT matrix is $F_n$, the
  upper left part of it is $F_{n/2}$; lower left part is similar.
\item For upper right part it is a bit more interesting: for every column $2k$
  from left the column of index $2k+1$ is equal to $2k$ multiplied by
  $\omega^j$.
\item Lower right part is a bit more complicated: it is multiplied by
  $-\omega^j$.
\end{itemize}
The whole matrix reminds a Hadamard matrix a bit, but remember: we have swapped
values if $\ket \alpha$ and columns in $F_N$.

Now consider actual multiplication (slide 14-3 page 4). It looks now obvious
that there should be a recursive circuit - see slide 14-3 page 5 for a schema.
The size for $n\times n$ matrix: $S(n) - 2S(\frac n2) + O(n) \means S(n) = O(n
\log n)$ - instead of $O(n^2)$.

\paragraph{In quantum circuit}

Our input is the amplitudes on $m$ qubits so the state is described by $M = 2^m$
complex vector. Now try to perform a unitary transformation $QFT_M$
% video 14-4
The structure is to be similar to the classical circuit. First step is to
separate odd from even rows - based on least significant bit (this separates
odds from evens).

After applying $QFT_{M/2}$ to the $m-1$ bits we already have the ``odds'' part
(as superposition with zero's will not change anything). Then we apply H to the
least significant bit. As H transforms $0$ to $+$ and $1$ to $-$, we get our ``j
from evens added''/ ``j from odds subtracted'' functionality.

The trick is to use controls: $\omega^J = \omega^{J_{m-2}J_{m-3} \dots J_0} =
\omega^{J_{m-2} \times 2^{m-2}} \times \omega^{J_{m-3} \times 2^{m-3}} \dots
\omega^{J_0 \times 2^0}$ - see slides 14-3, page 3.

So the size of a circuit is $S(m) = S(m-1) + O(m) \means S(m) = O(m^2)$. As $M =
2^m$, $S=O(\log^2 M)$ - exponentially better the classical circuit.

\subsection{Quantum Search}
\label{sec:QuantumSearch}

Goal: search for the marked entry. Classically: try every entry in turn (or
random entries) which shows $O(N/2)$ time.

Typical computation problem, so-called NP-complete problems, are hard to find an
answer bu easy to check if given entity is the needed one. Finding a solution to
NP-complete problem can be viewed as a search problem like
$$(x_1 \vee -x_2 \vee) \wedge (x_2 \vee -x_5 \vee x_6) \wedge \dots$$
- is there a configuration of $x_1, x_2, \dots$ that satisfy the above formula?

There are $2^N$ possible configuration. As the source data is unstructured, we
can not use indexing. But:
\begin{itemize}
\item Let $N=2^n$, and each entry in ``haystack'' corresponds to some set of $n$
  Boolean values.
\item Some number out of $N$ entries satisfy the required condition. We're
  looking for any one of them
\end{itemize}
\video{15-1}{06:30} The idea is to use the fact that we have an exponential
superposition to prove all of these $n$ entries in parallel, in so-to-speak
``different quantum universes''. I.e. we can take $n$ qubits, put them into
equal superposition of all n-bit strings from 0 to $N-1$:
$$\frac1{\sqrt{N}}\sum\limits_{y=0}^{N-1} \ket y $$  
or digital ``haystack'' corresponds to
$$f(x) = (x_1 \vee \bar x_2 \vee x_3) \wedge (\dots) \wedge \dots (\;)\text{,
  where } x = x_1, x_2, \dots x_n$$

Now let's feed our superposition $\frac1{\sqrt{N}}\sum\limits_{y=0}^{N-1} \ket y
$ to the circuit implementing $U_f$ (\slide{15-1}7). Outputs will be a
superposition $\sum\limits_y \ket y$ and function result $\ket{f(y)}$.

Measuring this will be a bad idea, as this will give no better then probing a
random entry of the table.

\subparagraph{Theorem:} Any quantum algorithm must take at least $\sqrt N$ time
(still exponential in $n$).

\subsubsection{Grover's Algorithm} is a theoretically fastest realisation of a
search - $O(\sqrt N)$ time.

\subparagraph{Problem } Given $f:{0, 1, \dots , N-1} \to {0, 1}$, find $x:f(x) =
1$. {\bf Hardest case:} There is exactly one $x: f(x) = 1$.

Classical circuit: $N$ inputs for $x$, single output for $f(x)$. Quantum
circuit: $N$ input for $x$, single input for $0$; $n$ output for $x$, single
output for $f(c)$. In other variant: feed $x$ and some bit $b$ as input, get $x$
and $b \oplus f(x)$ as output.

The trick will be to feed a superposition $\sum_x \alpha_x \ket x, b$ to input
and obtain the same superposition $\sum_x \alpha_x \ket x, ket {b \oplus f(x)}$
as output:

The quantum algorithm uses two new primitives:
\begin{itemize}
\item {\bf Phase Inversion} if exists $x^*: f(x^*) = 1$, then phase inversion
  modifies superposition $\sum \limits_x \alpha_x \ket x$ in such a way that
  $Phased(\alpha_{x^*}) = - \alpha_{x^*}$ (see \slide{15}2), or
$$\sum_x \alpha_x \ket x \xrightarrow{\text{Phase inversion}} \sum_{x \not
  \equiv x^*}\alpha_x \ket x - \alpha_{x^*}\ket{x^*}$$ Implementation - see
later \video{15-2}{04:00}
\item {\bf Inversion About Mean} - \slide{15-2}4

  Flip everything around mean value $\mu: \sum\limits_x \alpha_x \ket x \to
  \sum\limits_x(2\mu - \alpha_x) \ket x$ (there is quantum circuit, see later).
\end{itemize}

\subparagraph{Steps}
\begin{itemize}
\item Start with the equal superposition of all N items: $\frac1{\sqrt N} \sum_x
  \ket x$.
\item select (?) one $x$ and perform phase inversion on it (\slide{15-2}6).
\item perform inversion about mean: $x*$ moves up, new value is $\frac3{\sqrt
    N}$; in other points it becomes slightly less then $\frac1{\sqrt N}$ (\slide
  {15-2}7).

  At this point, amplitude of $x^*$ becomes roughly 3 times more then all
  others.
\item on repeat, the disproportion increases: new amplitude in $x^*$ becomes
  $\frac5{\sqrt N}$. After some amount of steps our amplitude in $x^*$ becomes
  closer and closer to $\frac1{\sqrt2}$
\end{itemize}

So, {\bf amplitude of the rest} when the needle has $\frac1{\sqrt2}$:
$\frac1{\sqrt{2N}}$. We're making $2 \times \frac1{2N} = \sqrt{\frac2N}$
improvement per step, and reach $\frac1{\sqrt2}$ in $O(\sqrt N)$ steps:
$$\frac{1/{\sqrt2}}{\sqrt2/{\sqrt N}} = \frac{\sqrt n}{2} \text{ steps}$$

\subsubsection{Implementing Grover's algorithm}
\label{sec:GroverImplmenting}

{\it Problem (remind):} Given $f:{0, 1, \dots , N-1} \to {0, 1}$, find $x:f(x) =
1$. {\bf Hardest case:} There is exactly one $x: f(x) = 1$.

We need two new elements to build a solver:

\paragraph{Phase Inversion} 
\video{15-3}

How do we send $f(x)$ to the phase? Use the effect that if we provide a $b$ bit
as input to $U_f$ quantum circuit we get result inverted if $f(x) = 1$. 
\video{15-3}{02:00}

But - we try to build $\hat U_f$ in such way that it returns $(-1)^{f(x)}\ket x$
when provided with $\ket x$ input (\slide{15-3}3): $\sum_x \alpha_x \ket x \to
\sum_x (-1)^{f(x)} \alpha_x \ket x$. \video{15-3}{03:33} - so only the needle's
phase will be inverted.

As such, if we send $\ket-$ to the input of the $U_f$, we'll get relevant output
qubit in state $(-1)^{f(x)}$. Proof: as $\ket- = \hqrt \ket0 - \hqrt \ket1$, and
we have two cases:
\begin{itemize}
\item Case 1: $f(x) = 0$; output is unchanged: $\ket-$
\item Case 2: $f(x) = 1$; output is toggled: $\ket- \to \hqrt \ket1 - \hqrt
  \ket0 = - (\ket-) = (-1)^{f(x)}$
\end{itemize}
The final circuit - see LL corner of \slide{15-3}3

\video{15-3}{07:35}
\paragraph{Reflection About Mean}

We start with $\sum_x \alpha_x \ket x$; getting $\mu = \frac{\sum \alpha_x}N$
and need to get a new superposition $\sum_x (2\mu - \alpha_x) \ket x$

For the implementing circuit - see \slide{15-3}4. The trick is similar to Phase
Inversion: input is $\ket x, \ket-$. The function $U_{\neq 0^n}$ returns 0 if
all $n$ input are 0 and 1 otherwise: $g(x) = \left\{\begin{array}{cl} 0 & if \;
    x = 0 \dots 0 \\ 1 & otherwise \end{array} \right.$ 

\subparagraph{How does it work?}
Reflection about mean is the same as doing reflection about $\ket u =
\frac1{\sqrt N} \sum_x \ket x$. First we move (map to) the uniform superposition
to the $\ket 0$ vector (this is done by Hadamard transform). Then we perform
inversion about $\ket{0 \dots }$ using $\left(\begin{array}{cccc} 1 & & & 0 \\ &
    -1 & & \\ &  & \ddots & \\ 0 & & & -1 \end{array} \right)$  - leaving
all-zero string alone and inverting (reflecting) everything else. Then -
Hadamard transform back:
$$H^{\otimes n} \left[
  \begin{pmatrix} 2 & & & 0 \\  & 0 & \\ & & \ddots \\ 0 & & & 0
  \end{pmatrix} - \begin{pmatrix} 1 & & & 0 \\  & 1 & \\ & & \ddots \\ 0 & & &
    1  \end{pmatrix}\right] H^{\otimes n}= $$ $$= H^{\otimes n}
  \begin{pmatrix} 2 & & & 0 \\  & 0 & \\ & & \ddots \\ 0 & & & 0
  \end{pmatrix} H^{\otimes n} - \underbrace{ H^{\otimes n} I H^{\otimes
      n}}_{I}$$
As Hadamard consists of $1/\sqrt{N}$, this will end in
$$= \begin{pmatrix} \frac2N &\frac2N & \dots  & \frac2N \\ 
  \frac2N& & & \vdots \\ \vdots & & & \vdots \\ \frac2N & \frac2N & \dots & \frac2N
  \end{pmatrix} - I = \begin{pmatrix} \frac2N-1 &\frac2N & \dots  & \frac2N \\ 
  \frac2N&\frac2N - 1 & & \vdots \\ \vdots & & & \vdots \\ \frac2N & \frac2N &
  \dots & \frac2N-1\end{pmatrix}$$ 
\video{15-3}{13:36}
This operator (diagonal entries $\frac2N-1$, all other are $\frac2N$) is really
a reflection: when applied to $\sum_x \alpha_x \ket x$: as  $\frac2N(\alpha_0 +
\alpha_1 + \dots + \alpha_{N-1} = 2\mu$, so:
$$\begin{pmatrix} \frac2N-1 &\frac2N & \dots  & \frac2N \\ 
  \frac2N&\frac2N - 1 & & \vdots \\ \vdots & & & \vdots \\ \frac2N & \frac2N &
  \dots & \frac2N-1\end{pmatrix}
\begin{pmatrix} \alpha_0 \\ \alpha_1 \\ \vdots \\ \alpha_{N-1} \end{pmatrix} =
\begin{pmatrix} 2\mu - \alpha_0 \\  2\mu - \alpha_1 \\ \vdots \\  2\mu -
  \alpha_{N-1} \end{pmatrix}$$ 
So $\sum_x \alpha_x \ket x \to \sum_x (2\mu - \alpha_x) \ket x$ - exactly as
needed. Final schema is shown in \slide{15-3}8. The ``working'' part (a part
after initialisation) is to be repeated $O(\sqrt N)$ times. Then - measure the
first n qubits of the output, and with the probability at least $\half$ we should
get a needle.

Then we check: if it is marked (is a needle), we repeat, otherwise - repeat the
process again.

Another observation for Grover's algorithm - the number of iteration should be
right ($\sqrt N$). If it is to be run too long, it might ``un-compute'' the
answer: eventually amplitude of the needle (after it reaches it's maximum)
starts reducing. 

If there is more then one needle, the Grover's algorithm runs faster, but
``un-computing'' starts earlier.

The whole algorithm is quite general, even while it gives only quadratic (not
exponential) speed-up, it solves quite generic problem, so it can be applied to
the solutions of a number of problems.

\subsection{Quantum complexity theory}
\label{sec:QuantumComplexityTheory}
\video{16-1}

Our old theorem about ``Any quantum algorithm must take at least $\sqrt N$ time''
(which means - make at least $\sqrt N$ queries), assuming $N=2^n$ gives minimum
time $O(2^{n/2})$ dependence on input size. This is exponential on number of
bites. 

{\bf Prof} 1 quantum query: no algorithm can guarantee success probability >
$c/N$,where $C$ - amount of needles. 

Perform test run with empty (i.e. no needle) haystack. Suppose algorithm makes
query; placing needle in the location that gets queried with minimum squared
amplitude: $x^* \leq \frac1N$

So change from $\ket \psi$ measured in running algorithm on empty stack to $\ket
{\psi'}$  measured on one-needle stack can not be more then $\frac1{\sqrt N}$ -
as $|\alpha_x|^2 \leq \frac1N$

But subsequent queries amplitudes can change depending on previous answers. For
$t$ steps, probability of success is $P[success] = \frac{O(t^2)}N$

\paragraph{Hybrid Argument}

Let's work with serial (not parallel) executing gates running on the non-empty
stack. Then, replacing last steps $f \to f'$ we'll get approaches to the
solution - $\frac1{\sqrt N}$ at the time.

As quantum evolution is unitary $\to$ last step will preserve angle between
input and output vectors. So replacing (one by one) the empty haystack runs with
the $f'$ we'll approach to the solution (\slide{16-1}6) that the probability
changes by $O(\frac{t^2}N)$.

The whole story with Grover's algorithm is that it manages these changes to line
up. 

This not necessarily means that quantum computer can not solve NP-complete
problem in polynomial time - just banality that ``any quantum algorithm must use
the structure of the problem''. Some algorithms do show polynomial time, it is
just impossible to rule out from the pure problem syntax if this problem is
solvable in polynomial or exponent time. 

In 2001 the paper about so-called {\bf adiabatic optimisation} framework was
published. Really this is an algorithms for solving ``satisfy-ability'' of the
algorithms. On small examples $n=[10 \dots 15]$ it showed polynomial times.

\paragraph{Adiabatic Quantum Optimisation}

We start with Hamiltonian $H_0$ of known ground state and try to gradually
transform it to the desirable output $H_f$ (the Hamiltonian which is easy to
implement and works that its ground state is the output of our algorithm (the
solution for the actual problem). Gradual transformation goes as
$$H(t) = (1-t)H_0 + tH_f$$ 

The theorem says that if you do this slowly enough ($T=\frac1{\min_t g(t)^2})$
where $g(t)$ is the difference 2 smallest eigenvalues of $H(t)$): if
\begin{itemize}
\item $E_0$ = ground energy
\item $E_1 = 1^{st}$ excited energy
\end{itemize}
then $g = E_1-E_0$

\subparagraph{Example} 3SAT as a local Hamiltonian Problem
\video{16-2}{09:24}

\begin{itemize}
\item $n$ bits -> $n$ qubits
\item 
\end{itemize}

\paragraph{Extended Church-Turing Thesis}

Any ``reasonable'' model of computation can be simulated on a (probabilistic)
Turing Machine with at most polynomial simulation overhead.
\begin{itemize}
\item Turing Machine describes the set of functions that are humanly
  compute-able (i.e. the class P describes what you could compute with an
  unlimited amount of paper at your disposal).
\item The class P represents what can be physically computed (in a reasonable
  amount of time).
\end{itemize}

Quantum computations is the only model of computation that violates the Extended
Church-Turing thesis.

BQP - bounded error quantum polynomial time.

\end{document}